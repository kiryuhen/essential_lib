# Профилирование производительности в Python

## Содержание
1. [Введение в профилирование](#введение-в-профилирование)
2. [Принципы оптимизации](#принципы-оптимизации)
3. [Измерение времени выполнения](#измерение-времени-выполнения)
4. [Встроенный модуль profiler](#встроенный-модуль-profiler)
5. [Профилирование с модулем cProfile](#профилирование-с-модулем-cprofile)
6. [Визуализация результатов с помощью snakeviz](#визуализация-результатов-с-помощью-snakeviz)
7. [Профилирование памяти с memory_profiler](#профилирование-памяти-с-memory_profiler)
8. [Построчное профилирование с line_profiler](#построчное-профилирование-с-line_profiler)
9. [Оптимизация узких мест](#оптимизация-узких-мест)
10. [Практические задания](#практические-задания)
11. [Мини-проект: Оптимизация обработки данных](#мини-проект-оптимизация-обработки-данных)

## Введение в профилирование

Профилирование — это процесс измерения производительности программы с целью выявления «узких мест», требующих оптимизации. Это ключевой инструмент для разработчиков, стремящихся создавать эффективный код.

### Зачем нужно профилирование?

1. **Идентификация узких мест** — выявление частей кода, которые потребляют непропорционально много ресурсов.
2. **Обоснованная оптимизация** — вместо случайных «улучшений» профилирование позволяет сосредоточиться на действительно проблемных участках.
3. **Измерение эффекта оптимизации** — количественная оценка результатов внесенных изменений.
4. **Понимание поведения программы** — более глубокое понимание того, как работает код во время выполнения.
5. **Совершенствование навыков кодирования** — развитие интуиции о том, какие решения ведут к эффективному коду.

### Что можно профилировать?

1. **Время выполнения** — сколько времени занимают различные части программы.
2. **Использование памяти** — сколько памяти выделяется и освобождается.
3. **Количество вызовов функций** — как часто и кем вызываются различные функции.
4. **Ввод-вывод** — сколько времени тратится на операции ввода-вывода.
5. **Использование CPU** — какая доля процессорного времени используется программой.

### Типы профилирования

1. **Детерминистическое профилирование** — отслеживает каждый вызов функции и каждое событие. Точное, но замедляет программу.
2. **Статистическое профилирование** — периодически отбирает состояние программы. Менее точное, но с меньшим влиянием на производительность.
3. **Инструментальное профилирование** — модифицирует код, чтобы включить инструкции для сбора данных о производительности.
4. **Событийное профилирование** — собирает данные только для определенных событий.

## Принципы оптимизации

Прежде чем погрузиться в инструменты профилирования, важно понять ключевые принципы оптимизации кода.

### Закон Кнута о преждевременной оптимизации

> "Преждевременная оптимизация — корень всех зол." — Дональд Кнут

Этот принцип напоминает, что оптимизация должна основываться на реальных данных профилирования, а не на интуиции или предположениях. Оптимизация имеет свою цену:
- Усложнение кода
- Затраты времени на разработку
- Потенциальные новые ошибки
- Ухудшение читаемости и поддерживаемости

### Правило 90/10

Обычно 90% времени выполнения программы тратится на 10% кода. Принцип говорит о том, что нужно сосредоточить усилия по оптимизации на этих критических 10% кода, а не пытаться оптимизировать всё подряд.

### KISS (Keep It Simple, Stupid)

Простой код часто оказывается более эффективным, чем сложные "оптимизированные" решения. Простота повышает поддерживаемость и снижает вероятность ошибок.

### Компромиссы при оптимизации

Оптимизация почти всегда связана с компромиссами:

1. **Время vs память** — более быстрый алгоритм может требовать больше памяти.
2. **Производительность vs читаемость** — оптимизированный код может быть менее понятным.
3. **Общий случай vs частный случай** — оптимизация для конкретных сценариев может ухудшить производительность в общем случае.
4. **Время разработки vs время выполнения** — сложная оптимизация требует больше времени на разработку.

### Процесс оптимизации

1. **Измерить** — определить текущую производительность (профилирование).
2. **Проанализировать** — выявить узкие места на основе данных.
3. **Оптимизировать** — внести изменения, направленные на улучшение.
4. **Снова измерить** — проверить, дали ли изменения ожидаемый эффект.

## Измерение времени выполнения

Самый простой способ начать профилирование — измерить время выполнения частей программы.

### Использование модуля time

Модуль `time` предоставляет функции для работы со временем, включая измерение времени выполнения.

```python
import time

# Измерение времени выполнения блока кода
start_time = time.time()
# Выполняемый код...
result = sum(range(10**7))
end_time = time.time()

execution_time = end_time - start_time
print(f"Время выполнения: {execution_time:.6f} секунд")
```

Для более точных измерений, особенно для коротких операций, лучше использовать `time.perf_counter()`:

```python
import time

# Более точное измерение времени для коротких операций
start_time = time.perf_counter()
# Выполняемый код...
result = sum(range(10**5))
end_time = time.perf_counter()

execution_time = end_time - start_time
print(f"Время выполнения: {execution_time:.9f} секунд")
```

### Декоратор для измерения времени

Удобно создать декоратор для измерения времени выполнения функций:

```python
import time
import functools

def timer(func):
    """
    Декоратор, измеряющий время выполнения функции.
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        execution_time = end_time - start_time
        print(f"Функция {func.__name__} выполнилась за {execution_time:.6f} секунд")
        return result
    return wrapper

# Использование декоратора
@timer
def calculate_sum(n):
    return sum(range(n))

# Вызов функции
calculate_sum(10**7)
```

### Контекстный менеджер для измерения времени

Контекстный менеджер позволяет измерять время выполнения блока кода:

```python
import time
import contextlib

@contextlib.contextmanager
def timing(description):
    """
    Контекстный менеджер для измерения времени выполнения.
    """
    start = time.perf_counter()
    yield
    elapsed_time = time.perf_counter() - start
    print(f"{description}: {elapsed_time:.6f} секунд")

# Использование контекстного менеджера
with timing("Вычисление суммы"):
    result = sum(range(10**7))
```

### Модуль timeit

Модуль `timeit` предназначен для точного измерения небольших фрагментов кода, минимизируя влияние случайных факторов:

```python
import timeit

# Измерение времени выполнения фрагмента кода
execution_time = timeit.timeit(
    stmt='sum(range(100))',  # Измеряемый код
    number=10000             # Количество повторений
)

print(f"Среднее время выполнения: {execution_time / 10000:.9f} секунд")
```

Измерение времени выполнения функции:

```python
import timeit

def calculate_sum(n):
    return sum(range(n))

# Измерение времени выполнения функции
execution_time = timeit.timeit(
    stmt='calculate_sum(100)',
    setup='from __main__ import calculate_sum',
    number=10000
)

print(f"Среднее время выполнения: {execution_time / 10000:.9f} секунд")
```

Сравнение разных реализаций:

```python
import timeit

# Первый способ: суммирование с помощью цикла
def sum_with_loop(n):
    total = 0
    for i in range(n):
        total += i
    return total

# Второй способ: использование функции sum
def sum_with_builtin(n):
    return sum(range(n))

# Третий способ: использование математической формулы
def sum_with_formula(n):
    return (n * (n - 1)) // 2

# Сравнение производительности
n = 10**6
methods = {
    'Цикл': lambda: sum_with_loop(n),
    'Встроенная функция': lambda: sum_with_builtin(n),
    'Формула': lambda: sum_with_formula(n)
}

for name, method in methods.items():
    execution_time = timeit.timeit(method, number=10)
    print(f"{name}: {execution_time:.6f} секунд")
```

## Встроенный модуль profiler

Python имеет встроенный модуль `profile` для профилирования кода, а также его более быструю реализацию на C — `cProfile`.

### Использование модуля profile

```python
import profile

def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Профилирование функции
profile.run('fibonacci(20)')
```

Недостатком модуля `profile` является его низкая производительность, так как он реализован на чистом Python. Для более производительного профилирования лучше использовать `cProfile`.

## Профилирование с модулем cProfile

Модуль `cProfile` — это реализация профилировщика на языке C, которая имеет меньшие накладные расходы по сравнению с `profile`.

### Основы использования cProfile

```python
import cProfile

def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Профилирование функции
cProfile.run('fibonacci(30)')
```

### Сохранение результатов профилирования в файл

```python
import cProfile

def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Профилирование функции и сохранение результатов в файл
cProfile.run('fibonacci(30)', 'fibonacci_stats')
```

### Анализ результатов с помощью модуля pstats

```python
import pstats
from pstats import SortKey

# Загрузка результатов профилирования
p = pstats.Stats('fibonacci_stats')

# Сортировка результатов по кумулятивному времени
p.sort_stats(SortKey.CUMULATIVE).print_stats(10)  # Вывод топ-10 функций

# Сортировка по количеству вызовов
p.sort_stats(SortKey.CALLS).print_stats(10)

# Сортировка по времени выполнения
p.sort_stats(SortKey.TIME).print_stats(10)
```

### Профилирование больших программ

Для профилирования больших программ можно создать специальный скрипт:

```python
# profiling_script.py
import cProfile
import pstats
import sys

# Импорт основного модуля программы
import main_program

# Функция для запуска профилирования
def run_profiling():
    # Параметры командной строки
    args = sys.argv[1:]
    
    # Запуск профилировщика
    cProfile.run('main_program.main(args)', 'program_stats')
    
    # Анализ результатов
    p = pstats.Stats('program_stats')
    p.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(20)

if __name__ == '__main__':
    run_profiling()
```

### Профилирование отдельных функций

```python
import cProfile
import pstats
import io

def profile_func(func, *args, **kwargs):
    """
    Профилирование отдельной функции.
    
    Args:
        func: Функция для профилирования.
        *args, **kwargs: Аргументы для функции.
    
    Returns:
        Результат выполнения функции.
    """
    # Создание профилировщика
    pr = cProfile.Profile()
    
    # Запуск профилирования
    pr.enable()
    result = func(*args, **kwargs)
    pr.disable()
    
    # Вывод результатов
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats()
    print(s.getvalue())
    
    return result

# Пример использования
def my_function(n):
    """Функция для тестирования профилирования."""
    total = 0
    for i in range(n):
        total += i * i
    return total

# Профилирование функции
profile_func(my_function, 10**6)
```

## Визуализация результатов с помощью snakeviz

`snakeviz` — это инструмент для визуализации результатов профилирования, полученных с помощью `cProfile`. Он представляет данные в виде интерактивной диаграммы Санки, что делает анализ более наглядным.

### Установка snakeviz

```bash
pip install snakeviz
```

### Использование snakeviz

1. Сначала профилируем программу и сохраняем результаты:

```python
import cProfile

def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Профилирование и сохранение результатов
cProfile.run('fibonacci(30)', 'fibonacci_stats')
```

2. Затем визуализируем результаты с помощью snakeviz:

```bash
snakeviz fibonacci_stats
```

Это откроет веб-браузер с интерактивной визуализацией.

### Интеграция snakeviz в код

```python
import cProfile
import subprocess
import os

def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Профилирование функции
profile_filename = 'fibonacci_stats'
cProfile.run('fibonacci(30)', profile_filename)

# Запуск snakeviz
subprocess.call(['snakeviz', profile_filename])
```

## Профилирование памяти с memory_profiler

`memory_profiler` — это модуль, который позволяет отслеживать использование памяти программой Python по строкам кода.

### Установка memory_profiler

```bash
pip install memory_profiler
```

### Основы использования memory_profiler

```python
from memory_profiler import profile

@profile
def memory_intensive_function():
    """Функция, интенсивно использующая память."""
    # Создание большого списка
    big_list = [i for i in range(10**6)]
    
    # Создание большого словаря
    big_dict = {i: i*i for i in range(10**5)}
    
    # Немного обработки для предотвращения оптимизации
    result = sum(big_list) + sum(big_dict.values())
    
    return result

# Вызов функции
memory_intensive_function()
```

Выполнение этого кода выведет информацию о потреблении памяти для каждой строки внутри функции.

### Профилирование скрипта целиком

```bash
python -m memory_profiler your_script.py
```

### Построение графиков потребления памяти

Для визуализации использования памяти можно установить дополнительный пакет `matplotlib`:

```bash
pip install matplotlib
```

Затем использовать команду `mprof`:

```bash
# Запуск скрипта с отслеживанием использования памяти
mprof run your_script.py

# Построение графика
mprof plot
```

### Пример более сложного профилирования памяти

```python
from memory_profiler import profile
import numpy as np

@profile
def create_arrays():
    """Создание различных массивов NumPy."""
    # Использование списков Python
    python_list = [i * 2 for i in range(10**6)]
    
    # Преобразование в массив NumPy
    numpy_array = np.array(python_list)
    
    # Освобождение памяти
    del python_list
    
    # Операции с массивом
    squared = numpy_array ** 2
    
    # Более эффективное создание массива
    direct_array = np.arange(10**6) * 2
    
    # Операции с новым массивом
    direct_squared = direct_array ** 2
    
    # Результат для предотвращения оптимизации
    return numpy_array.sum() + squared.sum() + direct_array.sum() + direct_squared.sum()

# Вызов функции
create_arrays()
```

## Построчное профилирование с line_profiler

`line_profiler` — это инструмент для построчного профилирования времени выполнения функций Python.

### Установка line_profiler

```bash
pip install line_profiler
```

### Основы использования line_profiler

1. Создайте скрипт с декоратором `@profile`:

```python
# my_script.py

@profile
def slow_function():
    """Функция с разной скоростью выполнения по строкам."""
    total = 0
    
    # Быстрая операция
    for i in range(1000):
        total += i
    
    # Медленная операция
    for i in range(1000):
        for j in range(1000):
            total += i * j
    
    # Средняя по скорости операция
    for i in range(10000):
        total += i * i
    
    return total

# Вызов функции
slow_function()
```

2. Запустите скрипт с помощью `kernprof`:

```bash
kernprof -l -v my_script.py
```

Параметры:
- `-l` — используется для построчного профилирования
- `-v` — выводит отчет профилирования

### Анализ результатов line_profiler программно

```python
import line_profiler

def slow_function():
    """Функция с разной скоростью выполнения по строкам."""
    total = 0
    
    # Быстрая операция
    for i in range(1000):
        total += i
    
    # Медленная операция
    for i in range(1000):
        for j in range(1000):
            total += i * j
    
    # Средняя по скорости операция
    for i in range(10000):
        total += i * i
    
    return total

# Создание профилировщика
profile = line_profiler.LineProfiler()

# Добавление функции в профилировщик
profile.add_function(slow_function)

# Обертывание основной функции для профилирования
wrapper = profile(slow_function)

# Вызов функции через обертку
wrapper()

# Вывод результатов
profile.print_stats()
```

## Оптимизация узких мест

После выявления узких мест с помощью профилирования следующий шаг — оптимизация кода. Рассмотрим некоторые общие приемы оптимизации.

### 1. Оптимизация алгоритмов

Улучшение алгоритмической сложности — часто самый эффективный способ оптимизации.

```python
# Неэффективная реализация поиска (O(n))
def find_element(arr, target):
    for elem in arr:
        if elem == target:
            return True
    return False

# Более эффективная реализация для отсортированных массивов (O(log n))
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return True
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return False
```

### 2. Использование подходящих структур данных

Выбор правильной структуры данных может значительно улучшить производительность.

```python
import time

# Пример: поиск элементов в разных структурах данных
n = 10**6
elements = list(range(n))
elements_set = set(elements)
elements_dict = dict.fromkeys(elements, True)

# Время поиска в списке
start = time.perf_counter()
found = 999999 in elements
end = time.perf_counter()
print(f"Список: {end - start:.6f} секунд")

# Время поиска в множестве
start = time.perf_counter()
found = 999999 in elements_set
end = time.perf_counter()
print(f"Множество: {end - start:.6f} секунд")

# Время поиска в словаре
start = time.perf_counter()
found = 999999 in elements_dict
end = time.perf_counter()
print(f"Словарь: {end - start:.6f} секунд")
```

### 3. Использование генераторов вместо списков

Генераторы используют меньше памяти, так как вычисляют элементы по мере необходимости.

```python
import sys

# Создание списка
big_list = [i for i in range(10**6)]
print(f"Размер списка: {sys.getsizeof(big_list)} байт")

# Создание генератора
big_generator = (i for i in range(10**6))
print(f"Размер генератора: {sys.getsizeof(big_generator)} байт")
```

### 4. Оптимизация циклов

```python
import time

# Неоптимизированный вариант
start = time.perf_counter()
result = []
for i in range(10**6):
    result.append(i * i)
end = time.perf_counter()
print(f"Append в цикле: {end - start:.6f} секунд")

# Оптимизированный вариант с генератором списка
start = time.perf_counter()
result = [i * i for i in range(10**6)]
end = time.perf_counter()
print(f"Генератор списка: {end - start:.6f} секунд")

# Предварительное выделение памяти
start = time.perf_counter()
result = [0] * 10**6
for i in range(10**6):
    result[i] = i * i
end = time.perf_counter()
print(f"Предварительное выделение памяти: {end - start:.6f} секунд")
```

### 5. Локальные переменные и избегание глобальных

```python
import time

# Глобальная переменная
x = 0

def increment_global():
    """Функция, использующая глобальную переменную."""
    global x
    for _ in range(10**6):
        x += 1

def increment_local():
    """Функция, использующая локальную переменную."""
    x = 0
    for _ in range(10**6):
        x += 1
    return x

# Сравнение производительности
start = time.perf_counter()
increment_global()
end = time.perf_counter()
print(f"Глобальная переменная: {end - start:.6f} секунд")

start = time.perf_counter()
increment_local()
end = time.perf_counter()
print(f"Локальная переменная: {end - start:.6f} секунд")
```

### 6. Использование встроенных функций и методов

```python
import time

# Ручная реализация суммы
def manual_sum(n):
    total = 0
    for i in range(n):
        total += i
    return total

# Сравнение со встроенной функцией
n = 10**7
start = time.perf_counter()
result1 = manual_sum(n)
end = time.perf_counter()
print(f"Ручная сумма: {end - start:.6f} секунд")

start = time.perf_counter()
result2 = sum(range(n))
end = time.perf_counter()
print(f"Встроенная функция sum: {end - start:.6f} секунд")
```

### 7. Распаковка в цикле for

```python
import time

# Словарь для тестирования
test_dict = {i: i*i for i in range(10**6)}

# Метод 1: использование items()
start = time.perf_counter()
total = 0
for k, v in test_dict.items():
    total += v
end = time.perf_counter()
print(f"items(): {end - start:.6f} секунд")

# Метод 2: использование keys() и обращение к словарю
start = time.perf_counter()
total = 0
for k in test_dict.keys():
    total += test_dict[k]
end = time.perf_counter()
print(f"keys() + lookup: {end - start:.6f} секунд")
```

### 8. Использование NumPy для вычислений

```python
import time
import numpy as np

# Обычные списки Python
start = time.perf_counter()
py_list1 = [i for i in range(10**6)]
py_list2 = [i * 2 for i in range(10**6)]
py_result = [a + b for a, b in zip(py_list1, py_list2)]
end = time.perf_counter()
print(f"Python списки: {end - start:.6f} секунд")

# NumPy массивы
start = time.perf_counter()
np_arr1 = np.arange(10**6)
np_arr2 = np_arr1 * 2
np_result = np_arr1 + np_arr2
end = time.perf_counter()
print(f"NumPy массивы: {end - start:.6f} секунд")
```

## Практические задания

### Задание 1: Профилирование и оптимизация функции поиска

Профилируйте и оптимизируйте следующую функцию, которая ищет все числа в диапазоне от 1 до n, которые делятся на все числа от 1 до k.

```python
def find_divisible_numbers(n, k):
    """
    Находит все числа от 1 до n, которые делятся на все числа от 1 до k.
    
    Args:
        n (int): Верхняя граница диапазона.
        k (int): Верхняя граница делителей.
        
    Returns:
        list: Список чисел, удовлетворяющих условиям.
    """
    result = []
    for i in range(1, n+1):
        divisible = True
        for j in range(1, k+1):
            if i % j != 0:
                divisible = False
                break
        if divisible:
            result.append(i)
    return result

# Пример использования
numbers = find_divisible_numbers(10000, 20)
print(f"Найдено {len(numbers)} чисел: {numbers}")
```

#### Решение

1. Сначала профилируем функцию:

```python
import cProfile
import pstats
import io

def profile_func(func, *args, **kwargs):
    """
    Профилирование функции.
    """
    pr = cProfile.Profile()
    pr.enable()
    result = func(*args, **kwargs)
    pr.disable()
    
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats()
    print(s.getvalue())
    
    return result

# Профилирование исходной функции
profile_func(find_divisible_numbers, 10000, 20)
```

2. Оптимизируем функцию:

```python
import math

def find_divisible_numbers_optimized(n, k):
    """
    Оптимизированная функция для поиска чисел, делящихся на все числа от 1 до k.
    
    Оптимизации:
    1. Используем НОК (наименьшее общее кратное) вместо проверки каждого делителя.
    2. Генерируем только кратные НОК.
    
    Args:
        n (int): Верхняя граница диапазона.
        k (int): Верхняя граница делителей.
        
    Returns:
        list: Список чисел, удовлетворяющих условиям.
    """
    # Функция для вычисления НОК двух чисел
    def lcm(a, b):
        return a * b // math.gcd(a, b)
    
    # Вычисляем НОК всех чисел от 1 до k
    current_lcm = 1
    for i in range(1, k+1):
        current_lcm = lcm(current_lcm, i)
    
    # Генерируем все кратные НОК до n
    return [i for i in range(current_lcm, n+1, current_lcm)]

# Профилирование оптимизированной функции
profile_func(find_divisible_numbers_optimized, 10000, 20)
```

3. Сравниваем результаты и производительность:

```python
import time

def compare_implementations(n, k):
    """
    Сравнивает производительность и результаты двух реализаций.
    """
    # Измеряем время оригинальной реализации
    start = time.perf_counter()
    original_result = find_divisible_numbers(n, k)
    original_time = time.perf_counter() - start
    
    # Измеряем время оптимизированной реализации
    start = time.perf_counter()
    optimized_result = find_divisible_numbers_optimized(n, k)
    optimized_time = time.perf_counter() - start
    
    # Проверяем, что результаты совпадают
    results_match = set(original_result) == set(optimized_result)
    
    # Выводим результаты сравнения
    print(f"Результаты совпадают: {results_match}")
    print(f"Оригинальная реализация: {original_time:.6f} секунд")
    print(f"Оптимизированная реализация: {optimized_time:.6f} секунд")
    print(f"Ускорение: {original_time / optimized_time:.2f}x")

# Сравниваем реализации
compare_implementations(10000, 20)
```

### Задание 2: Профилирование и оптимизация работы с большими данными

Профилируйте и оптимизируйте следующий код, который обрабатывает большой объем данных:

```python
import random
import string

def generate_data(n_records):
    """
    Генерирует случайные данные.
    
    Args:
        n_records (int): Количество генерируемых записей.
        
    Returns:
        list: Список словарей с данными.
    """
    data = []
    for _ in range(n_records):
        record = {
            'id': ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)),
            'value': random.randint(1, 1000),
            'text': ''.join(random.choices(string.ascii_lowercase, k=20))
        }
        data.append(record)
    return data

def process_data(data):
    """
    Обрабатывает данные: фильтрует, трансформирует и агрегирует.
    
    Args:
        data (list): Список словарей с данными.
        
    Returns:
        tuple: (отфильтрованные данные, сумма, средние значения по первой букве)
    """
    # Фильтрация: оставляем только записи с value > 500
    filtered_data = []
    for record in data:
        if record['value'] > 500:
            filtered_data.append(record)
    
    # Трансформация: добавляем новое поле - квадрат value
    for record in filtered_data:
        record['value_squared'] = record['value'] ** 2
    
    # Агрегация: вычисляем сумму всех value
    total_value = 0
    for record in filtered_data:
        total_value += record['value']
    
    # Группировка: среднее значение value по первой букве поля text
    groups = {}
    for record in filtered_data:
        first_letter = record['text'][0]
        if first_letter not in groups:
            groups[first_letter] = {'count': 0, 'sum': 0}
        groups[first_letter]['count'] += 1
        groups[first_letter]['sum'] += record['value']
    
    avg_by_letter = {}
    for letter, stats in groups.items():
        avg_by_letter[letter] = stats['sum'] / stats['count']
    
    return filtered_data, total_value, avg_by_letter

# Генерация и обработка данных
data = generate_data(100000)
result = process_data(data)
print(f"Отфильтровано записей: {len(result[0])}")
print(f"Общая сумма: {result[1]}")
print(f"Средние значения по первой букве: {len(result[2])} групп")
```

#### Решение

1. Сначала профилируем код:

```python
import cProfile
import pstats
import io

def profile_func(func, *args, **kwargs):
    """
    Профилирование функции.
    """
    pr = cProfile.Profile()
    pr.enable()
    result = func(*args, **kwargs)
    pr.disable()
    
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats()
    print(s.getvalue())
    
    return result

# Генерация данных
data = generate_data(100000)

# Профилирование обработки данных
profile_func(process_data, data)
```

2. Оптимизируем код:

```python
def process_data_optimized(data):
    """
    Оптимизированная функция обработки данных.
    
    Оптимизации:
    1. Используем генераторы списков вместо append в циклах.
    2. Используем dict.setdefault для упрощения работы с группами.
    3. Вычисляем square_value сразу при фильтрации.
    4. Используем sum() для суммирования.
    
    Args:
        data (list): Список словарей с данными.
        
    Returns:
        tuple: (отфильтрованные данные, сумма, средние значения по первой букве)
    """
    # Фильтрация и трансформация в одном шаге
    filtered_data = []
    for record in data:
        if record['value'] > 500:
            # Создаем новую копию словаря, чтобы не изменять оригинал
            new_record = record.copy()
            new_record['value_squared'] = new_record['value'] ** 2
            filtered_data.append(new_record)
    
    # Агрегация: используем встроенную функцию sum
    total_value = sum(record['value'] for record in filtered_data)
    
    # Группировка: используем defaultdict
    from collections import defaultdict
    group_counts = defaultdict(int)
    group_sums = defaultdict(int)
    
    for record in filtered_data:
        first_letter = record['text'][0]
        group_counts[first_letter] += 1
        group_sums[first_letter] += record['value']
    
    # Вычисление средних значений
    avg_by_letter = {
        letter: group_sums[letter] / group_counts[letter]
        for letter in group_counts
    }
    
    return filtered_data, total_value, avg_by_letter

# Профилирование оптимизированной функции
profile_func(process_data_optimized, data)
```

3. Еще более оптимизированная версия с использованием Pandas:

```python
import pandas as pd

def process_data_with_pandas(data):
    """
    Обработка данных с использованием Pandas.
    
    Args:
        data (list): Список словарей с данными.
        
    Returns:
        tuple: (отфильтрованные данные, сумма, средние значения по первой букве)
    """
    # Преобразуем данные в DataFrame
    df = pd.DataFrame(data)
    
    # Фильтрация
    filtered_df = df[df['value'] > 500].copy()
    
    # Трансформация
    filtered_df['value_squared'] = filtered_df['value'] ** 2
    
    # Агрегация
    total_value = filtered_df['value'].sum()
    
    # Создаем столбец с первой буквой
    filtered_df['first_letter'] = filtered_df['text'].str[0]
    
    # Группировка
    avg_by_letter = filtered_df.groupby('first_letter')['value'].mean().to_dict()
    
    # Преобразуем обратно в список словарей для совместимости
    filtered_data = filtered_df.to_dict('records')
    
    return filtered_data, total_value, avg_by_letter

# Профилирование версии с Pandas
try:
    profile_func(process_data_with_pandas, data)
except NameError:
    print("Для использования этой функции необходимо установить pandas: pip install pandas")
```

4. Сравниваем производительность:

```python
import time

def compare_implementations(data):
    """
    Сравнивает производительность и результаты реализаций.
    """
    # Измеряем время оригинальной реализации
    start = time.perf_counter()
    original_result = process_data(data)
    original_time = time.perf_counter() - start
    
    # Измеряем время оптимизированной реализации
    start = time.perf_counter()
    optimized_result = process_data_optimized(data)
    optimized_time = time.perf_counter() - start
    
    # Проверяем базовые результаты
    original_stats = (len(original_result[0]), original_result[1], len(original_result[2]))
    optimized_stats = (len(optimized_result[0]), optimized_result[1], len(optimized_result[2]))
    
    # Выводим результаты сравнения
    print(f"Оригинальная реализация: {original_time:.6f} секунд")
    print(f"Оптимизированная реализация: {optimized_time:.6f} секунд")
    print(f"Ускорение: {original_time / optimized_time:.2f}x")
    print(f"Результаты согласуются: {original_stats == optimized_stats}")
    
    # Если установлен pandas, сравниваем и с ним
    try:
        import pandas as pd
        
        start = time.perf_counter()
        pandas_result = process_data_with_pandas(data)
        pandas_time = time.perf_counter() - start
        
        pandas_stats = (len(pandas_result[0]), pandas_result[1], len(pandas_result[2]))
        
        print(f"Реализация с Pandas: {pandas_time:.6f} секунд")
        print(f"Ускорение с Pandas: {original_time / pandas_time:.2f}x")
        print(f"Результаты с Pandas согласуются: {original_stats == pandas_stats}")
    except ImportError:
        pass

# Сравниваем реализации
compare_implementations(data)
```

### Задание 3: Профилирование и оптимизация использования памяти

Профилируйте и оптимизируйте использование памяти в следующем коде, который генерирует большую матрицу и вычисляет статистики:

```python
import random

def generate_matrix(rows, cols):
    """
    Генерирует матрицу случайных чисел.
    
    Args:
        rows (int): Количество строк.
        cols (int): Количество столбцов.
        
    Returns:
        list: Матрица в виде списка списков.
    """
    matrix = []
    for i in range(rows):
        row = []
        for j in range(cols):
            row.append(random.randint(1, 100))
        matrix.append(row)
    return matrix

def calculate_statistics(matrix):
    """
    Вычисляет статистики для матрицы: сумму, среднее, минимум, максимум для каждой строки и столбца.
    
    Args:
        matrix (list): Матрица в виде списка списков.
        
    Returns:
        tuple: (статистики строк, статистики столбцов)
    """
    rows = len(matrix)
    cols = len(matrix[0]) if rows > 0 else 0
    
    # Статистики для строк
    row_stats = []
    for i in range(rows):
        row = matrix[i]
        stats = {
            'sum': sum(row),
            'avg': sum(row) / len(row),
            'min': min(row),
            'max': max(row)
        }
        row_stats.append(stats)
    
    # Статистики для столбцов
    col_stats = []
    for j in range(cols):
        column = [matrix[i][j] for i in range(rows)]
        stats = {
            'sum': sum(column),
            'avg': sum(column) / len(column),
            'min': min(column),
            'max': max(column)
        }
        col_stats.append(stats)
    
    return row_stats, col_stats

# Генерация большой матрицы и расчет статистик
matrix = generate_matrix(1000, 1000)  # Матрица 1000x1000
stats = calculate_statistics(matrix)
print(f"Статистики для {len(matrix)}x{len(matrix[0])} матрицы рассчитаны")
print(f"Статистики строк: {len(stats[0])}")
print(f"Статистики столбцов: {len(stats[1])}")
```

#### Решение

1. Сначала профилируем использование памяти:

```python
from memory_profiler import profile

@profile
def memory_test():
    """
    Функция для тестирования использования памяти.
    """
    matrix = generate_matrix(1000, 1000)
    stats = calculate_statistics(matrix)
    return stats

# Запуск профилирования памяти
memory_test()
```

2. Оптимизируем код для уменьшения использования памяти:

```python
import numpy as np
from memory_profiler import profile

def generate_matrix_numpy(rows, cols):
    """
    Генерирует матрицу случайных чисел, используя NumPy.
    
    Args:
        rows (int): Количество строк.
        cols (int): Количество столбцов.
        
    Returns:
        numpy.ndarray: Матрица в виде массива NumPy.
    """
    return np.random.randint(1, 101, size=(rows, cols))

def calculate_statistics_numpy(matrix):
    """
    Вычисляет статистики для матрицы, используя NumPy.
    
    Args:
        matrix (numpy.ndarray): Матрица в виде массива NumPy.
        
    Returns:
        tuple: (статистики строк, статистики столбцов)
    """
    # Статистики для строк (ось 1)
    row_sums = np.sum(matrix, axis=1)
    row_avgs = np.mean(matrix, axis=1)
    row_mins = np.min(matrix, axis=1)
    row_maxs = np.max(matrix, axis=1)
    
    # Собираем статистики строк в список словарей
    row_stats = [
        {'sum': s, 'avg': a, 'min': mi, 'max': ma}
        for s, a, mi, ma in zip(row_sums, row_avgs, row_mins, row_maxs)
    ]
    
    # Статистики для столбцов (ось 0)
    col_sums = np.sum(matrix, axis=0)
    col_avgs = np.mean(matrix, axis=0)
    col_mins = np.min(matrix, axis=0)
    col_maxs = np.max(matrix, axis=0)
    
    # Собираем статистики столбцов в список словарей
    col_stats = [
        {'sum': s, 'avg': a, 'min': mi, 'max': ma}
        for s, a, mi, ma in zip(col_sums, col_avgs, col_mins, col_maxs)
    ]
    
    return row_stats, col_stats

@profile
def memory_test_numpy():
    """
    Функция для тестирования использования памяти с NumPy.
    """
    matrix = generate_matrix_numpy(1000, 1000)
    stats = calculate_statistics_numpy(matrix)
    return stats

# Запуск профилирования памяти
memory_test_numpy()
```

3. Оптимизированная версия без создания промежуточных массивов:

```python
def calculate_statistics_optimized(matrix):
    """
    Оптимизированная функция для вычисления статистик.
    Избегает создания промежуточных списков для столбцов.
    
    Args:
        matrix (list): Матрица в виде списка списков.
        
    Returns:
        tuple: (статистики строк, статистики столбцов)
    """
    rows = len(matrix)
    cols = len(matrix[0]) if rows > 0 else 0
    
    # Статистики для строк
    row_stats = []
    for i in range(rows):
        row = matrix[i]
        row_sum = sum(row)
        stats = {
            'sum': row_sum,
            'avg': row_sum / len(row),
            'min': min(row),
            'max': max(row)
        }
        row_stats.append(stats)
    
    # Инициализация статистик для столбцов
    col_sums = [0] * cols
    col_mins = [float('inf')] * cols
    col_maxs = [float('-inf')] * cols
    
    # Вычисляем статистики за один проход по матрице
    for i in range(rows):
        for j in range(cols):
            value = matrix[i][j]
            col_sums[j] += value
            col_mins[j] = min(col_mins[j], value)
            col_maxs[j] = max(col_maxs[j], value)
    
    # Формируем статистики столбцов
    col_stats = []
    for j in range(cols):
        stats = {
            'sum': col_sums[j],
            'avg': col_sums[j] / rows,
            'min': col_mins[j],
            'max': col_maxs[j]
        }
        col_stats.append(stats)
    
    return row_stats, col_stats

@profile
def memory_test_optimized():
    """
    Функция для тестирования использования памяти с оптимизированной версией.
    """
    matrix = generate_matrix(1000, 1000)
    stats = calculate_statistics_optimized(matrix)
    return stats

# Запуск профилирования памяти
memory_test_optimized()
```

4. Сравниваем производительность и использование памяти:

```python
import time
import sys

def measure_memory_usage(func, *args, **kwargs):
    """
    Измеряет объем используемой памяти при выполнении функции.
    
    Примечание: это очень приблизительная оценка, использующая sys.getsizeof().
    Для точного измерения используйте memory_profiler.
    
    Args:
        func: Функция для измерения.
        *args, **kwargs: Аргументы функции.
        
    Returns:
        Результат функции.
    """
    import gc
    gc.collect()
    
    # Фиксируем объекты до выполнения
    old_objects = set(id(o) for o in gc.get_objects())
    
    # Выполняем функцию
    result = func(*args, **kwargs)
    
    # Фиксируем объекты после выполнения
    new_objects = set(id(o) for o in gc.get_objects())
    
    # Находим новые объекты
    created_objects = new_objects - old_objects
    created_memory = sum(sys.getsizeof(o) for o in gc.get_objects() if id(o) in created_objects)
    
    print(f"Функция {func.__name__} использовала примерно {created_memory / (1024*1024):.2f} МБ памяти")
    
    return result

def compare_implementations():
    """
    Сравнивает производительность и использование памяти для разных реализаций.
    """
    # Оригинальная реализация
    start = time.perf_counter()
    matrix = generate_matrix(1000, 1000)
    stats_original = calculate_statistics(matrix)
    original_time = time.perf_counter() - start
    print(f"Оригинальная реализация: {original_time:.6f} секунд")
    
    # Оптимизированная реализация
    start = time.perf_counter()
    stats_optimized = calculate_statistics_optimized(matrix)
    optimized_time = time.perf_counter() - start
    print(f"Оптимизированная реализация: {optimized_time:.6f} секунд")
    print(f"Ускорение: {original_time / optimized_time:.2f}x")
    
    # NumPy реализация
    try:
        import numpy as np
        start = time.perf_counter()
        matrix_np = generate_matrix_numpy(1000, 1000)
        stats_numpy = calculate_statistics_numpy(matrix_np)
        numpy_time = time.perf_counter() - start
        print(f"NumPy реализация: {numpy_time:.6f} секунд")
        print(f"Ускорение с NumPy: {original_time / numpy_time:.2f}x")
    except ImportError:
        print("NumPy не установлен. Выполните pip install numpy для сравнения.")
    
    # Проверяем полученные результаты
    def compare_stats(stats1, stats2):
        row_match = len(stats1[0]) == len(stats2[0])
        col_match = len(stats1[1]) == len(stats2[1])
        return row_match and col_match
    
    print(f"Результаты согласуются (оригинал vs. оптимизированный): {compare_stats(stats_original, stats_optimized)}")
    
    try:
        print(f"Результаты согласуются (оригинал vs. NumPy): {compare_stats(stats_original, stats_numpy)}")
    except NameError:
        pass

# Сравниваем реализации
compare_implementations()
```

## Мини-проект: Оптимизация обработки данных

В этом мини-проекте мы создадим, профилируем и оптимизируем систему обработки данных, которая имитирует реальные задачи анализа данных.

### Задача

Разработать систему, которая:
1. Загружает большой набор записей из CSV файла
2. Фильтрует записи по нескольким критериям
3. Выполняет вычисления и трансформации данных
4. Агрегирует результаты по категориям
5. Сохраняет результаты

### Исходная реализация

```python
import random
import string
import csv
import os
import time

class DataProcessor:
    def __init__(self, filename=None):
        """
        Инициализирует обработчик данных.
        
        Args:
            filename (str, optional): Имя файла для загрузки данных. По умолчанию None.
        """
        self.data = []
        if filename and os.path.exists(filename):
            self.load_from_csv(filename)
    
    def generate_sample_data(self, n_records=10000):
        """
        Генерирует образец данных для тестирования.
        
        Args:
            n_records (int, optional): Количество записей. По умолчанию 10000.
        """
        categories = ['A', 'B', 'C', 'D', 'E']
        regions = ['North', 'South', 'East', 'West', 'Central']
        
        for i in range(n_records):
            record = {
                'id': i,
                'category': random.choice(categories),
                'region': random.choice(regions),
                'value': random.uniform(1.0, 1000.0),
                'quantity': random.randint(1, 100),
                'active': random.choice([True, False]),
                'description': ''.join(random.choices(string.ascii_lowercase, k=20))
            }
            self.data.append(record)
    
    def save_to_csv(self, filename):
        """
        Сохраняет данные в CSV файл.
        
        Args:
            filename (str): Имя файла для сохранения.
        """
        if not self.data:
            print("Нет данных для сохранения.")
            return
        
        with open(filename, 'w', newline='') as csvfile:
            fieldnames = self.data[0].keys()
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for record in self.data:
                writer.writerow(record)
    
    def load_from_csv(self, filename):
        """
        Загружает данные из CSV файла.
        
        Args:
            filename (str): Имя файла для загрузки.
        """
        self.data = []
        with open(filename, 'r', newline='') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                # Преобразование типов
                try:
                    record = {
                        'id': int(row['id']),
                        'category': row['category'],
                        'region': row['region'],
                        'value': float(row['value']),
                        'quantity': int(row['quantity']),
                        'active': row['active'].lower() == 'true',
                        'description': row['description']
                    }
                    self.data.append(record)
                except (ValueError, KeyError) as e:
                    print(f"Ошибка при загрузке строки: {row}. Причина: {e}")
    
    def filter_data(self, conditions):
        """
        Фильтрует данные по заданным условиям.
        
        Args:
            conditions (dict): Словарь условий в формате {поле: (оператор, значение)}.
        
        Returns:
            pandas.DataFrame: Отфильтрованные данные.
        """
        filtered_data = self.data.copy()
        
        for field, (operator, value) in conditions.items():
            if field not in filtered_data.columns:
                continue
            
            if operator == 'eq':
                filtered_data = filtered_data[filtered_data[field] == value]
            elif operator == 'ne':
                filtered_data = filtered_data[filtered_data[field] != value]
            elif operator == 'gt':
                filtered_data = filtered_data[filtered_data[field] > value]
            elif operator == 'lt':
                filtered_data = filtered_data[filtered_data[field] < value]
            elif operator == 'ge':
                filtered_data = filtered_data[filtered_data[field] >= value]
            elif operator == 'le':
                filtered_data = filtered_data[filtered_data[field] <= value]
            elif operator == 'in':
                filtered_data = filtered_data[filtered_data[field].isin(value)]
            elif operator == 'contains':
                filtered_data = filtered_data[filtered_data[field].astype(str).str.contains(str(value))]
            else:
                print(f"Неподдерживаемый оператор: {operator}")
        
        return filtered_data
    
    def transform_data(self, records, transformations):
        """
        Применяет трансформации к данным.
        
        Args:
            records (pandas.DataFrame): Записи для трансформации.
            transformations (dict): Словарь трансформаций в формате {новое_поле: функция(record)}.
        
        Returns:
            pandas.DataFrame: Трансформированные данные.
        """
        transformed_data = records.copy()
        
        for new_field, transform_func in transformations.items():
            try:
                # Векторизированная трансформация, если возможно
                if new_field == 'total':
                    transformed_data[new_field] = transformed_data['value'] * transformed_data['quantity']
                elif new_field == 'discount':
                    # Создаем маску для категорий A и B
                    mask = transformed_data['category'].isin(['A', 'B'])
                    # Применяем разные значения в зависимости от маски
                    transformed_data[new_field] = 0.0
                    transformed_data.loc[mask, new_field] = transformed_data.loc[mask, 'value'] * 0.1
                elif new_field == 'net_value':
                    mask = transformed_data['category'].isin(['A', 'B'])
                    # Применяем разные формулы
                    transformed_data[new_field] = transformed_data['value'] * transformed_data['quantity']
                    transformed_data.loc[mask, new_field] = transformed_data.loc[mask, 'value'] * transformed_data.loc[mask, 'quantity'] * 0.9
                else:
                    # Если не удалось векторизовать, применяем функцию к каждой строке
                    transformed_data[new_field] = records.apply(transform_func, axis=1)
            except Exception as e:
                print(f"Ошибка при трансформации поля {new_field}: {e}")
        
        return transformed_data
    
    def aggregate_data(self, records, group_by, aggregations):
        """
        Агрегирует данные по указанным полям.
        
        Args:
            records (pandas.DataFrame): Записи для агрегации.
            group_by (str): Поле для группировки.
            aggregations (dict): Словарь агрегаций в формате {новое_поле: (поле, функция)}.
        
        Returns:
            dict: Агрегированные данные.
        """
        if group_by not in records.columns:
            print(f"Поле {group_by} не найдено")
            return {}
        
        # Создаем словарь агрегирующих функций в формате pandas
        agg_funcs = {}
        for agg_field, (field, func) in aggregations.items():
            if field not in records.columns:
                continue
            
            if func == 'sum':
                agg_funcs[agg_field] = (field, 'sum')
            elif func == 'avg':
                agg_funcs[agg_field] = (field, 'mean')
            elif func == 'min':
                agg_funcs[agg_field] = (field, 'min')
            elif func == 'max':
                agg_funcs[agg_field] = (field, 'max')
            elif func == 'count':
                agg_funcs[agg_field] = (field, 'count')
        
        # Группировка и агрегация
        if not agg_funcs:
            return {}
        
        # Создаем словарь агрегаций в формате, который понимает pandas
        pandas_aggs = {}
        for key, (field, func) in agg_funcs.items():
            if field not in pandas_aggs:
                pandas_aggs[field] = []
            pandas_aggs[field].append(func)
        
        # Выполняем группировку и агрегацию
        grouped = records.groupby(group_by).agg(pandas_aggs)
        
        # Преобразуем результат в словарь
        result = {}
        
        # Обрабатываем случай с мультииндексом в столбцах
        if isinstance(grouped.columns, pd.MultiIndex):
            # Преобразуем мультииндекс в обычные имена столбцов
            grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]
            
            # Переименовываем столбцы в соответствии с заданными именами агрегатов
            rename_dict = {}
            for agg_field, (field, func) in agg_funcs.items():
                if func == 'mean':
                    func = 'mean'  # в pandas 'avg' называется 'mean'
                col_name = f"{field}_{func}"
                if col_name in grouped.columns:
                    rename_dict[col_name] = agg_field
            
            if rename_dict:
                grouped = grouped.rename(columns=rename_dict)
        
        # Преобразуем в словарь
        for group_value, group_data in grouped.iterrows():
            result[group_value] = group_data.to_dict()
        
        return result
    
    def process_pipeline(self, pipeline):
        """
        Выполняет конвейер обработки данных.
        
        Args:
            pipeline (dict): Словарь с этапами конвейера.
        
        Returns:
            tuple: (отфильтрованные данные, трансформированные данные, агрегированные данные)
        """
        # Этап 1: Фильтрация
        start = time.time()
        filtered_data = self.data
        if 'filter' in pipeline:
            filtered_data = self.filter_data(pipeline['filter'])
            print(f"Отфильтровано: {len(filtered_data)} записей за {time.time() - start:.2f} секунд")
        
        # Этап 2: Трансформация
        start = time.time()
        transformed_data = filtered_data
        if 'transform' in pipeline:
            transformed_data = self.transform_data(filtered_data, pipeline['transform'])
            print(f"Трансформировано: {len(transformed_data)} записей за {time.time() - start:.2f} секунд")
        
        # Этап 3: Агрегация
        start = time.time()
        aggregated_data = None
        if 'aggregate' in pipeline:
            group_by = pipeline['aggregate'].get('group_by')
            aggregations = pipeline['aggregate'].get('aggregations', {})
            if group_by and aggregations:
                aggregated_data = self.aggregate_data(transformed_data, group_by, aggregations)
                print(f"Агрегировано: {len(aggregated_data)} групп за {time.time() - start:.2f} секунд")
        
        # Этап 4: Сохранение
        if 'save' in pipeline and not transformed_data.empty:
            start = time.time()
            self.data = transformed_data
            self.save_to_csv(pipeline['save'])
            print(f"Данные сохранены в файл: {pipeline['save']} за {time.time() - start:.2f} секунд")
        
        return filtered_data, transformed_data, aggregated_data

# Профилирование реализации на pandas
def profile_pandas_processor():
    """
    Профилирует обработчик данных на основе pandas.
    """
    processor = PandasDataProcessor()
    processor.generate_sample_data(100000)
    processor.save_to_csv("sample_data_pandas.csv")
    
    processor = PandasDataProcessor("sample_data_pandas.csv")
    
    pipeline = {
        'filter': {
            'active': ('eq', True),
            'value': ('gt', 500.0),
            'region': ('in', ['North', 'South']),
        },
        'transform': {
            'total': lambda r: r['value'] * r['quantity'],
            'discount': lambda r: r['value'] * 0.1 if r['category'] in ['A', 'B'] else 0,
            'net_value': lambda r: r['value'] * r['quantity'] * 0.9 if r['category'] in ['A', 'B'] else r['value'] * r['quantity'],
        },
        'aggregate': {
            'group_by': 'category',
            'aggregations': {
                'total_sum': ('total', 'sum'),
                'avg_quantity': ('quantity', 'avg'),
                'max_value': ('value', 'max'),
                'count': ('id', 'count'),
            }
        },
        'save': "processed_data_pandas.csv"
    }
    
    # Профилирование выполнения конвейера
    import cProfile
    import pstats
    import io
    
    pr = cProfile.Profile()
    pr.enable()
    filtered, transformed, aggregated = processor.process_pipeline(pipeline)
    pr.disable()
    
    # Вывод результатов профилирования
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(30)
    print(s.getvalue())
    
    return filtered, transformed, aggregated

# Сравнение всех трех реализаций
def compare_all_processors():
    """
    Сравнивает производительность всех трех реализаций обработчиков данных.
    """
    # Генерация данных
    print("Генерация данных для тестирования...")
    
    # Размер тестовых данных
    n_records = 100000
    
    # Оригинальный обработчик
    original = DataProcessor()
    original.generate_sample_data(n_records)
    original.save_to_csv("sample_data_original.csv")
    
    # Оптимизированный обработчик
    optimized = OptimizedDataProcessor()
    optimized.generate_sample_data(n_records)
    optimized.save_to_csv("sample_data_optimized.csv")
    
    # Pandas обработчик
    try:
        pandas_processor = PandasDataProcessor()
        pandas_processor.generate_sample_data(n_records)
        pandas_processor.save_to_csv("sample_data_pandas.csv")
    except NameError:
        print("Pandas не установлен. Установите pandas для полного сравнения: pip install pandas")
        pandas_available = False
    else:
        pandas_available = True
    
    # Настройка конвейера
    pipeline = {
        'filter': {
            'active': ('eq', True),
            'value': ('gt', 500.0),
            'region': ('in', ['North', 'South']),
        },
        'transform': {
            'total': lambda r: r['value'] * r['quantity'],
            'discount': lambda r: r['value'] * 0.1 if r['category'] in ['A', 'B'] else 0,
            'net_value': lambda r: r['value'] * r['quantity'] * 0.9 if r['category'] in ['A', 'B'] else r['value'] * r['quantity'],
        },
        'aggregate': {
            'group_by': 'category',
            'aggregations': {
                'total_sum': ('total', 'sum'),
                'avg_quantity': ('quantity', 'avg'),
                'max_value': ('value', 'max'),
                'count': ('id', 'count'),
            }
        }
    }
    
    # Результаты сравнения
    results = {}
    
    # Измерение времени оригинального обработчика
    print("\nТестирование оригинального обработчика...")
    original = DataProcessor("sample_data_original.csv")
    
    start = time.time()
    filtered_original, transformed_original, aggregated_original = original.process_pipeline(pipeline)
    original_time = time.time() - start
    
    results['original'] = {
        'time': original_time,
        'filtered': len(filtered_original),
        'transformed': len(transformed_original),
        'aggregated': len(aggregated_original) if aggregated_original else 0
    }
    
    print(f"Оригинальный обработчик: {original_time:.2f} секунд")
    
    # Измерение времени оптимизированного обработчика
    print("\nТестирование оптимизированного обработчика...")
    optimized = OptimizedDataProcessor("sample_data_optimized.csv")
    
    start = time.time()
    filtered_optimized, transformed_optimized, aggregated_optimized = optimized.process_pipeline(pipeline)
    optimized_time = time.time() - start
    
    results['optimized'] = {
        'time': optimized_time,
        'filtered': len(filtered_optimized),
        'transformed': len(transformed_optimized),
        'aggregated': len(aggregated_optimized) if aggregated_optimized else 0
    }
    
    print(f"Оптимизированный обработчик: {optimized_time:.2f} секунд")
    print(f"Ускорение: {original_time / optimized_time:.2f}x")
    
    # Измерение времени pandas обработчика
    if pandas_available:
        print("\nТестирование pandas обработчика...")
        pandas_processor = PandasDataProcessor("sample_data_pandas.csv")
        
        start = time.time()
        filtered_pandas, transformed_pandas, aggregated_pandas = pandas_processor.process_pipeline(pipeline)
        pandas_time = time.time() - start
        
        results['pandas'] = {
            'time': pandas_time,
            'filtered': len(filtered_pandas),
            'transformed': len(transformed_pandas),
            'aggregated': len(aggregated_pandas) if aggregated_pandas else 0
        }
        
        print(f"Pandas обработчик: {pandas_time:.2f} секунд")
        print(f"Ускорение относительно оригинала: {original_time / pandas_time:.2f}x")
        print(f"Ускорение относительно оптимизированного: {optimized_time / pandas_time:.2f}x")
    
    # Сводка результатов
    print("\nСводка результатов:")
    print(f"{'Реализация':<20} {'Время (сек)':<15} {'Фильтрация':<15} {'Трансформация':<15} {'Агрегация':<15}")
    print("-" * 80)
    
    for name, stats in results.items():
        print(f"{name:<20} {stats['time']:<15.2f} {stats['filtered']:<15} {stats['transformed']:<15} {stats['aggregated']:<15}")
    
    # Проверка результатов
    are_results_consistent = True
    baseline = results['original']
    
    for name, stats in results.items():
        if name == 'original':
            continue
        
        if (stats['filtered'] != baseline['filtered'] or
            stats['transformed'] != baseline['transformed'] or
            stats['aggregated'] != baseline['aggregated']):
            are_results_consistent = False
            break
    
    print(f"\nРезультаты согласуются между всеми реализациями: {are_results_consistent}")

# Запуск сравнения всех реализаций
# compare_all_processors()

# Выполнение отдельных тестов
# Раскомментируйте нужные строки для запуска конкретных тестов

# 1. Профилирование оригинальной реализации
# profile_processor()

# 2. Профилирование оптимизированной реализации
# profile_optimized_processor()

# 3. Профилирование pandas реализации
# try:
#     profile_pandas_processor()
# except NameError:
#     print("Pandas не установлен. Установите pandas: pip install pandas")

# 4. Сравнение производительности оригинальной и оптимизированной реализаций
# compare_processors()

# 5. Полное сравнение всех трех реализаций
# try:
#     compare_all_processors()
# except NameError:
#     print("Pandas не установлен для полного сравнения. Сравниваем только оригинальную и оптимизированную реализации...")
#     compare_processors()
```

### Анализ и выводы

В результате профилирования и оптимизации нашего мини-проекта мы получили следующие основные выводы:

1. **Оригинальная реализация** имеет множество неэффективных операций:
   - Избыточное создание объектов
   - Неоптимальная работа с коллекциями данных
   - Повторный расчет одних и тех же значений

2. **Оптимизированная реализация** показывает значительное улучшение:
   - Предкомпиляция условий фильтрации
   - Использование `defaultdict` для группировки
   - Минимизация создания копий данных
   - Более эффективная работа с итерациями

3. **Pandas реализация** демонстрирует наилучшую производительность:
   - Векторизация операций
   - Оптимизированные структуры данных
   - Высокоэффективные встроенные методы для группировки и агрегации

Общие принципы оптимизации, которые мы применили:

1. **Минимизация создания объектов** — повторное использование существующих объектов вместо создания новых.
2. **Векторизация операций** — обработка наборов данных вместо поэлементной обработки.
3. **Использование специализированных структур данных** — выбор структуры данных, оптимальной для конкретной задачи.
4. **Предварительные вычисления** — кэширование результатов, которые используются многократно.
5. **Сокращение количества проходов по данным** — объединение нескольких операций в один проход.

Этот мини-проект демонстрирует типичный процесс оптимизации:
1. Измерение текущей производительности с помощью профилирования
2. Выявление узких мест
3. Реализация оптимизаций
4. Повторное измерение для проверки эффективности изменений

Важно отметить, что оптимизация всегда представляет собой компромисс между производительностью, читаемостью кода и затратами на разработку. В зависимости от конкретной ситуации может быть оптимальным выбор разных подходов.

## Заключение

Профилирование и оптимизация — неотъемлемые части разработки качественного программного обеспечения. Профилирование позволяет выявить узкие места, которые ограничивают производительность, а оптимизация направлена на устранение этих ограничений.

В этом материале мы рассмотрели:

1. **Принципы оптимизации**, включая классические правила, такие как закон Кнута о преждевременной оптимизации и правило 90/10.

2. **Методы измерения времени выполнения**, от простых измерений с помощью `time.time()` до более сложных инструментов, таких как `timeit`.

3. **Профилирование с использованием встроенных модулей** `cProfile` и `profile` для анализа функций и выявления "горячих точек" программы.

4. **Визуализацию результатов профилирования** с помощью инструментов вроде `snakeviz`, которые помогают наглядно представить данные профилирования.

5. **Профилирование памяти** с использованием `memory_profiler` для выявления утечек памяти и оптимизации использования ресурсов.

6. **Построчное профилирование** с помощью `line_profiler` для выявления конкретных строк кода, являющихся узкими местами.

7. **Практические методы оптимизации**, включая улучшение алгоритмов, структур данных, использование генераторов и другие техники.

Ключевые идеи, которые следует помнить:

- **Профилируйте перед оптимизацией** — не угадывайте, где находятся узкие места; используйте инструменты для их точного определения.
- **Сосредоточьтесь на критических участках** — оптимизируйте те 10% кода, которые потребляют 90% ресурсов.
- **Измеряйте эффект оптимизаций** — всегда проверяйте, действительно ли ваши изменения улучшают производительность.
- **Учитывайте компромиссы** — между временем и памятью, между производительностью и читаемостью кода.
- **Используйте специализированные библиотеки**, такие как NumPy или Pandas, когда это оправдано объемом данных и характером задачи.

Профилирование и оптимизация — это не разовые действия, а непрерывный процесс улучшения кода. С ростом и изменением программы могут появляться новые узкие места, требующие внимания. Регулярное профилирование помогает поддерживать высокую производительность программы на протяжении всего жизненного цикла разработки.

Помните, что цель оптимизации — не создать самый быстрый код, а код, который достаточно быстр для решения конкретной задачи с учетом всех ограничений и требований.
ператор, значение)}.
                Поддерживаемые операторы: 'eq', 'ne', 'gt', 'lt', 'ge', 'le', 'in', 'contains'.
        
        Returns:
            list: Отфильтрованные данные.
        """
        result = []
        
        for record in self.data:
            match = True
            
            for field, (operator, value) in conditions.items():
                if field not in record:
                    match = False
                    break
                
                field_value = record[field]
                
                if operator == 'eq':
                    if field_value != value:
                        match = False
                        break
                elif operator == 'ne':
                    if field_value == value:
                        match = False
                        break
                elif operator == 'gt':
                    if field_value <= value:
                        match = False
                        break
                elif operator == 'lt':
                    if field_value >= value:
                        match = False
                        break
                elif operator == 'ge':
                    if field_value < value:
                        match = False
                        break
                elif operator == 'le':
                    if field_value > value:
                        match = False
                        break
                elif operator == 'in':
                    if field_value not in value:
                        match = False
                        break
                elif operator == 'contains':
                    if value not in str(field_value):
                        match = False
                        break
                else:
                    print(f"Неподдерживаемый оператор: {operator}")
                    match = False
                    break
            
            if match:
                result.append(record)
        
        return result
    
    def transform_data(self, records, transformations):
        """
        Применяет трансформации к данным.
        
        Args:
            records (list): Записи для трансформации.
            transformations (dict): Словарь трансформаций в формате {новое_поле: функция(record)}.
        
        Returns:
            list: Трансформированные данные.
        """
        result = []
        
        for record in records:
            new_record = record.copy()
            
            for new_field, transform_func in transformations.items():
                try:
                    new_record[new_field] = transform_func(record)
                except Exception as e:
                    print(f"Ошибка при трансформации поля {new_field}: {e}")
            
            result.append(new_record)
        
        return result
    
    def aggregate_data(self, records, group_by, aggregations):
        """
        Агрегирует данные по указанным полям.
        
        Args:
            records (list): Записи для агрегации.
            group_by (str): Поле для группировки.
            aggregations (dict): Словарь агрегаций в формате {новое_поле: (поле, функция)}.
                Функции: 'sum', 'avg', 'min', 'max', 'count'.
        
        Returns:
            dict: Агрегированные данные.
        """
        groups = {}
        
        for record in records:
            if group_by not in record:
                continue
            
            group_value = record[group_by]
            
            if group_value not in groups:
                groups[group_value] = {
                    'count': 0,
                    'sum': {},
                    'min': {},
                    'max': {}
                }
            
            group = groups[group_value]
            group['count'] += 1
            
            for agg_field, (field, func) in aggregations.items():
                if field not in record:
                    continue
                
                value = record[field]
                
                if func == 'sum' or func == 'avg':
                    if field not in group['sum']:
                        group['sum'][field] = 0
                    group['sum'][field] += value
                elif func == 'min':
                    if field not in group['min'] or value < group['min'][field]:
                        group['min'][field] = value
                elif func == 'max':
                    if field not in group['max'] or value > group['max'][field]:
                        group['max'][field] = value
        
        # Вычисление окончательных агрегатов
        result = {}
        
        for group_value, group in groups.items():
            result[group_value] = {}
            
            for agg_field, (field, func) in aggregations.items():
                if func == 'sum':
                    if field in group['sum']:
                        result[group_value][agg_field] = group['sum'][field]
                elif func == 'avg':
                    if field in group['sum'] and group['count'] > 0:
                        result[group_value][agg_field] = group['sum'][field] / group['count']
                elif func == 'min':
                    if field in group['min']:
                        result[group_value][agg_field] = group['min'][field]
                elif func == 'max':
                    if field in group['max']:
                        result[group_value][agg_field] = group['max'][field]
                elif func == 'count':
                    result[group_value][agg_field] = group['count']
        
        return result
    
    def process_pipeline(self, pipeline):
        """
        Выполняет конвейер обработки данных.
        
        Args:
            pipeline (dict): Словарь с этапами конвейера:
                - filter: условия фильтрации
                - transform: трансформации
                - aggregate: параметры агрегации
                - save: имя файла для сохранения
        
        Returns:
            tuple: (отфильтрованные данные, трансформированные данные, агрегированные данные)
        """
        # Этап 1: Фильтрация
        filtered_data = self.data
        if 'filter' in pipeline:
            filtered_data = self.filter_data(pipeline['filter'])
            print(f"Отфильтровано: {len(filtered_data)} записей")
        
        # Этап 2: Трансформация
        transformed_data = filtered_data
        if 'transform' in pipeline:
            transformed_data = self.transform_data(filtered_data, pipeline['transform'])
            print(f"Трансформировано: {len(transformed_data)} записей")
        
        # Этап 3: Агрегация
        aggregated_data = None
        if 'aggregate' in pipeline:
            group_by = pipeline['aggregate'].get('group_by')
            aggregations = pipeline['aggregate'].get('aggregations', {})
            if group_by and aggregations:
                aggregated_data = self.aggregate_data(transformed_data, group_by, aggregations)
                print(f"Агрегировано: {len(aggregated_data)} групп")
        
        # Этап 4: Сохранение
        if 'save' in pipeline and transformed_data:
            self.data = transformed_data
            self.save_to_csv(pipeline['save'])
            print(f"Данные сохранены в файл: {pipeline['save']}")
        
        return filtered_data, transformed_data, aggregated_data

# Пример использования
if __name__ == "__main__":
    # Подготовка данных
    processor = DataProcessor()
    processor.generate_sample_data(100000)
    processor.save_to_csv("sample_data.csv")
    
    # Загрузка данных
    start_time = time.time()
    processor = DataProcessor("sample_data.csv")
    print(f"Загружено {len(processor.data)} записей за {time.time() - start_time:.2f} секунд")
    
    # Настройка конвейера обработки
    pipeline = {
        'filter': {
            'active': ('eq', True),
            'value': ('gt', 500.0),
            'region': ('in', ['North', 'South']),
        },
        'transform': {
            'total': lambda r: r['value'] * r['quantity'],
            'discount': lambda r: r['value'] * 0.1 if r['category'] in ['A', 'B'] else 0,
            'net_value': lambda r: r['value'] * r['quantity'] * 0.9 if r['category'] in ['A', 'B'] else r['value'] * r['quantity'],
        },
        'aggregate': {
            'group_by': 'category',
            'aggregations': {
                'total_sum': ('total', 'sum'),
                'avg_quantity': ('quantity', 'avg'),
                'max_value': ('value', 'max'),
                'count': ('id', 'count'),
            }
        },
        'save': "processed_data.csv"
    }
    
    # Выполнение конвейера
    start_time = time.time()
    filtered, transformed, aggregated = processor.process_pipeline(pipeline)
    print(f"Конвейер выполнен за {time.time() - start_time:.2f} секунд")
    
    # Вывод результатов агрегации
    if aggregated:
        print("\nРезультаты агрегации:")
        for category, stats in aggregated.items():
            print(f"Категория: {category}")
            for stat_name, value in stats.items():
                print(f"  {stat_name}: {value:.2f}")
```

### Профилирование исходной реализации

```python
import cProfile
import pstats
import io

def profile_processor():
    """
    Профилирует обработчик данных.
    """
    processor = DataProcessor()
    processor.generate_sample_data(100000)
    processor.save_to_csv("sample_data.csv")
    
    processor = DataProcessor("sample_data.csv")
    
    pipeline = {
        'filter': {
            'active': ('eq', True),
            'value': ('gt', 500.0),
            'region': ('in', ['North', 'South']),
        },
        'transform': {
            'total': lambda r: r['value'] * r['quantity'],
            'discount': lambda r: r['value'] * 0.1 if r['category'] in ['A', 'B'] else 0,
            'net_value': lambda r: r['value'] * r['quantity'] * 0.9 if r['category'] in ['A', 'B'] else r['value'] * r['quantity'],
        },
        'aggregate': {
            'group_by': 'category',
            'aggregations': {
                'total_sum': ('total', 'sum'),
                'avg_quantity': ('quantity', 'avg'),
                'max_value': ('value', 'max'),
                'count': ('id', 'count'),
            }
        },
        'save': "processed_data.csv"
    }
    
    # Профилирование выполнения конвейера
    pr = cProfile.Profile()
    pr.enable()
    filtered, transformed, aggregated = processor.process_pipeline(pipeline)
    pr.disable()
    
    # Вывод результатов профилирования
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(30)
    print(s.getvalue())

# Запуск профилирования
profile_processor()
```

### Оптимизированная реализация

```python
import csv
import os
import time
import random
import string
from collections import defaultdict

class OptimizedDataProcessor:
    def __init__(self, filename=None):
        """
        Инициализирует оптимизированный обработчик данных.
        
        Args:
            filename (str, optional): Имя файла для загрузки данных. По умолчанию None.
        """
        self.data = []
        if filename and os.path.exists(filename):
            self.load_from_csv(filename)
    
    def generate_sample_data(self, n_records=10000):
        """
        Генерирует образец данных для тестирования.
        
        Args:
            n_records (int, optional): Количество записей. По умолчанию 10000.
        """
        categories = ['A', 'B', 'C', 'D', 'E']
        regions = ['North', 'South', 'East', 'West', 'Central']
        
        # Предварительное выделение памяти
        self.data = [{} for _ in range(n_records)]
        
        for i in range(n_records):
            record = {
                'id': i,
                'category': random.choice(categories),
                'region': random.choice(regions),
                'value': random.uniform(1.0, 1000.0),
                'quantity': random.randint(1, 100),
                'active': random.choice([True, False]),
                'description': ''.join(random.choices(string.ascii_lowercase, k=20))
            }
            self.data[i] = record
    
    def save_to_csv(self, filename):
        """
        Сохраняет данные в CSV файл.
        
        Args:
            filename (str): Имя файла для сохранения.
        """
        if not self.data:
            print("Нет данных для сохранения.")
            return
        
        with open(filename, 'w', newline='') as csvfile:
            fieldnames = self.data[0].keys()
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            writer.writerows(self.data)  # Запись всех строк одним вызовом
    
    def load_from_csv(self, filename):
        """
        Загружает данные из CSV файла.
        
        Args:
            filename (str): Имя файла для загрузки.
        """
        self.data = []
        type_converters = {
            'id': int,
            'value': float,
            'quantity': int,
            'active': lambda x: x.lower() == 'true'
        }
        
        with open(filename, 'r', newline='') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                # Преобразование типов
                record = {}
                try:
                    for key, value in row.items():
                        if key in type_converters:
                            record[key] = type_converters[key](value)
                        else:
                            record[key] = value
                    self.data.append(record)
                except (ValueError, KeyError) as e:
                    print(f"Ошибка при загрузке строки: {row}. Причина: {e}")
    
    def filter_data(self, conditions):
        """
        Фильтрует данные по заданным условиям.
        
        Args:
            conditions (dict): Словарь условий в формате {поле: (оператор, значение)}.
        
        Returns:
            list: Отфильтрованные данные.
        """
        # Предкомпиляция условий для оптимизации
        compiled_conditions = []
        
        for field, (operator, value) in conditions.items():
            if operator == 'eq':
                condition = lambda r, f=field, v=value: f in r and r[f] == v
            elif operator == 'ne':
                condition = lambda r, f=field, v=value: f in r and r[f] != v
            elif operator == 'gt':
                condition = lambda r, f=field, v=value: f in r and r[f] > v
            elif operator == 'lt':
                condition = lambda r, f=field, v=value: f in r and r[f] < v
            elif operator == 'ge':
                condition = lambda r, f=field, v=value: f in r and r[f] >= v
            elif operator == 'le':
                condition = lambda r, f=field, v=value: f in r and r[f] <= v
            elif operator == 'in':
                condition = lambda r, f=field, v=value: f in r and r[f] in v
            elif operator == 'contains':
                condition = lambda r, f=field, v=value: f in r and v in str(r[f])
            else:
                print(f"Неподдерживаемый оператор: {operator}")
                condition = lambda r: False
            
            compiled_conditions.append(condition)
        
        # Фильтрация с использованием скомпилированных условий
        result = []
        for record in self.data:
            if all(condition(record) for condition in compiled_conditions):
                result.append(record)
        
        return result
    
    def transform_data(self, records, transformations):
        """
        Применяет трансформации к данным.
        
        Args:
            records (list): Записи для трансформации.
            transformations (dict): Словарь трансформаций в формате {новое_поле: функция(record)}.
        
        Returns:
            list: Трансформированные данные.
        """
        result = [{**record} for record in records]  # Создаем копии записей
        
        for new_field, transform_func in transformations.items():
            for i, record in enumerate(result):
                try:
                    record[new_field] = transform_func(records[i])
                except Exception as e:
                    print(f"Ошибка при трансформации поля {new_field}: {e}")
        
        return result
    
    def aggregate_data(self, records, group_by, aggregations):
        """
        Агрегирует данные по указанным полям.
        
        Args:
            records (list): Записи для агрегации.
            group_by (str): Поле для группировки.
            aggregations (dict): Словарь агрегаций в формате {новое_поле: (поле, функция)}.
        
        Returns:
            dict: Агрегированные данные.
        """
        # Используем defaultdict для упрощения работы с группами
        groups = defaultdict(lambda: {
            'count': 0,
            'sum': defaultdict(float),
            'min': defaultdict(lambda: float('inf')),
            'max': defaultdict(lambda: float('-inf'))
        })
        
        # Группировка данных
        for record in records:
            if group_by not in record:
                continue
            
            group_value = record[group_by]
            group = groups[group_value]
            group['count'] += 1
            
            for _, (field, func) in aggregations.items():
                if field not in record:
                    continue
                
                value = record[field]
                
                if func in ('sum', 'avg'):
                    group['sum'][field] += value
                elif func == 'min':
                    group['min'][field] = min(group['min'][field], value)
                elif func == 'max':
                    group['max'][field] = max(group['max'][field], value)
        
        # Вычисление окончательных агрегатов
        result = {}
        
        for group_value, group in groups.items():
            result[group_value] = {}
            
            for agg_field, (field, func) in aggregations.items():
                if func == 'sum':
                    result[group_value][agg_field] = group['sum'][field]
                elif func == 'avg':
                    result[group_value][agg_field] = group['sum'][field] / group['count'] if group['count'] > 0 else 0
                elif func == 'min':
                    if group['min'][field] != float('inf'):
                        result[group_value][agg_field] = group['min'][field]
                    else:
                        result[group_value][agg_field] = None
                elif func == 'max':
                    if group['max'][field] != float('-inf'):
                        result[group_value][agg_field] = group['max'][field]
                    else:
                        result[group_value][agg_field] = None
                elif func == 'count':
                    result[group_value][agg_field] = group['count']
        
        return result
    
    def process_pipeline(self, pipeline):
        """
        Выполняет конвейер обработки данных.
        
        Args:
            pipeline (dict): Словарь с этапами конвейера.
        
        Returns:
            tuple: (отфильтрованные данные, трансформированные данные, агрегированные данные)
        """
        # Этап 1: Фильтрация
        start = time.time()
        filtered_data = self.data
        if 'filter' in pipeline:
            filtered_data = self.filter_data(pipeline['filter'])
            print(f"Отфильтровано: {len(filtered_data)} записей за {time.time() - start:.2f} секунд")
        
        # Этап 2: Трансформация
        start = time.time()
        transformed_data = filtered_data
        if 'transform' in pipeline:
            transformed_data = self.transform_data(filtered_data, pipeline['transform'])
            print(f"Трансформировано: {len(transformed_data)} записей за {time.time() - start:.2f} секунд")
        
        # Этап 3: Агрегация
        start = time.time()
        aggregated_data = None
        if 'aggregate' in pipeline:
            group_by = pipeline['aggregate'].get('group_by')
            aggregations = pipeline['aggregate'].get('aggregations', {})
            if group_by and aggregations:
                aggregated_data = self.aggregate_data(transformed_data, group_by, aggregations)
                print(f"Агрегировано: {len(aggregated_data)} групп за {time.time() - start:.2f} секунд")
        
        # Этап 4: Сохранение
        if 'save' in pipeline and transformed_data:
            start = time.time()
            self.data = transformed_data
            self.save_to_csv(pipeline['save'])
            print(f"Данные сохранены в файл: {pipeline['save']} за {time.time() - start:.2f} секунд")
        
        return filtered_data, transformed_data, aggregated_data

# Профилирование оптимизированной реализации
def profile_optimized_processor():
    """
    Профилирует оптимизированный обработчик данных.
    """
    processor = OptimizedDataProcessor()
    processor.generate_sample_data(100000)
    processor.save_to_csv("sample_data_optimized.csv")
    
    processor = OptimizedDataProcessor("sample_data_optimized.csv")
    
    pipeline = {
        'filter': {
            'active': ('eq', True),
            'value': ('gt', 500.0),
            'region': ('in', ['North', 'South']),
        },
        'transform': {
            'total': lambda r: r['value'] * r['quantity'],
            'discount': lambda r: r['value'] * 0.1 if r['category'] in ['A', 'B'] else 0,
            'net_value': lambda r: r['value'] * r['quantity'] * 0.9 if r['category'] in ['A', 'B'] else r['value'] * r['quantity'],
        },
        'aggregate': {
            'group_by': 'category',
            'aggregations': {
                'total_sum': ('total', 'sum'),
                'avg_quantity': ('quantity', 'avg'),
                'max_value': ('value', 'max'),
                'count': ('id', 'count'),
            }
        },
        'save': "processed_data_optimized.csv"
    }
    
    # Профилирование выполнения конвейера
    import cProfile
    import pstats
    import io
    
    pr = cProfile.Profile()
    pr.enable()
    filtered, transformed, aggregated = processor.process_pipeline(pipeline)
    pr.disable()
    
    # Вывод результатов профилирования
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(30)
    print(s.getvalue())
    
    return filtered, transformed, aggregated

# Сравнение производительности
def compare_processors():
    """
    Сравнивает производительность оригинального и оптимизированного обработчиков данных.
    """
    # Генерация данных
    print("Генерация данных для тестирования...")
    
    # Оригинальный обработчик
    original = DataProcessor()
    original.generate_sample_data(100000)
    original.save_to_csv("sample_data_original.csv")
    
    # Оптимизированный обработчик
    optimized = OptimizedDataProcessor()
    optimized.generate_sample_data(100000)
    optimized.save_to_csv("sample_data_optimized.csv")
    
    # Настройка конвейера
    pipeline = {
        'filter': {
            'active': ('eq', True),
            'value': ('gt', 500.0),
            'region': ('in', ['North', 'South']),
        },
        'transform': {
            'total': lambda r: r['value'] * r['quantity'],
            'discount': lambda r: r['value'] * 0.1 if r['category'] in ['A', 'B'] else 0,
            'net_value': lambda r: r['value'] * r['quantity'] * 0.9 if r['category'] in ['A', 'B'] else r['value'] * r['quantity'],
        },
        'aggregate': {
            'group_by': 'category',
            'aggregations': {
                'total_sum': ('total', 'sum'),
                'avg_quantity': ('quantity', 'avg'),
                'max_value': ('value', 'max'),
                'count': ('id', 'count'),
            }
        }
    }
    
    # Измерение времени оригинального обработчика
    print("\nТестирование оригинального обработчика...")
    original = DataProcessor("sample_data_original.csv")
    
    start = time.time()
    filtered_original, transformed_original, aggregated_original = original.process_pipeline(pipeline)
    original_time = time.time() - start
    
    print(f"Оригинальный обработчик: {original_time:.2f} секунд")
    
    # Измерение времени оптимизированного обработчика
    print("\nТестирование оптимизированного обработчика...")
    optimized = OptimizedDataProcessor("sample_data_optimized.csv")
    
    start = time.time()
    filtered_optimized, transformed_optimized, aggregated_optimized = optimized.process_pipeline(pipeline)
    optimized_time = time.time() - start
    
    print(f"Оптимизированный обработчик: {optimized_time:.2f} секунд")
    print(f"Ускорение: {original_time / optimized_time:.2f}x")
    
    # Проверка результатов
    original_stats = (len(filtered_original), len(transformed_original), len(aggregated_original) if aggregated_original else 0)
    optimized_stats = (len(filtered_optimized), len(transformed_optimized), len(aggregated_optimized) if aggregated_optimized else 0)
    
    print(f"Результаты согласуются: {original_stats == optimized_stats}")

# Запуск сравнения
compare_processors()
```

### Версия с использованием pandas

```python
import pandas as pd
import numpy as np
import time

class PandasDataProcessor:
    def __init__(self, filename=None):
        """
        Инициализирует обработчик данных на основе pandas.
        
        Args:
            filename (str, optional): Имя файла для загрузки данных. По умолчанию None.
        """
        self.data = pd.DataFrame()
        if filename:
            self.load_from_csv(filename)
    
    def generate_sample_data(self, n_records=10000):
        """
        Генерирует образец данных для тестирования.
        
        Args:
            n_records (int, optional): Количество записей. По умолчанию 10000.
        """
        import random
        import string
        
        categories = ['A', 'B', 'C', 'D', 'E']
        regions = ['North', 'South', 'East', 'West', 'Central']
        
        # Генерация данных с использованием numpy для ускорения
        self.data = pd.DataFrame({
            'id': np.arange(n_records),
            'category': np.random.choice(categories, n_records),
            'region': np.random.choice(regions, n_records),
            'value': np.random.uniform(1.0, 1000.0, n_records),
            'quantity': np.random.randint(1, 101, n_records),
            'active': np.random.choice([True, False], n_records),
            'description': [''.join(random.choices(string.ascii_lowercase, k=20)) for _ in range(n_records)]
        })
    
    def save_to_csv(self, filename):
        """
        Сохраняет данные в CSV файл.
        
        Args:
            filename (str): Имя файла для сохранения.
        """
        if self.data.empty:
            print("Нет данных для сохранения.")
            return
        
        self.data.to_csv(filename, index=False)
    
    def load_from_csv(self, filename):
        """
        Загружает данные из CSV файла.
        
        Args:
            filename (str): Имя файла для загрузки.
        """
        try:
            self.data = pd.read_csv(filename)
            
            # Преобразование типов
            if 'active' in self.data.columns:
                self.data['active'] = self.data['active'].astype(bool)
        except Exception as e:
            print(f"Ошибка при загрузке файла: {e}")
    
    def filter_data(self, conditions):
        """
        Фильтрует данные по заданным условиям.
        
        Args:
            conditions (dict): Словарь условий в формате {поле: (о