# Кэширование в Python

## Содержание
- [Введение](#введение)
- [Основы кэширования](#основы-кэширования)
- [Кэширование в Django](#кэширование-в-django)
- [Кэширование в Flask](#кэширование-в-flask)
- [Кэширование в FastAPI](#кэширование-в-fastapi)
- [Memcached](#memcached)
- [Redis](#redis)
- [Локальное кэширование](#локальное-кэширование)
- [Валидация и инвалидация кэша](#валидация-и-инвалидация-кэша)
- [Паттерны кэширования](#паттерны-кэширования)
- [Производительность и масштабирование](#производительность-и-масштабирование)
- [Кэширование в распределенных системах](#кэширование-в-распределенных-системах)
- [Сравнительный анализ](#сравнительный-анализ)
- [Лучшие практики](#лучшие-практики)
- [Подводные камни](#подводные-камни)
- [Полезные ресурсы](#полезные-ресурсы)

## Введение

Кэширование — это техника временного хранения копий данных в быстродоступной памяти с целью повышения производительности и снижения нагрузки на ресурсы системы. Правильно реализованное кэширование может значительно ускорить работу веб-приложений, снизить время отклика, уменьшить нагрузку на базу данных и улучшить масштабируемость.

В контексте Python и веб-разработки существует множество стратегий и инструментов для реализации различных видов кэширования: от простого in-memory кэша до распределенных систем кэширования. Этот документ охватывает основные концепции кэширования, их реализацию в популярных Python-фреймворках, инструменты кэширования и лучшие практики.

## Основы кэширования

### Типы кэширования

1. **In-memory кэширование** — данные хранятся в оперативной памяти приложения
2. **Файловое кэширование** — данные сохраняются в файлах на диске
3. **Кэширование в базе данных** — использование БД для хранения кэша
4. **Распределенный кэш** — кэш, распределенный между несколькими серверами (Redis, Memcached)
5. **HTTP-кэширование** — использование HTTP-заголовков для кэширования на стороне клиента или промежуточных прокси

### Основные концепции

#### Ключи кэша

Ключи кэша должны быть уникальными и представительными для хранимых данных.

```python
# Примеры ключей кэша
cache_key = "user:{}".format(user_id)  # Для данных пользователя
cache_key = "product:{}:reviews".format(product_id)  # Для отзывов о продукте
cache_key = "blog:recent_posts:{}".format(count)  # Для списка последних постов
```

#### Время жизни (TTL)

TTL (Time To Live) определяет, как долго запись будет храниться в кэше до автоматической инвалидации.

```python
# Пример установки TTL
cache.set("key", value, timeout=3600)  # Хранить 1 час
cache.set("key", value, timeout=86400)  # Хранить 1 день
cache.set("key", value, timeout=None)  # Хранить бессрочно (или до заполнения кэша)
```

#### Стратегии вытеснения

Когда кэш заполняется, необходимо определить, какие элементы удалить:

1. **LRU (Least Recently Used)** — удаление наименее недавно использованных элементов
2. **LFU (Least Frequently Used)** — удаление наименее часто используемых элементов
3. **FIFO (First In, First Out)** — удаление самых старых элементов
4. **TTL-based** — удаление элементов с истекшим временем жизни

#### Сериализация

Для хранения сложных объектов в кэше необходима сериализация.

```python
import json
import pickle

# JSON сериализация (для простых типов данных)
cache.set("key", json.dumps(data))
data = json.loads(cache.get("key"))

# Pickle сериализация (для Python-объектов)
cache.set("key", pickle.dumps(complex_object))
complex_object = pickle.loads(cache.get("key"))
```

## Кэширование в Django

Django предоставляет комплексную систему кэширования с несколькими бэкендами и удобным API.

### Настройка кэширования

```python
# settings.py

# Кэширование в памяти
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',
        'LOCATION': 'unique-snowflake',
    }
}

# Файловое кэширование
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
        'LOCATION': '/var/tmp/django_cache',
        'TIMEOUT': 600,  # 10 минут
        'OPTIONS': {
            'MAX_ENTRIES': 1000,
            'CULL_FREQUENCY': 2,  # Удаляет 1/2 записей при заполнении
        }
    }
}

# Memcached
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.memcached.PyMemcacheCache',
        'LOCATION': '127.0.0.1:11211',
    }
}

# Redis с django-redis
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/1',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'PARSER_CLASS': 'redis.connection.HiredisParser',
        }
    }
}

# Использование нескольких кэшей
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/1',
    },
    'sessions': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/2',
    },
    'filesystem': {
        'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
        'LOCATION': '/var/tmp/django_cache',
    }
}
```

### Низкоуровневое API

```python
from django.core.cache import cache, caches

# Базовые операции
cache.set('key', 'value', timeout=300)  # Сохранить в кэше на 5 минут
value = cache.get('key')  # Получить из кэша
cache.delete('key')  # Удалить из кэша

# Получение с дефолтным значением
value = cache.get('key', 'default')  # Вернет 'default', если ключа нет

# Инкремент/декремент
cache.set('counter', 0)
cache.incr('counter')  # 1
cache.incr('counter', 10)  # 11
cache.decr('counter')  # 10

# Проверка наличия ключа
if cache.has_key('key'):
    # Ключ существует
    pass

# Атомарные операции добавления и обновления
cache.add('key', 'value')  # Добавит только если ключа нет
cache.set_many({'key1': 'value1', 'key2': 'value2'})  # Установка нескольких значений
values = cache.get_many(['key1', 'key2'])  # Получение нескольких значений
cache.delete_many(['key1', 'key2'])  # Удаление нескольких значений

# Работа с конкретным кэшем
file_cache = caches['filesystem']
file_cache.set('key', 'value')
```

### Кэширование представлений

```python
from django.views.decorators.cache import cache_page, cache_control

# Кэширование всего представления на 15 минут
@cache_page(60 * 15)
def my_view(request):
    # ...
    return response

# Кэширование с учетом GET-параметров
@cache_page(60 * 15, key_prefix="custom_prefix")
def my_view(request):
    # ...
    return response

# Комбинирование с HTTP-кэшированием
@cache_control(max_age=3600, public=True)
@cache_page(60 * 15)
def my_view(request):
    # ...
    return response

# Кэширование с учетом пользователя
@cache_page(60 * 15, key_prefix="user_{}")
def user_specific_view(request):
    # Используется для создания уникального кэша для каждого пользователя
    request.META['CACHE_KEY_PREFIX'] = f"user_{request.user.id}"
    # ...
    return response
```

### Кэширование шаблонов

```django
{% load cache %}

{# Кэширование фрагмента шаблона на 500 секунд #}
{% cache 500 sidebar %}
    {# Дорогостоящие вычисления для сайдбара #}
    {% include "expensive_sidebar.html" %}
{% endcache %}

{# Кэширование с переменными #}
{% cache 500 user_sidebar request.user.id %}
    {# Персонализированный сайдбар #}
    {% include "user_sidebar.html" with user=request.user %}
{% endcache %}
```

### Кэширование данных модели

```python
from django.core.cache import cache
from django.db import models

class CachedModel(models.Model):
    name = models.CharField(max_length=100)
    data = models.JSONField()
    
    def save(self, *args, **kwargs):
        # Инвалидация кэша при сохранении модели
        cache.delete(f"model_{self.pk}")
        super().save(*args, **kwargs)
    
    def delete(self, *args, **kwargs):
        # Инвалидация кэша при удалении модели
        cache.delete(f"model_{self.pk}")
        super().delete(*args, **kwargs)
    
    @classmethod
    def get_cached(cls, pk):
        """Получение модели из кэша или БД"""
        cache_key = f"model_{pk}"
        obj = cache.get(cache_key)
        
        if obj is None:
            try:
                obj = cls.objects.get(pk=pk)
                cache.set(cache_key, obj, timeout=3600)  # Кэшировать на 1 час
            except cls.DoesNotExist:
                return None
        
        return obj
```

### Кэширование запросов QuerySet

```python
from django.core.cache import cache

def get_top_products(category_id, limit=10):
    """Получает топовые продукты с кэшированием."""
    cache_key = f"top_products_{category_id}_{limit}"
    
    # Пытаемся получить из кэша
    products = cache.get(cache_key)
    
    if products is None:
        # Если в кэше нет, выполняем запрос
        products = Product.objects.filter(
            category_id=category_id,
            is_active=True
        ).order_by('-sales_count')[:limit]
        
        # Преобразуем QuerySet в список, чтобы избежать проблем с сериализацией
        products = list(products)
        
        # Сохраняем в кэш
        cache.set(cache_key, products, timeout=3600)  # 1 час
    
    return products
```

### Кэширование индивидуальных запросов

Django ORM включает встроенный механизм кэширования запросов с помощью `QuerySet.cache()`.

```python
# Django 4.1+
products = Product.objects.filter(category_id=category_id).cache()

# Или для более ранних версий с django-cachalot или django-cache-machine
from django_cachalot.decorators import cachalot_query

@cachalot_query
def get_products(category_id):
    return Product.objects.filter(category_id=category_id)
```

### Middleware для кэширования страниц

```python
# settings.py
MIDDLEWARE = [
    'django.middleware.cache.UpdateCacheMiddleware',  # Должен быть первым
    # ... другие middleware
    'django.middleware.cache.FetchFromCacheMiddleware',  # Должен быть последним
]

# Настройки кэширования страниц
CACHE_MIDDLEWARE_ALIAS = 'default'  # Используемый кэш
CACHE_MIDDLEWARE_SECONDS = 600  # Время кэширования (10 минут)
CACHE_MIDDLEWARE_KEY_PREFIX = 'site_cache'  # Префикс ключей
```

## Кэширование в Flask

Flask не имеет встроенной системы кэширования, но предоставляет несколько расширений для этой цели.

### Flask-Caching

Flask-Caching — популярное расширение для реализации различных стратегий кэширования.

```python
from flask import Flask
from flask_caching import Cache

app = Flask(__name__)

# Настройка кэширования
config = {
    "DEBUG": True,
    "CACHE_TYPE": "SimpleCache",  # Простой кэш в памяти
    "CACHE_DEFAULT_TIMEOUT": 300  # 5 минут
}
app.config.from_mapping(config)
cache = Cache(app)

# Примеры других типов кэша
'''
# Memcached
config = {
    "CACHE_TYPE": "MemcachedCache",
    "CACHE_MEMCACHED_SERVERS": ["127.0.0.1:11211"],
    "CACHE_DEFAULT_TIMEOUT": 300
}

# Redis
config = {
    "CACHE_TYPE": "RedisCache",
    "CACHE_REDIS_HOST": "127.0.0.1",
    "CACHE_REDIS_PORT": 6379,
    "CACHE_REDIS_DB": 0,
    "CACHE_DEFAULT_TIMEOUT": 300
}

# Файловый кэш
config = {
    "CACHE_TYPE": "FileSystemCache",
    "CACHE_DIR": "/tmp/flask-cache",
    "CACHE_DEFAULT_TIMEOUT": 300
}
'''

# Кэширование представлений
@app.route('/')
@cache.cached(timeout=60)  # Кэшировать на 1 минуту
def index():
    # Дорогостоящая операция
    return render_template('index.html')

# Кэширование с динамическими ключами
@app.route('/user/<user_id>')
@cache.cached(timeout=60, key_prefix='user_view_%s')
def user_view(user_id):
    # Кэш будет уникальным для каждого user_id
    return render_template('user.html', user=get_user(user_id))

# Ручное кэширование функций
@cache.memoize(timeout=50)
def get_user(user_id):
    # Результат кэшируется с учетом аргументов
    return User.query.get(user_id)

# Кэширование с условием
@app.route('/post/<post_id>')
def post_view(post_id):
    # Использовать кэш только для GET-запросов к определенным постам
    if request.method == 'GET' and int(post_id) < 1000:
        cache_key = f'post_{post_id}'
        content = cache.get(cache_key)
        if content is None:
            content = get_post_content(post_id)
            cache.set(cache_key, content, timeout=3600)
        return content
    else:
        return get_post_content(post_id)

# Инвалидация кэша
@app.route('/user/<user_id>/update', methods=['POST'])
def update_user(user_id):
    # Обновление данных пользователя
    update_user_in_db(user_id, request.form)
    
    # Инвалидация кэша для этого пользователя
    cache.delete_memoized(get_user, user_id)
    cache.delete(f'user_view_{user_id}')
    
    return redirect(url_for('user_view', user_id=user_id))

# Очистка всего кэша
@app.route('/admin/clear-cache', methods=['POST'])
def clear_cache():
    cache.clear()
    return 'Cache cleared!'
```

### Кэширование функций и методов

```python
from flask import Flask
from flask_caching import Cache
from functools import wraps

app = Flask(__name__)
cache = Cache(app, config={'CACHE_TYPE': 'SimpleCache'})

# Базовое кэширование функции
@cache.memoize(timeout=60)
def get_data(param1, param2):
    # Результат кэшируется с учетом параметров
    return expensive_operation(param1, param2)

# Кэширование с условием
def conditional_cached(timeout=50, unless=None):
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            if unless and unless():
                return f(*args, **kwargs)
            
            return cache.memoize(timeout=timeout)(f)(*args, **kwargs)
        return decorated_function
    return decorator

@conditional_cached(timeout=60, unless=lambda: 'no-cache' in request.args)
def get_conditional_data():
    return expensive_operation()

# Кэширование методов класса
class User:
    @cache.memoize(timeout=50)
    def get_permissions(self, role_id):
        # Медленное получение разрешений для роли
        return db.session.query(Permission).filter_by(role_id=role_id).all()

    def clear_cache(self):
        # Инвалидация кэша для этого пользователя
        cache.delete_memoized(self.get_permissions)
```

### Кэширование запросов к API

```python
from flask import Flask, jsonify
from flask_caching import Cache
import requests

app = Flask(__name__)
cache = Cache(app, config={'CACHE_TYPE': 'SimpleCache'})

# Кэширование запросов к внешнему API
@cache.memoize(timeout=300)
def fetch_from_api(endpoint, params=None):
    response = requests.get(f"https://api.example.com/{endpoint}", params=params)
    return response.json()

@app.route('/api/products')
def get_products():
    category = request.args.get('category', 'all')
    page = request.args.get('page', 1)
    
    # Данные из внешнего API с кэшированием
    data = fetch_from_api('products', {'category': category, 'page': page})
    
    return jsonify(data)

# Инвалидация кэша для определенного эндпоинта
@app.route('/api/refresh', methods=['POST'])
def refresh_cache():
    endpoint = request.json.get('endpoint')
    cache.delete_memoized(fetch_from_api, endpoint)
    return jsonify({'status': 'success'})
```

### HTTP-кэширование в Flask

```python
from flask import Flask, make_response
from datetime import datetime, timedelta

app = Flask(__name__)

@app.route('/static-content')
def static_content():
    content = get_static_content()
    
    response = make_response(content)
    
    # Настройка HTTP-кэширования
    # Cache-Control
    response.headers['Cache-Control'] = 'public, max-age=3600'  # 1 час
    
    # Expires
    expires = datetime.now() + timedelta(hours=1)
    response.headers['Expires'] = expires.strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    # ETag
    response.headers['ETag'] = generate_etag(content)
    
    # Last-Modified
    last_modified = get_last_modified_time()
    response.headers['Last-Modified'] = last_modified.strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    return response

@app.route('/api/data')
def get_data():
    # Проверка условных запросов
    if_none_match = request.headers.get('If-None-Match')
    if_modified_since = request.headers.get('If-Modified-Since')
    
    etag = generate_etag(current_data)
    last_modified = get_last_modified_time()
    
    # Если данные не изменились, вернуть 304 Not Modified
    if if_none_match and if_none_match == etag:
        return '', 304
    
    if if_modified_since:
        client_time = datetime.strptime(if_modified_since, '%a, %d %b %Y %H:%M:%S GMT')
        if client_time >= last_modified:
            return '', 304
    
    # Данные изменились или первый запрос, вернуть актуальные данные
    response = make_response(jsonify(current_data))
    response.headers['Cache-Control'] = 'private, max-age=60'  # 1 минута
    response.headers['ETag'] = etag
    response.headers['Last-Modified'] = last_modified.strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    return response
```

## Кэширование в FastAPI

FastAPI не имеет встроенного решения для кэширования, но его можно реализовать с помощью различных библиотек и подходов.

### Использование FastAPI-Cache

FastAPI-Cache — расширение для кэширования ответов в FastAPI.

```python
from fastapi import FastAPI, Depends
from fastapi_cache import FastAPICache
from fastapi_cache.backends.redis import RedisBackend
from fastapi_cache.decorator import cache
from redis import asyncio as aioredis
import time

app = FastAPI()

@app.on_event("startup")
async def startup():
    # Инициализация Redis-кэша
    redis = aioredis.from_url("redis://localhost", encoding="utf8", decode_responses=True)
    FastAPICache.init(RedisBackend(redis), prefix="fastapi-cache")

# Кэширование эндпоинта на 60 секунд
@app.get("/users/{user_id}")
@cache(expire=60)
async def get_user(user_id: int):
    # Симуляция долгой операции
    time.sleep(1)
    return {"user_id": user_id, "name": f"User {user_id}"}

# Кэширование с переменным TTL
@app.get("/products")
@cache(expire=60, namespace="products")
async def get_products(category: str = "all", ttl: int = 60):
    # Динамическая установка TTL через параметр
    # Namespace позволяет группировать связанные кэши
    return {"category": category, "products": []}

# Кэширование с ключевыми зависимостями
@app.get("/user-data/{user_id}")
@cache(expire=60, namespace="user-data")
async def get_user_data(user_id: int, include_profile: bool = False):
    # Кэш будет учитывать оба параметра (user_id и include_profile)
    # ...
    return {"user_id": user_id, "profile": {} if include_profile else None}

# Инвалидация кэша
@app.post("/users/{user_id}")
async def update_user(user_id: int, user_data: dict):
    # Обновление пользователя
    # ...
    
    # Инвалидация кэша
    await FastAPICache.clear(namespace="users")
    
    return {"status": "updated"}
```

### Ручное кэширование с Redis

```python
from fastapi import FastAPI, Depends
from redis import asyncio as aioredis
import json
import pickle

app = FastAPI()

# Зависимость для получения Redis-соединения
async def get_redis():
    redis = aioredis.from_url("redis://localhost", encoding="utf8", decode_responses=True)
    try:
        yield redis
    finally:
        await redis.close()

# Функция для кэширования
async def cache_response(redis, key, response, expire=60):
    await redis.set(key, json.dumps(response), ex=expire)

# Функция для получения из кэша
async def get_cached_response(redis, key):
    cached = await redis.get(key)
    if cached:
        return json.loads(cached)
    return None

# Использование в эндпоинте
@app.get("/users/{user_id}")
async def get_user(user_id: int, redis = Depends(get_redis)):
    # Формирование ключа кэша
    cache_key = f"user:{user_id}"
    
    # Попытка получить из кэша
    cached_user = await get_cached_response(redis, cache_key)
    if cached_user:
        return cached_user
    
    # Если в кэше нет, получаем и кэшируем
    user_data = await fetch_user_data(user_id)
    await cache_response(redis, cache_key, user_data, expire=300)  # 5 минут
    
    return user_data

# Кэширование бинарных данных
async def cache_binary(redis, key, data, expire=60):
    # Для бинарных данных используем pickle и отключаем decode_responses
    r = await aioredis.from_url("redis://localhost", decode_responses=False)
    await r.set(key, pickle.dumps(data), ex=expire)
    await r.close()

async def get_cached_binary(redis, key):
    r = await aioredis.from_url("redis://localhost", decode_responses=False)
    cached = await r.get(key)
    await r.close()
    
    if cached:
        return pickle.loads(cached)
    return None
```

### HTTP-кэширование с использованием Starlette

FastAPI построен на Starlette, который предоставляет инструменты для HTTP-кэширования.

```python
from fastapi import FastAPI, Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
from datetime import datetime, timedelta
import hashlib
import time

app = FastAPI()

# Middleware для HTTP-кэширования
class CacheControlMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        
        # Добавление заголовков кэширования для GET-запросов
        if request.method == "GET":
            response.headers["Cache-Control"] = "public, max-age=60"  # 1 минута
        
        return response

app.add_middleware(CacheControlMiddleware)

# Эндпоинт с настройкой ETag
@app.get("/api/data")
async def get_data(response: Response, version: int = 1):
    data = {"version": version, "items": get_data_items()}
    
    # Генерация ETag на основе данных
    data_str = str(data)
    etag = hashlib.md5(data_str.encode()).hexdigest()
    
    # Установка ETag
    response.headers["ETag"] = etag
    
    # Проверка If-None-Match
    if_none_match = request.headers.get("If-None-Match")
    if if_none_match and if_none_match == etag:
        return Response(status_code=304)  # Not Modified
    
    return data

# Эндпоинт с настройкой Last-Modified
@app.get("/api/resources")
async def get_resources(response: Response):
    resources = get_resources_data()
    last_modified = get_last_update_time()
    
    # Установка Last-Modified
    response.headers["Last-Modified"] = last_modified.strftime("%a, %d %b %Y %H:%M:%S GMT")
    
    # Проверка If-Modified-Since
    if_modified_since = request.headers.get("If-Modified-Since")
    if if_modified_since:
        client_time = datetime.strptime(if_modified_since, "%a, %d %b %Y %H:%M:%S GMT")
        if client_time >= last_modified:
            return Response(status_code=304)  # Not Modified
    
    return resources
```

### Кэширование межкомпонентной связи

```python
from fastapi import FastAPI, Depends
from redis import asyncio as aioredis
import json
import httpx
import time

app = FastAPI()

# Зависимость для Redis
async def get_redis():
    redis = aioredis.from_url("redis://localhost", encoding="utf8", decode_responses=True)
    try:
        yield redis
    finally:
        await redis.close()

# Кэширование запросов к микросервисам
async def cached_service_request(service_name, endpoint, params=None, expire=60, redis=None):
    # Формирование ключа кэша
    param_str = json.dumps(params) if params else ""
    cache_key = f"service:{service_name}:{endpoint}:{param_str}"
    
    # Проверка кэша
    if redis:
        cached = await redis.get(cache_key)
        if cached:
            return json.loads(cached)
    
    # Выполнение запроса
    async with httpx.AsyncClient() as client:
        base_url = get_service_url(service_name)
        response = await client.get(f"{base_url}/{endpoint}", params=params)
        response.raise_for_status()
        data = response.json()
    
    # Кэширование результата
    if redis:
        await redis.set(cache_key, json.dumps(data), ex=expire)
    
    return data

@app.get("/aggregated-data")
async def get_aggregated_data(redis = Depends(get_redis)):
    # Получение данных из нескольких сервисов с кэшированием
    start_time = time.time()
    
    try:
        # Параллельный сбор данных из разных сервисов
        import asyncio
        user_data, product_data, order_data = await asyncio.gather(
            cached_service_request("user-service", "users", expire=300, redis=redis),
            cached_service_request("product-service", "products", expire=600, redis=redis),
            cached_service_request("order-service", "orders", expire=60, redis=redis)
        )
        
        # Агрегирование данных
        result = {
            "users": len(user_data),
            "products": len(product_data),
            "orders": len(order_data),
            # ... дополнительная обработка
        }
        
        execution_time = time.time() - start_time
        result["execution_time"] = execution_time
        
        return result
    except Exception as e:
        return {"error": str(e)}
```

## Memcached

Memcached — высокопроизводительная распределенная система кэширования в оперативной памяти, идеально подходящая для хранения небольших фрагментов произвольных данных.

### Установка и настройка

```bash
# Ubuntu/Debian
sudo apt-get install memcached

# CentOS/RHEL
sudo yum install memcached

# macOS
brew install memcached

# Запуск сервера
memcached -d -m 64 -p 11211 -u memcache
```

### Использование в Python

```python
import pymemcache
import json

# Сериализатор для сложных типов данных
class JsonSerde:
    def serialize(self, key, value):
        if isinstance(value, str):
            return value.encode('utf-8'), 1
        return json.dumps(value).encode('utf-8'), 2
    
    def deserialize(self, key, value, flags):
        if flags == 1:
            return value.decode('utf-8')
        if flags == 2:
            return json.loads(value.decode('utf-8'))
        return value

# Подключение к серверу
client = pymemcache.client.base.Client(('localhost', 11211), serde=JsonSerde())

# Базовые операции
client.set('key', 'value', expire=60)  # Сохранение с TTL 60 секунд
value = client.get('key')  # Получение значения

# Сохранение сложных данных
client.set('user', {'id': 1, 'name': 'John', 'email': 'john@example.com'})
user = client.get('user')  # Автоматически десериализуется

# Счетчики
client.set('counter', 0)
client.incr('counter', 1)  # Инкремент на 1
current = client.get('counter')

# Множественные операции
client.set_multi({
    'key1': 'value1',
    'key2': 'value2',
    'key3': 'value3'
})
values = client.get_multi(['key1', 'key2', 'key3'])

# Условные операции
client.add('new_key', 'value')  # Успешно только если ключ не существует
client.replace('existing_key', 'new_value')  # Успешно только если ключ существует

# Удаление
client.delete('key')

# Очистка всего кэша
client.flush_all()

# Статистика
stats = client.stats()
```

### Пул соединений

```python
from pymemcache.client.hash import HashClient

# Создание пула серверов
servers = [
    ('192.168.1.1', 11211),
    ('192.168.1.2', 11211),
    ('192.168.1.3', 11211)
]
client = HashClient(servers, serde=JsonSerde())

# Теперь операции распределятся между серверами
client.set('key', 'value')
value = client.get('key')
```

### Кэширование запросов к БД

```python
import pymemcache
import json
import hashlib
from functools import wraps

# Создание клиента
client = pymemcache.client.base.Client(('localhost', 11211))

# Декоратор для кэширования запросов
def cache_query(timeout=300):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Создание хеша для ключа кэша
            key_parts = [func.__name__]
            key_parts.extend([str(arg) for arg in args])
            key_parts.extend([f"{k}:{v}" for k, v in sorted(kwargs.items())])
            key = hashlib.md5(":".join(key_parts).encode()).hexdigest()
            
            # Попытка получить из кэша
            cached = client.get(key)
            if cached:
                return json.loads(cached.decode('utf-8'))
            
            # Выполнение запроса
            result = func(*args, **kwargs)
            
            # Сохранение в кэш
            client.set(key, json.dumps(result).encode('utf-8'), expire=timeout)
            
            return result
        return wrapper
    return decorator

# Использование декоратора
@cache_query(timeout=600)
def get_user_orders(user_id):
    # Здесь запрос к БД
    return db.execute("SELECT * FROM orders WHERE user_id = %s", (user_id,)).fetchall()
```

### Управление устареванием кэша

```python
import pymemcache
import time
import json

client = pymemcache.client.base.Client(('localhost', 11211))

class CacheManager:
    def __init__(self, cache_client):
        self.cache = cache_client
    
    def get_with_timestamp(self, key):
        """Получает значение с временной меткой для определения свежести."""
        value = self.cache.get(f"{key}:data")
        timestamp = self.cache.get(f"{key}:timestamp")
        
        if value is None or timestamp is None:
            return None, None
        
        return json.loads(value.decode('utf-8')), float(timestamp.decode('utf-8'))
    
    def set_with_timestamp(self, key, value, expire=None):
        """Сохраняет значение вместе с временной меткой."""
        now = time.time()
        
        self.cache.set(f"{key}:data", json.dumps(value).encode('utf-8'), expire=expire)
        self.cache.set(f"{key}:timestamp", str(now).encode('utf-8'), expire=expire)
        
        return True
    
    def get_or_refresh(self, key, refresh_func, expire=300, refresh_threshold=60):
        """
        Получает значение из кэша или обновляет его, если оно устарело.
        
        Args:
            key: Ключ кэша
            refresh_func: Функция для получения новых данных
            expire: Время жизни кэша в секундах
            refresh_threshold: Пороговое значение устаревания в секундах
        """
        value, timestamp = self.get_with_timestamp(key)
        now = time.time()
        
        # Если данных нет или они слишком старые, обновляем
        if value is None or now - timestamp > refresh_threshold:
            try:
                new_value = refresh_func()
                self.set_with_timestamp(key, new_value, expire=expire)
                return new_value
            except Exception as e:
                # В случае ошибки при обновлении, возвращаем старые данные
                if value is not None:
                    return value
                raise e
        
        return value

# Использование
cache_manager = CacheManager(client)

def get_current_weather(city):
    # Функция получения данных с внешнего API
    weather_data = fetch_from_weather_api(city)
    return weather_data

# Получение данных с автоматическим обновлением
weather = cache_manager.get_or_refresh(
    f"weather:{city}",
    lambda: get_current_weather(city),
    expire=1800,  # 30 минут TTL
    refresh_threshold=600  # Обновлять если старше 10 минут
)
```

## Redis

Redis — хранилище структур данных в памяти, которое может использоваться как база данных, кэш, брокер сообщений и многое другое.

### Установка и настройка

```bash
# Ubuntu/Debian
sudo apt-get install redis-server

# CentOS/RHEL
sudo yum install redis

# macOS
brew install redis

# Запуск сервера
redis-server /etc/redis/redis.conf
```

### Использование с redis-py

```python
import redis
import json
import pickle

# Подключение к Redis
r = redis.Redis(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=True  # Автоматическое декодирование ответов в строки
)

# Базовые операции с строками
r.set('key', 'value', ex=60)  # Сохранение с TTL 60 секунд
value = r.get('key')  # Получение значения

# Работа с JSON
user_data = {'id': 1, 'name': 'John', 'email': 'john@example.com'}
r.set('user:1', json.dumps(user_data))
user = json.loads(r.get('user:1'))

# Работа с бинарными данными
binary_r = redis.Redis(host='localhost', port=6379, db=0)  # без decode_responses
complex_obj = {'matrix': [[1, 2], [3, 4]], 'binary': b'\x00\x01\x02\x03'}
binary_r.set('binary_data', pickle.dumps(complex_obj))
loaded_obj = pickle.loads(binary_r.get('binary_data'))

# Структуры данных Redis

# Хеши (словари)
r.hset('user:2', mapping={'name': 'Alice', 'email': 'alice@example.com', 'age': '30'})
r.hset('user:2', 'role', 'admin')  # Добавление отдельного поля
name = r.hget('user:2', 'name')
user_fields = r.hgetall('user:2')

# Списки
r.rpush('list_key', 'item1', 'item2', 'item3')  # Добавление в конец
r.lpush('list_key', 'item0')  # Добавление в начало
r.lrange('list_key', 0, -1)  # Получение всех элементов
r.lpop('list_key')  # Удаление и возврат первого элемента
r.rpop('list_key')  # Удаление и возврат последнего элемента

# Множества
r.sadd('set_key', 'item1', 'item2', 'item3')
r.sismember('set_key', 'item1')  # Проверка наличия элемента
r.smembers('set_key')  # Получение всех элементов
r.srem('set_key', 'item1')  # Удаление элемента

# Упорядоченные множества (сортированные наборы)
r.zadd('leaderboard', {'user1': 100, 'user2': 150, 'user3': 75})
r.zscore('leaderboard', 'user1')  # Получение счета
r.zrange('leaderboard', 0, -1, withscores=True)  # Получение всех элементов
r.zrevrange('leaderboard', 0, 2, withscores=True)  # Топ-3

# Время жизни ключей
r.expire('key', 300)  # Установка TTL в секундах
ttl = r.ttl('key')  # Получение оставшегося TTL

# Атомарные операции
r.incr('counter')  # Инкремент на 1
r.incrby('counter', 10)  # Инкремент на 10
r.decr('counter')  # Декремент на 1
```

### Использование Redis для кэширования

```python
import redis
import json
import time
import hashlib
from functools import wraps

# Подключение к Redis
r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

# Декоратор для кэширования функций
def redis_cache(prefix='cache', expire=300):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Создание хеша для ключа кэша
            key_parts = [prefix, func.__name__]
            key_parts.extend([str(arg) for arg in args])
            key_parts.extend([f"{k}:{v}" for k, v in sorted(kwargs.items())])
            cache_key = hashlib.md5(":".join(key_parts).encode()).hexdigest()
            
            # Попытка получить из кэша
            cached = r.get(cache_key)
            if cached:
                return json.loads(cached)
            
            # Выполнение функции
            result = func(*args, **kwargs)
            
            # Сохранение в кэш
            r.set(cache_key, json.dumps(result), ex=expire)
            
            return result
        return wrapper
    return decorator

# Использование декоратора
@redis_cache(prefix='weather', expire=600)
def get_weather(city):
    # Имитация запроса к API
    time.sleep(1)  # Задержка для демонстрации
    return {"city": city, "temp": 25, "conditions": "sunny"}

# Функция для инвалидации кэша
def invalidate_cache(prefix, func_name, *args, **kwargs):
    key_parts = [prefix, func_name]
    key_parts.extend([str(arg) for arg in args])
    key_parts.extend([f"{k}:{v}" for k, v in sorted(kwargs.items())])
    cache_key = hashlib.md5(":".join(key_parts).encode()).hexdigest()
    r.delete(cache_key)
```

### Redis Pipeline для пакетной обработки

```python
import redis
import time

r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

# Использование pipeline для множественных операций
def bulk_update_with_pipeline(user_ids, data):
    pipeline = r.pipeline()
    
    for user_id in user_ids:
        key = f"user:{user_id}"
        pipeline.hset(key, mapping=data)
        pipeline.expire(key, 3600)  # TTL 1 час
    
    # Выполнение всех команд за один сетевой запрос
    results = pipeline.execute()
    return results

# Транзакции в Redis
def atomic_operation():
    pipeline = r.pipeline(transaction=True)  # Включение транзакции
    
    try:
        # Все команды будут выполнены атомарно
        pipeline.incr('counter')
        pipeline.sadd('active_users', 'user123')
        pipeline.hset('last_active', 'user123', int(time.time()))
        
        results = pipeline.execute()
        return results
    except redis.exceptions.RedisError as e:
        # Обработка ошибок
        return None
```

### Паттерны использования Redis

```python
import redis
import json
import time

r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

# Паттерн: Кэширование с версионностью
def versioned_cache_set(key, data, expire=300):
    version = r.incr(f"{key}:version")
    r.set(f"{key}:v{version}", json.dumps(data), ex=expire)
    return version

def versioned_cache_get(key, version=None):
    if version is None:
        version = r.get(f"{key}:version")
        if version is None:
            return None
    
    cached = r.get(f"{key}:v{version}")
    if cached:
        return json.loads(cached)
    return None

# Паттерн: Блокировка для предотвращения гонки данных
def with_lock(lock_name, timeout=10, expiry=30):
    lock_key = f"lock:{lock_name}"
    
    # Генерация уникального токена для этой блокировки
    lock_token = f"{time.time()}-{time.monotonic()}"
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Попытка получить блокировку
            acquired = r.set(lock_key, lock_token, nx=True, ex=expiry)
            
            if not acquired:
                # Если блокировка не получена, ждем и повторяем попытку
                for _ in range(timeout):
                    time.sleep(1)
                    acquired = r.set(lock_key, lock_token, nx=True, ex=expiry)
                    if acquired:
                        break
                else:
                    raise TimeoutError(f"Could not acquire lock: {lock_name}")
            
            try:
                # Выполнение функции под блокировкой
                return func(*args, **kwargs)
            finally:
                # Освобождение блокировки (только если это наша блокировка)
                if r.get(lock_key) == lock_token:
                    r.delete(lock_key)
        
        return wrapper
    
    return decorator

# Паттерн: Pub/Sub для инвалидации кэша
def setup_cache_invalidation():
    # Создаем отдельное подключение для подписки
    pubsub = r.pubsub()
    
    # Подписываемся на канал инвалидации
    pubsub.subscribe('cache_invalidation')
    
    # Запускаем обработку сообщений в отдельном потоке
    def message_handler():
        for message in pubsub.listen():
            if message['type'] == 'message':
                data = json.loads(message['data'])
                if 'pattern' in data:
                    # Удаление ключей по шаблону
                    for key in r.keys(data['pattern']):
                        r.delete(key)
                elif 'key' in data:
                    # Удаление конкретного ключа
                    r.delete(data['key'])
    
    import threading
    thread = threading.Thread(target=message_handler, daemon=True)
    thread.start()

# Инвалидация кэша через Pub/Sub
def invalidate_cache_key(key):
    r.publish('cache_invalidation', json.dumps({'key': key}))

def invalidate_cache_pattern(pattern):
    r.publish('cache_invalidation', json.dumps({'pattern': pattern}))
```

## Локальное кэширование

Локальное кэширование хранит данные в памяти процесса, что обеспечивает максимальную скорость доступа.

### Использование встроенного functools.lru_cache

```python
from functools import lru_cache
import time

# Простой кэш с ограничением по размеру
@lru_cache(maxsize=128)
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Кэш с TTL
def timed_lru_cache(maxsize=128, typed=False, ttl_seconds=60):
    """Декоратор LRU-кэша с временем жизни."""
    def wrapper_cache(func):
        func = lru_cache(maxsize=maxsize, typed=typed)(func)
        func.lifetime = ttl_seconds
        func.expiration = time.time() + ttl_seconds
        
        def wrapped_func(*args, **kwargs):
            if time.time() > func.expiration:
                func.cache_clear()
                func.expiration = time.time() + func.lifetime
            
            return func(*args, **kwargs)
        
        wrapped_func.cache_info = func.cache_info
        wrapped_func.cache_clear = func.cache_clear
        
        return wrapped_func
    
    return wrapper_cache

@timed_lru_cache(ttl_seconds=300)
def get_data(param):
    """Функция с кэшем, который автоматически очищается через 5 минут."""
    # Тяжелая операция
    time.sleep(1)
    return f"Data for {param}: {time.time()}"
```

### Использование cachetools

cachetools — библиотека, предоставляющая различные типы кэшей для Python.

```python
import cachetools
import time
from functools import wraps

# LRU-кэш (Least Recently Used)
lru_cache = cachetools.LRUCache(maxsize=100)

# TTL-кэш (Time-To-Live)
ttl_cache = cachetools.TTLCache(maxsize=100, ttl=300)  # 5 минут

# LFU-кэш (Least Frequently Used)
lfu_cache = cachetools.LFUCache(maxsize=100)

# Декоратор с cachetools
def cached(cache, key=lambda *args, **kwargs: cachetools.keys.hashkey(*args, **kwargs)):
    """Декоратор для кэширования функций с произвольным кэшем."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            k = key(*args, **kwargs)
            try:
                return cache[k]
            except KeyError:
                pass  # Ключ не найден
            
            v = func(*args, **kwargs)
            try:
                cache[k] = v
            except ValueError:
                pass  # Кэш переполнен
            return v
        return wrapper
    return decorator

# Использование декоратора с различными типами кэшей
@cached(ttl_cache)
def get_weather(city):
    """Функция с TTL-кэшем на 5 минут."""
    # Тяжелая операция
    time.sleep(1)
    return {"city": city, "temp": 25, "conditions": "sunny"}

@cached(lru_cache)
def get_user_profile(user_id):
    """Функция с LRU-кэшем."""
    # Тяжелая операция
    time.sleep(1)
    return {"id": user_id, "name": f"User {user_id}"}

# Собственная функция ключа для сложных аргументов
def custom_key_func(*args, **kwargs):
    """Создает ключ кэша на основе части аргументов."""
    # Используем только первый аргумент и параметр 'type'
    return cachetools.keys.hashkey(args[0] if args else None, kwargs.get('type'))

@cached(lfu_cache, key=custom_key_func)
def search_items(query, type=None, page=1, limit=10):
    """Функция с LFU-кэшем и пользовательской функцией ключа."""
    # Тяжелая операция
    time.sleep(1)
    return {"query": query, "type": type, "results": [f"Item {i}" for i in range(limit)]}
```

### Создание собственного кэша

```python
import time
import threading
from collections import OrderedDict

class CustomCache:
    """
    Простая реализация кэша с поддержкой TTL и ограничением размера.
    """
    def __init__(self, maxsize=100, ttl=None):
        self.maxsize = maxsize
        self.ttl = ttl
        self.cache = OrderedDict()  # Для поддержки LRU
        self.expires = {}  # Для хранения времени истечения
        self.lock = threading.RLock()  # Для потокобезопасности
    
    def get(self, key, default=None):
        """Получение значения из кэша."""
        with self.lock:
            # Проверка на истечение срока действия
            if key in self.expires and time.time() > self.expires[key]:
                self.cache.pop(key, None)
                self.expires.pop(key, None)
                return default
            
            if key in self.cache:
                # Перемещение к концу для LRU
                value = self.cache.pop(key)
                self.cache[key] = value
                return value
            
            return default
    
    def set(self, key, value, ttl=None):
        """Установка значения в кэш."""
        with self.lock:
            # Если ключ уже существует, удаляем его
            if key in self.cache:
                self.cache.pop(key)
                self.expires.pop(key, None)
            
            # Проверка на переполнение
            if len(self.cache) >= self.maxsize:
                # Удаление самого старого элемента (LRU)
                oldest_key, _ = self.cache.popitem(last=False)
                self.expires.pop(oldest_key, None)
            
            # Добавление нового элемента
            self.cache[key] = value
            
            # Установка времени истечения
            timeout = ttl if ttl is not None else self.ttl
            if timeout is not None:
                self.expires[key] = time.time() + timeout
    
    def delete(self, key):
        """Удаление ключа из кэша."""
        with self.lock:
            self.cache.pop(key, None)
            self.expires.pop(key, None)
    
    def clear(self):
        """Очистка всего кэша."""
        with self.lock:
            self.cache.clear()
            self.expires.clear()
    
    def __len__(self):
        """Количество элементов в кэше."""
        with self.lock:
            return len(self.cache)
    
    def __contains__(self, key):
        """Проверка наличия ключа в кэше."""
        with self.lock:
            if key not in self.cache:
                return False
            
            # Проверка на истечение срока действия
            if key in self.expires and time.time() > self.expires[key]:
                self.cache.pop(key, None)
                self.expires.pop(key, None)
                return False
            
            return True

# Использование
cache = CustomCache(maxsize=100, ttl=300)
cache.set('key1', 'value1')
cache.set('key2', 'value2', ttl=60)  # Переопределение TTL

value1 = cache.get('key1')
if 'key2' in cache:
    # Ключ 'key2' существует и не истек
    pass
```

## Валидация и инвалидация кэша

### Инвалидация по времени (TTL)

```python
# Установка TTL в Redis
import redis
r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

# Установка TTL при создании ключа
r.set('key', 'value', ex=60)  # TTL = 60 секунд

# Установка TTL для существующего ключа
r.expire('key', 30)  # Изменение TTL на 30 секунд

# Получение оставшегося TTL
ttl = r.ttl('key')
```

### Инвалидация по событию

```python
# Инвалидация кэша при изменении данных
class Product(models.Model):
    name = models.CharField(max_length=100)
    price = models.DecimalField(max_digits=10, decimal_places=2)
    
    def save(self, *args, **kwargs):
        # Инвалидация кэша при сохранении модели
        cache_keys = [
            f'product:{self.pk}',
            'products:all',
            f'products:category:{self.category_id}'
        ]
        
        # Сохранение модели
        result = super().save(*args, **kwargs)
        
        # Удаление ключей из кэша
        from django.core.cache import cache
        cache.delete_many(cache_keys)
        
        return result
```

### Инвалидация по шаблону

```python
# Инвалидация кэша по шаблону в Redis
import redis
r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

# Получение всех ключей, соответствующих шаблону
keys = r.keys('user:*')

# Удаление всех найденных ключей
if keys:
    r.delete(*keys)

# Более эффективная инвалидация с использованием транзакции
def invalidate_pattern(pattern):
    pipe = r.pipeline()
    
    # Поиск всех ключей по шаблону
    keys = r.keys(pattern)
    
    if keys:
        # Удаление всех найденных ключей
        pipe.delete(*keys)
    
    # Выполнение команд
    pipe.execute()
```

### Версионное кэширование

```python
# Версионное кэширование предотвращает проблемы с устаревшими данными
import redis
import json

r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

class VersionedCache:
    def __init__(self, redis_client, namespace):
        self.redis = redis_client
        self.namespace = namespace
        
        # Создаем ключ версии, если он не существует
        if not self.redis.exists(f"{namespace}:version"):
            self.redis.set(f"{namespace}:version", 1)
    
    def get_version(self):
        """Получение текущей версии кэша."""
        return int(self.redis.get(f"{self.namespace}:version"))
    
    def increment_version(self):
        """Увеличение версии кэша."""
        return self.redis.incr(f"{self.namespace}:version")
    
    def get(self, key):
        """Получение значения с учетом версии."""
        version = self.get_version()
        value = self.redis.get(f"{self.namespace}:{version}:{key}")
        
        if value:
            return json.loads(value)
        return None
    
    def set(self, key, value, expire=None):
        """Установка значения с учетом версии."""
        version = self.get_version()
        self.redis.set(
            f"{self.namespace}:{version}:{key}",
            json.dumps(value),
            ex=expire
        )
    
    def invalidate_all(self):
        """Инвалидация всего кэша путем увеличения версии."""
        # Увеличение версии автоматически делает все прежние ключи неактуальными
        return self.increment_version()

# Использование
user_cache = VersionedCache(r, 'users')
user_cache.set('user:1', {'id': 1, 'name': 'John'})

# После изменения данных
user_cache.invalidate_all()  # Инвалидация всего кэша пользователей
```

### Стратегия "Записать в кэш, затем в БД"

```python
import redis
import json
import threading
import time

r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

class WriteBackCache:
    def __init__(self, redis_client, write_back_func, delay=5):
        self.redis = redis_client
        self.write_back_func = write_back_func
        self.delay = delay
        self.dirty_keys = set()
        self.lock = threading.Lock()
        
        # Запуск фонового потока для записи данных
        self.should_run = True
        self.thread = threading.Thread(target=self._write_back_worker, daemon=True)
        self.thread.start()
    
    def set(self, key, value, mark_dirty=True):
        """Запись в кэш и отметка ключа как "грязного"."""
        self.redis.set(key, json.dumps(value))
        
        if mark_dirty:
            with self.lock:
                self.dirty_keys.add(key)
    
    def get(self, key, default=None):
        """Получение значения из кэша."""
        value = self.redis.get(key)
        if value:
            return json.loads(value)
        return default
    
    def write_back(self, key):
        """Запись конкретного ключа в постоянное хранилище."""
        value = self.get(key)
        if value:
            self.write_back_func(key, value)
            
            with self.lock:
                if key in self.dirty_keys:
                    self.dirty_keys.remove(key)
    
    def _write_back_worker(self):
        """Фоновый поток для периодической записи "грязных" ключей."""
        while self.should_run:
            # Копирование набора "грязных" ключей для безопасной итерации
            with self.lock:
                keys_to_write = self.dirty_keys.copy()
            
            # Запись всех "грязных" ключей
            for key in keys_to_write:
                try:
                    self.write_back(key)
                except Exception as e:
                    print(f"Error writing back key {key}: {e}")
            
            time.sleep(self.delay)
    
    def flush_all(self):
        """Принудительная запись всех "грязных" ключей."""
        with self.lock:
            keys_to_write = self.dirty_keys.copy()
        
        for key in keys_to_write:
            self.write_back(key)
    
    def stop(self):
        """Остановка фонового потока и запись всех данных."""
        self.should_run = False
        self.thread.join()
        self.flush_all()

# Функция для записи в БД
def write_to_db(key, value):
    # Имитация записи в БД
    print(f"Writing to DB: {key} = {value}")
    time.sleep(0.1)  # Имитация задержки

# Использование
cache = WriteBackCache(r, write_to_db, delay=10)
cache.set('user:1', {'id': 1, 'name': 'John'})
cache.set('user:2', {'id': 2, 'name': 'Alice'})

# Данные будут автоматически записаны в БД через 10 секунд

# При завершении программы
cache.stop()  # Остановка потока и запись всех данных
```

## Паттерны кэширования

### Cache-Aside (Lazy Loading)

```python
from django.core.cache import cache

def get_user(user_id):
    """
    Паттерн Cache-Aside:
    1. Проверка кэша
    2. Если данные есть, возврат из кэша
    3. Если данных нет, запрос из БД
    4. Запись данных в кэш
    5. Возврат данных
    """
    cache_key = f"user:{user_id}"
    
    # Попытка получить из кэша
    user = cache.get(cache_key)
    
    if user is None:
        try:
            # Данных нет в кэше, получаем из БД
            user = User.objects.get(id=user_id)
            
            # Сохраняем в кэш
            cache.set(cache_key, user, timeout=3600)  # 1 час
        except User.DoesNotExist:
            # Пользователь не найден
            return None
    
    return user
```

### Read-Through / Write-Through

```python
class CacheManager:
    def __init__(self, cache_client, db_client):
        self.cache = cache_client
        self.db = db_client
    
    def get(self, key):
        """
        Паттерн Read-Through:
        Кэш автоматически загружает данные из БД при промахе.
        """
        value = self.cache.get(key)
        
        if value is None:
            # Промах кэша, загрузка из БД
            value = self.db.get(key)
            if value is not None:
                # Сохранение в кэш
                self.cache.set(key, value)
        
        return value
    
    def set(self, key, value):
        """
        Паттерн Write-Through:
        Данные записываются одновременно в кэш и в БД.
        """
        # Запись в БД
        self.db.set(key, value)
        
        # Запись в кэш
        self.cache.set(key, value)
        
        return value
```

### Write-Behind (Write-Back)

```python
import threading
import queue
import time

class WriteBehindCache:
    def __init__(self, cache_client, db_client, batch_size=100, delay=5):
        self.cache = cache_client
        self.db = db_client
        self.batch_size = batch_size
        self.delay = delay
        
        # Очередь операций записи
        self.write_queue = queue.Queue()
        
        # Запуск потока для обработки записей
        self.should_run = True
        self.thread = threading.Thread(target=self._process_writes, daemon=True)
        self.thread.start()
    
    def get(self, key):
        """Получение данных только из кэша."""
        return self.cache.get(key)
    
    def set(self, key, value):
        """
        Паттерн Write-Behind:
        1. Запись в кэш
        2. Добавление операции в очередь для асинхронной записи в БД
        """
        # Запись в кэш
        self.cache.set(key, value)
        
        # Добавление в очередь записи
        self.write_queue.put(('set', key, value))
        
        return value
    
    def delete(self, key):
        """Удаление из кэша и добавление в очередь удаления из БД."""
        self.cache.delete(key)
        self.write_queue.put(('delete', key, None))
    
    def _process_writes(self):
        """Фоновый поток для обработки операций записи."""
        while self.should_run:
            batch = []
            
            # Сбор операций для пакетной обработки
            try:
                while len(batch) < self.batch_size:
                    # Блокирующее получение с таймаутом
                    operation = self.write_queue.get(timeout=self.delay)
                    batch.append(operation)
                    self.write_queue.task_done()
            except queue.Empty:
                # Таймаут, обрабатываем собранный пакет
                pass
            
            # Обработка пакета операций
            if batch:
                try:
                    self._process_batch(batch)
                except Exception as e:
                    print(f"Error processing batch: {e}")
    
    def _process_batch(self, batch):
        """Обработка пакета операций записи."""
        for operation, key, value in batch:
            try:
                if operation == 'set':
                    self.db.set(key, value)
                elif operation == 'delete':
                    self.db.delete(key)
            except Exception as e:
                print(f"Error processing operation {operation} for key {key}: {e}")
    
    def flush(self):
        """Принудительная обработка всех операций в очереди."""
        self.write_queue.join()
    
    def stop(self):
        """Остановка потока и обработка оставшихся операций."""
        self.should_run = False
        self.thread.join(timeout=self.delay*2)
        self.flush()
```

### Cache Stampede Prevention (Thundering Herd)

```python
import threading
import time
import functools
import hashlib
import random

# Глобальный словарь для хранения информации о вычислениях
_computations = {}
_computations_lock = threading.Lock()

def cache_with_lock(ttl=300, stale_ttl=30):
    """
    Декоратор для предотвращения кэш-стампедов (Cache Stampede).
    
    Args:
        ttl: Основное время жизни кэша в секундах
        stale_ttl: Время, в течение которого устаревший кэш может использоваться
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Создание ключа на основе функции и аргументов
            key_parts = [func.__name__]
            key_parts.extend([str(arg) for arg in args])
            key_parts.extend([f"{k}:{v}" for k, v in sorted(kwargs.items())])
            key = hashlib.md5(":".join(key_parts).encode()).hexdigest()
            
            now = time.time()
            
            with _computations_lock:
                computation_info = _computations.get(key)
                
                if computation_info:
                    # Если кэш еще действителен
                    if now < computation_info['valid_until']:
                        return computation_info['value']
                    
                    # Если кэш устарел, но еще можно использовать stale данные
                    if now < computation_info['stale_until']:
                        # Если уже идет обновление, возвращаем устаревшие данные
                        if computation_info['refreshing']:
                            return computation_info['value']
                        
                        # Случайная задержка для предотвращения одновременного обновления
                        # несколькими потоками/процессами
                        if random.random() < 0.1:  # 10% вероятность обновления
                            # Помечаем, что идет обновление
                            _computations[key]['refreshing'] = True
                        else:
                            # Возвращаем устаревшие данные
                            return computation_info['value']
                    
                # Если нет информации о вычислении или stale время истекло
                # или мы решили обновить кэш
                if key not in _computations or now >= computation_info['stale_until'] or \
                   (now >= computation_info['valid_until'] and computation_info['refreshing']):
                    # Создаем или обновляем запись
                    _computations[key] = {
                        'refreshing': True,
                        'valid_until': now + ttl,
                        'stale_until': now + ttl + stale_ttl,
                        'value': computation_info['value'] if key in _computations else None
                    }
            
            # Вычисление нового значения
            try:
                result = func(*args, **kwargs)
                
                with _computations_lock:
                    _computations[key]['value'] = result
                    _computations[key]['refreshing'] = False
                
                return result
            except Exception as e:
                # В случае ошибки, сбрасываем флаг refreshing
                with _computations_lock:
                    if key in _computations:
                        _computations[key]['refreshing'] = False
                raise e
            
        return wrapper
    return decorator

# Использование
@cache_with_lock(ttl=60, stale_ttl=30)
def expensive_calculation(param):
    """Дорогостоящее вычисление, защищенное от кэш-стампедов."""
    # Имитация долгой операции
    time.sleep(2)
    return f"Result for {param}: {time.time()}"
```

### Caching Proxy

```python
class CachingProxy:
    """
    Прокси-класс для объекта с кэшированием результатов методов.
    """
    def __init__(self, real_object, cache_client, ttl=300):
        self.real_object = real_object
        self.cache = cache_client
        self.ttl = ttl
    
    def __getattr__(self, name):
        # Получение реального атрибута/метода
        attr = getattr(self.real_object, name)
        
        # Если это не метод, возвращаем как есть
        if not callable(attr):
            return attr
        
        # Если это метод, создаем кэширующую обертку
        @functools.wraps(attr)
        def wrapper(*args, **kwargs):
            # Формирование ключа кэша
            key_parts = [self.real_object.__class__.__name__, name]
            key_parts.extend([str(arg) for arg in args])
            key_parts.extend([f"{k}:{v}" for k, v in sorted(kwargs.items())])
            cache_key = hashlib.md5(":".join(key_parts).encode()).hexdigest()
            
            # Проверка кэша
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Вызов реального метода
            result = attr(*args, **kwargs)
            
            # Сохранение результата в кэш
            self.cache.set(cache_key, result, ex=self.ttl)
            
            return result
        
        return wrapper

# Пример использования
class ExpensiveService:
    def get_data(self, id):
        # Имитация дорогого запроса
        time.sleep(1)
        return {"id": id, "data": f"Data for {id}"}
    
    def calculate(self, x, y):
        # Имитация сложных вычислений
        time.sleep(0.5)
        return x * y

# Создание прокси с кэшированием
service = ExpensiveService()
cached_service = CachingProxy(service, redis_client, ttl=60)

# Теперь вызовы методов будут кэшироваться
result1 = cached_service.get_data(1)  # Медленно (вызов реального метода)
result2 = cached_service.get_data(1)  # Быстро (из кэша)
```

## Производительность и масштабирование

### Стратегии масштабирования кэша

#### Репликация кэша

```python
from pymemcache.client.hash import HashClient

# Настройка пула серверов для репликации
servers = [
    ('cache1.example.com', 11211),
    ('cache2.example.com', 11211)
]

# Создание клиента с репликацией
client = HashClient(servers, use_pooling=True)

# Операции будут распределяться между серверами
client.set('key', 'value')
value = client.get('key')
```

#### Шардирование кэша

```python
import hashlib

class ShardedCache:
    """
    Клиент для шардированного кэша.
    """
    def __init__(self, cache_clients):
        self.cache_clients = cache_clients
        self.shard_count = len(cache_clients)
    
    def _get_shard(self, key):
        """Определение шарда для ключа."""
        # Хеширование ключа для равномерного распределения
        hash_value = int(hashlib.md5(str(key).encode()).hexdigest(), 16)
        return hash_value % self.shard_count
    
    def get(self, key):
        """Получение значения из соответствующего шарда."""
        shard_index = self._get_shard(key)
        return self.cache_clients[shard_index].get(key)
    
    def set(self, key, value, timeout=None):
        """Запись значения в соответствующий шард."""
        shard_index = self._get_shard(key)
        return self.cache_clients[shard_index].set(key, value, timeout)
    
    def delete(self, key):
        """Удаление ключа из соответствующего шарда."""
        shard_index = self._get_shard(key)
        return self.cache_clients[shard_index].delete(key)

# Пример использования
from django.core.cache import caches

# Создание клиентов для шардов
cache_clients = [
    caches['shard1'],
    caches['shard2'],
    caches['shard3']
]

# Инициализация шардированного кэша
sharded_cache = ShardedCache(cache_clients)

# Использование
sharded_cache.set('key1', 'value1')
value = sharded_cache.get('key1')
```

### Оптимизация производительности

#### Сжатие данных

```python
import gzip
import pickle
import base64

def compress_data(data):
    """Сжатие данных для хранения в кэше."""
    # Сериализация и сжатие данных
    serialized = pickle.dumps(data)
    compressed = gzip.compress(serialized)
    
    # Кодирование в base64 для безопасного хранения
    encoded = base64.b64encode(compressed).decode('utf-8')
    
    return encoded

def decompress_data(encoded_data):
    """Распаковка данных из кэша."""
    # Декодирование из base64
    decoded = base64.b64decode(encoded_data.encode('utf-8'))
    
    # Распаковка и десериализация
    decompressed = gzip.decompress(decoded)
    data = pickle.loads(decompressed)
    
    return data

# Использование с Redis
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

def cache_set_compressed(key, data, expire=None):
    """Сохранение сжатых данных в кэш."""
    compressed = compress_data(data)
    r.set(key, compressed, ex=expire)

def cache_get_compressed(key):
    """Получение и распаковка данных из кэша."""
    compressed = r.get(key)
    if compressed:
        return decompress_data(compressed.decode('utf-8'))
    return None
```

#### Пакетные операции

```python
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

def batch_get(keys):
    """Получение нескольких ключей за один запрос."""
    if not keys:
        return {}
    
    # Получение значений для всех ключей
    values = r.mget(keys)
    
    # Создание словаря {ключ: значение}
    result = {}
    for i, key in enumerate(keys):
        value = values[i]
        if value:
            result[key] = value.decode('utf-8')
    
    return result

def batch_set(data_dict, expire=None):
    """Установка нескольких ключей за один запрос."""
    if not data_dict:
        return
    
    # Использование pipeline для атомарной операции
    pipe = r.pipeline()
    
    for key, value in data_dict.items():
        pipe.set(key, value, ex=expire)
    
    # Выполнение всех команд за один запрос
    pipe.execute()
```

#### Асинхронное кэширование

```python
import asyncio
import aioredis

async def get_cache():
    """Получение соединения с Redis."""
    if not hasattr(get_cache, 'redis'):
        get_cache.redis = await aioredis.create_redis_pool(
            'redis://localhost',
            encoding='utf-8'
        )
    return get_cache.redis

async def async_cache_get(key):
    """Асинхронное получение значения из кэша."""
    redis = await get_cache()
    return await redis.get(key)

async def async_cache_set(key, value, expire=None):
    """Асинхронная запись значения в кэш."""
    redis = await get_cache()
    await redis.set(key, value, expire=expire)

async def async_cache_multi_get(keys):
    """Асинхронное получение нескольких значений из кэша."""
    redis = await get_cache()
    values = await redis.mget(*keys)
    return {key: value for key, value in zip(keys, values) if value is not None}

# Использование
async def main():
    # Запись в кэш
    await async_cache_set('key1', 'value1', expire=60)
    
    # Получение из кэша
    value = await async_cache_get('key1')
    print(f"Value: {value}")
    
    # Получение нескольких значений
    values = await async_cache_multi_get(['key1', 'key2', 'key3'])
    print(f"Multiple values: {values}")
    
    # Закрытие соединения
    redis = await get_cache()
    redis.close()
    await redis.wait_closed()

# Запуск асинхронного кода
asyncio.run(main())
```

## Кэширование в распределенных системах

### Распределенные блокировки

```python
import redis
import time
import uuid

class RedisLock:
    """
    Реализация распределенной блокировки на основе Redis.
    """
    def __init__(self, redis_client, lock_name, expire=10, retry_interval=0.1, retry_times=50):
        self.redis = redis_client
        self.lock_name = f"lock:{lock_name}"
        self.expire = expire
        self.retry_interval = retry_interval
        self.retry_times = retry_times
        self.lock_value = str(uuid.uuid4())  # Уникальный идентификатор блокировки
    
    def acquire(self):
        """Попытка получить блокировку."""
        for _ in range(self.retry_times):
            # Атомарная операция установки ключа, если он не существует
            if self.redis.set(self.lock_name, self.lock_value, nx=True, ex=self.expire):
                return True
            
            # Если не удалось получить блокировку, ждем и пробуем снова
            time.sleep(self.retry_interval)
        
        return False
    
    def release(self):
        """Освобождение блокировки."""
        # Используем скрипт Lua для атомарного удаления блокировки
        # Проверяем, что блокировка все еще принадлежит нам
        script = """
        if redis.call('get', KEYS[1]) == ARGV[1] then
            return redis.call('del', KEYS[1])
        else
            return 0
        end
        """
        
        # Выполнение скрипта
        return self.redis.eval(script, 1, self.lock_name, self.lock_value)
    
    def __enter__(self):
        """Для использования с конструкцией with."""
        if not self.acquire():
            raise TimeoutError(f"Could not acquire lock: {self.lock_name}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Автоматическое освобождение блокировки при выходе из with."""
        self.release()

# Использование
r = redis.Redis(host='localhost', port=6379, db=0)

def update_shared_resource(resource_id, new_value):
    """Обновление разделяемого ресурса с использованием блокировки."""
    # Получение блокировки
    lock_name = f"resource:{resource_id}"
    
    with RedisLock(r, lock_name, expire=30) as lock:
        # Блокировка получена, можно безопасно обновлять ресурс
        current_value = get_resource(resource_id)
        
        # Обновление ресурса
        set_resource(resource_id, new_value)
        
        return new_value
```

### Консистентное хеширование

```python
import hashlib
import bisect

class ConsistentHash:
    """
    Реализация консистентного хеширования для распределения ключей между узлами.
    """
    def __init__(self, nodes=None, replicas=100):
        self.replicas = replicas
        self.ring = {}
        self.sorted_keys = []
        
        if nodes:
            for node in nodes:
                self.add_node(node)
    
    def add_node(self, node):
        """Добавление узла в кольцо хеширования."""
        for i in range(self.replicas):
            # Создание нескольких виртуальных узлов для равномерного распределения
            key = self._hash(f"{node}:{i}")
            self.ring[key] = node
        
        # Обновление сортированного списка ключей
        self.sorted_keys = sorted(self.ring.keys())
    
    def remove_node(self, node):
        """Удаление узла из кольца хеширования."""
        for i in range(self.replicas):
            key = self._hash(f"{node}:{i}")
            if key in self.ring:
                del self.ring[key]
        
        # Обновление сортированного списка ключей
        self.sorted_keys = sorted(self.ring.keys())
    
    def get_node(self, key):
        """Получение узла для ключа."""
        if not self.ring:
            return None
        
        # Хеширование ключа
        hash_key = self._hash(key)
        
        # Если хеш ключа больше всех хешей узлов, возвращаемся к началу кольца
        if hash_key > self.sorted_keys[-1]:
            return self.ring[self.sorted_keys[0]]
        
        # Поиск ближайшего узла по часовой стрелке
        idx = bisect.bisect(self.sorted_keys, hash_key)
        return self.ring[self.sorted_keys[idx]]
    
    def _hash(self, key):
        """Хеширование ключа."""
        return int(hashlib.md5(str(key).encode()).hexdigest(), 16)

# Использование
def create_distributed_cache(servers):
    """Создание распределенного кэша на основе консистентного хеширования."""
    consistent_hash = ConsistentHash(servers)
    
    # Словарь для хранения соединений к серверам
    connections = {server: get_cache_connection(server) for server in servers}
    
    def set(key, value, expire=None):
        """Запись значения в кэш."""
        server = consistent_hash.get_node(key)
        connections[server].set(key, value, expire)
    
    def get(key):
        """Получение значения из кэша."""
        server = consistent_hash.get_node(key)
        return connections[server].get(key)
    
    def delete(key):
        """Удаление значения из кэша."""
        server = consistent_hash.get_node(key)
        return connections[server].delete(key)
    
    return {
        'set': set,
        'get': get,
        'delete': delete
    }

# Добавление/удаление серверов без существенного перераспределения ключей
servers = ['cache1:11211', 'cache2:11211', 'cache3:11211']
cache = create_distributed_cache(servers)

# Добавление нового сервера
new_servers = servers + ['cache4:11211']
new_cache = create_distributed_cache(new_servers)
# Только ~25% ключей будут перемещены на новый сервер

### Распределенные счетчики

```python
import redis
import time
import random

class DistributedCounter:
    """
    Распределенный счетчик на основе Redis для высоконагруженных приложений.
    """
    def __init__(self, redis_client, counter_name, shards=10):
        self.redis = redis_client
        self.counter_name = counter_name
        self.shards = shards
    
    def increment(self, amount=1):
        """Увеличение счетчика на указанное значение."""
        # Выбор случайного шарда для распределения нагрузки
        shard = random.randint(0, self.shards - 1)
        key = f"{self.counter_name}:{shard}"
        
        # Увеличение значения в шарде
        return self.redis.incrby(key, amount)
    
    def get_value(self):
        """Получение текущего значения счетчика (сумма всех шардов)."""
        # Использование pipeline для эффективного получения всех шардов
        pipe = self.redis.pipeline()
        
        # Запрос значений всех шардов
        for shard in range(self.shards):
            key = f"{self.counter_name}:{shard}"
            pipe.get(key)
        
        # Выполнение всех запросов
        values = pipe.execute()
        
        # Суммирование значений (с учетом возможных None)
        total = 0
        for value in values:
            if value is not None:
                total += int(value)
        
        return total
    
    def reset(self):
        """Сброс счетчика."""
        pipe = self.redis.pipeline()
        
        # Удаление всех шардов
        for shard in range(self.shards):
            key = f"{self.counter_name}:{shard}"
            pipe.delete(key)
        
        pipe.execute()

# Использование
r = redis.Redis(host='localhost', port=6379, db=0)
page_views = DistributedCounter(r, 'page_views', shards=20)

# Увеличение счетчика
page_views.increment()

# Получение общего значения
total_views = page_views.get_value()
print(f"Total page views: {total_views}")
```

### Репликация Cache-aside

```python
import redis
import json
import time
import threading
import random

class ReplicatedCacheManager:
    """
    Менеджер кэша с репликацией для обеспечения высокой доступности.
    """
    def __init__(self, primary_redis, replica_redis, sync_interval=5):
        self.primary = primary_redis
        self.replicas = replica_redis if isinstance(replica_redis, list) else [replica_redis]
        self.sync_interval = sync_interval
        
        # Запуск потока синхронизации
        self._stop_sync = False
        self._sync_thread = threading.Thread(target=self._sync_replicas)
        self._sync_thread.daemon = True
        self._sync_thread.start()
    
    def set(self, key, value, expire=None):
        """Запись значения в основной кэш и асинхронная репликация."""
        # Сериализация данных
        serialized = json.dumps(value)
        
        # Запись в основной кэш
        self.primary.set(key, serialized, ex=expire)
        
        # Маркировка ключа для синхронизации
        self.primary.sadd('_replication_queue', key)
    
    def get(self, key):
        """
        Получение значения из кэша с учетом репликации.
        При недоступности основного кэша, пытается получить из реплик.
        """
        try:
            # Попытка получить из основного кэша
            value = self.primary.get(key)
            if value:
                return json.loads(value)
        except redis.RedisError:
            # Основной кэш недоступен, пробуем реплики
            pass
        
        # Перебор реплик в случайном порядке
        replicas = list(self.replicas)
        random.shuffle(replicas)
        
        for replica in replicas:
            try:
                value = replica.get(key)
                if value:
                    return json.loads(value)
            except redis.RedisError:
                continue
        
        return None
    
    def delete(self, key):
        """Удаление ключа из основного кэша и добавление в очередь синхронизации."""
        self.primary.delete(key)
        self.primary.sadd('_replication_deleted', key)
    
    def _sync_replicas(self):
        """Фоновый поток для синхронизации реплик."""
        while not self._stop_sync:
            try:
                # Получение ключей для репликации
                keys_to_sync = self.primary.smembers('_replication_queue')
                keys_to_delete = self.primary.smembers('_replication_deleted')
                
                if keys_to_sync or keys_to_delete:
                    # Получение значений для всех ключей
                    if keys_to_sync:
                        values = self.primary.mget(keys_to_sync)
                        key_values = list(zip(keys_to_sync, values))
                    
                    # Синхронизация с каждой репликой
                    for replica in self.replicas:
                        try:
                            pipe = replica.pipeline()
                            
                            # Репликация значений
                            for key, value in key_values:
                                if value is not None:
                                    # Получение TTL из основного кэша
                                    ttl = self.primary.ttl(key)
                                    if ttl > 0:
                                        pipe.set(key, value, ex=ttl)
                                    else:
                                        pipe.set(key, value)
                            
                            # Удаление ключей
                            for key in keys_to_delete:
                                pipe.delete(key)
                            
                            pipe.execute()
                        except redis.RedisError:
                            # Ошибка синхронизации с репликой
                            continue
                    
                    # Очистка очередей
                    self.primary.delete('_replication_queue')
                    self.primary.delete('_replication_deleted')
            
            except Exception as e:
                print(f"Error during replication: {e}")
            
            # Пауза перед следующей синхронизацией
            time.sleep(self.sync_interval)
    
    def stop(self):
        """Остановка потока синхронизации."""
        self._stop_sync = True
        self._sync_thread.join()

# Использование
primary = redis.Redis(host='primary.example.com', port=6379, db=0)
replicas = [
    redis.Redis(host='replica1.example.com', port=6379, db=0),
    redis.Redis(host='replica2.example.com', port=6379, db=0)
]

cache = ReplicatedCacheManager(primary, replicas)

# Операции с кэшем
cache.set('key', {'foo': 'bar'}, expire=300)
value = cache.get('key')
cache.delete('key')

# При завершении работы
cache.stop()
```

## Сравнительный анализ

### Сравнение систем кэширования

| Критерий | Memcached | Redis | In-memory | Файловый кэш |
|----------|-----------|-------|-----------|--------------|
| **Скорость** | Очень высокая | Очень высокая | Максимальная | Низкая |
| **Масштабируемость** | Хорошая | Отличная | Ограниченная | Ограниченная |
| **Персистентность** | Нет | Да | Нет | Да |
| **Типы данных** | Простые | Сложные (списки, хеши, и т.д.) | Любые Python | Любые сериализуемые |
| **Атомарные операции** | Базовые | Расширенные | Нет | Нет |
| **Распределенность** | Да | Да | Нет | Нет |
| **Репликация** | Нет (3rd-party) | Встроенная | Нет | Нет |
| **Политики вытеснения** | LRU | Настраиваемые | Зависит от реализации | Зависит от реализации |
| **Отказоустойчивость** | Низкая | Высокая | Нет | Средняя |
| **Мониторинг** | Базовый | Расширенный | Необходима реализация | Необходима реализация |
| **Поддержка TTL** | Да | Да | Необходима реализация | Необходима реализация |
| **Потребление памяти** | Низкое | Среднее | Низкое | Минимальное |

### Сравнение фреймворков по возможностям кэширования

| Критерий | Django | Flask | FastAPI |
|----------|--------|-------|---------|
| **Встроенная поддержка** | Да | Нет (Flask-Caching) | Нет (внешние библиотеки) |
| **Поддерживаемые бэкенды** | Множество | Зависит от Flask-Caching | Зависит от библиотеки |
| **Кэширование на уровне представлений** | Да | Через декораторы | Через декораторы |
| **Кэширование шаблонов** | Да | Ограниченное | Нет |
| **Низкоуровневое API** | Да | Да | Зависит от библиотеки |
| **Per-site кэширование** | Да | Нет | Нет |
| **Middleware** | Да | Нет | Через middleware |
| **Управление зависимостями кэша** | Ручное | Ручное | Через механизм зависимостей |
| **Интеграция с ORM** | Да | Нет | Зависит от ORM |
| **Документация** | Отличная | Хорошая | Ограниченная |

### Рекомендации по выбору

#### Выбор системы кэширования

1. **Memcached подходит, если:**
   - Требуется максимальная производительность
   - Данные относительно простые
   - Не требуется персистентность
   - Нужно горизонтальное масштабирование

2. **Redis подходит, если:**
   - Нужны сложные структуры данных
   - Важна персистентность
   - Требуются атомарные операции
   - Нужна встроенная репликация
   - Важны возможности мониторинга

3. **In-memory кэширование подходит, если:**
   - Приложение не распределенное
   - Масштаб небольшой
   - Кэш используется только в рамках одного процесса
   - Критична минимальная задержка

4. **Файловый кэш подходит, если:**
   - Объем данных превышает доступную память
   - Требуется персистентность между перезапусками
   - Производительность не критична

#### Выбор уровня кэширования

1. **Кэширование данных** — наиболее универсальный подход, подходит практически для любого приложения.

2. **Кэширование запросов к БД** — хороший выбор для приложений с интенсивным использованием базы данных и повторяющимися запросами.

3. **Кэширование фрагментов** — оптимально для сложных страниц с динамическими компонентами.

4. **Кэширование страниц** — максимальное повышение производительности для статичных или редко изменяющихся страниц.

5. **HTTP-кэширование** — отлично работает для публичного контента и снижает нагрузку на сервер.

## Лучшие практики

### Общие рекомендации

1. **Определите ключевые метрики**
   - Что является критичным: скорость отклика, снижение нагрузки на БД, масштабируемость?
   - Установите целевые показатели и отслеживайте их

2. **Выбирайте правильный уровень кэширования**
   - Кэшируйте ближе к источнику данных для максимальной переиспользуемости
   - Рассмотрите многоуровневое кэширование для критичных данных

3. **Определите политику кэширования**
   - TTL должен соответствовать частоте обновления данных
   - Для редко изменяемых данных используйте длительный TTL
   - Для часто изменяемых — короткий TTL или валидацию при изменении

4. **Мониторинг и анализ**
   - Отслеживайте hit/miss ratio для оценки эффективности
   - Анализируйте размер и частоту использования кэша
   - Настройте алерты на критичные показатели

### Генерация ключей кэша

1. **Уникальность**
   - Ключи должны быть уникальными для конкретных данных
   - Используйте префиксы для группировки связанных ключей

2. **Понятность**
   - Ключи должны быть понятными для отладки и анализа
   - Включайте информацию о типе данных и параметрах

3. **Консистентность**
   - Используйте единый подход к формированию ключей
   - Документируйте этот подход для всей команды

```python
# Хороший подход к формированию ключей
def make_cache_key(prefix, *args, **kwargs):
    """
    Формирование консистентного ключа кэша.
    
    Примеры:
    make_cache_key('user', 123) -> 'user:123'
    make_cache_key('search', q='python', page=2) -> 'search:q=python:page=2'
    """
    parts = [prefix]
    
    # Добавление позиционных аргументов
    parts.extend([str(arg) for arg in args])
    
    # Добавление именованных аргументов в отсортированном порядке
    for key, value in sorted(kwargs.items()):
        parts.append(f"{key}={value}")
    
    return ':'.join(parts)
```

### Стратегии инвалидации

1. **Инвалидация по времени**
   - Установите разумное TTL в зависимости от типа данных
   - Для некритичных данных допустимо использовать stale-while-revalidate

2. **Инвалидация по событию**
   - Инвалидируйте кэш при изменении связанных данных
   - Используйте каскадную инвалидацию для зависимых данных

3. **Версионирование кэша**
   - Используйте версии для атомарного обновления всего кэша
   - Эффективно для данных с сильными зависимостями

```python
class VersionedCacheManager:
    def __init__(self, cache_client, namespace):
        self.cache = cache_client
        self.namespace = namespace
        self.version_key = f"{namespace}_version"
        
        # Инициализация версии, если она не существует
        if not self.cache.get(self.version_key):
            self.cache.set(self.version_key, 1)
    
    def _get_version(self):
        return self.cache.get(self.version_key) or 1
    
    def make_key(self, key):
        version = self._get_version()
        return f"{self.namespace}:{version}:{key}"
    
    def get(self, key):
        cache_key = self.make_key(key)
        return self.cache.get(cache_key)
    
    def set(self, key, value, timeout=None):
        cache_key = self.make_key(key)
        return self.cache.set(cache_key, value, timeout)
    
    def invalidate_all(self):
        # Увеличение версии делает все существующие ключи неактуальными
        self.cache.incr(self.version_key)
```

### Оптимизация производительности

1. **Размер кэшируемых данных**
   - Не кэшируйте слишком большие объекты
   - Рассмотрите возможность компрессии данных

2. **Батчинг операций**
   - Используйте пакетные операции (mget, mset)
   - Для нескольких зависимых запросов используйте pipeline

3. **Минимизация сериализации**
   - Выбирайте эффективный формат сериализации (Protocol Buffers, MessagePack)
   - Для внутреннего кэша можно использовать нативные объекты

4. **Асинхронные операции**
   - Для неблокирующих фреймворков используйте асинхронные клиенты кэша
   - Рассмотрите фоновое обновление кэша

```python
import asyncio
import aioredis
import time

class AsyncCacheManager:
    """Асинхронный менеджер кэша с оптимизацией производительности."""
    
    def __init__(self, redis_url):
        self.redis_url = redis_url
        self.redis = None
    
    async def connect(self):
        """Подключение к Redis."""
        if self.redis is None:
            self.redis = await aioredis.create_redis_pool(self.redis_url)
        return self.redis
    
    async def get_multi(self, keys):
        """Оптимизированное получение нескольких ключей."""
        redis = await self.connect()
        values = await redis.mget(*keys)
        return {key: value for key, value in zip(keys, values) if value is not None}
    
    async def refresh_cache_background(self, key, fetch_func, ttl=300):
        """Фоновое обновление кэша без блокировки."""
        redis = await self.connect()
        
        # Проверяем, не обновляется ли уже кэш
        refresh_lock = await redis.set(
            f"{key}:refreshing",
            "1",
            expire=30,
            exist=aioredis.SET_IF_NOT_EXIST
        )
        
        if not refresh_lock:
            # Кэш уже обновляется другим процессом
            return
        
        try:
            # Получение новых данных
            new_data = await fetch_func()
            
            # Обновление кэша
            tr = redis.multi_exec()
            tr.set(key, new_data, expire=ttl)
            tr.delete(f"{key}:refreshing")
            await tr.execute()
        except Exception as e:
            # В случае ошибки освобождаем блокировку
            await redis.delete(f"{key}:refreshing")
            raise e
```

### Работа с распределенными системами

1. **Согласованность кэша**
   - Для систем с высокими требованиями к согласованности используйте строгую инвалидацию
   - Для остальных — eventual consistency с TTL

2. **Устойчивость к сбоям**
   - Используйте репликацию для критичных данных
   - Настройте таймауты и повторные попытки
   - Готовьте систему к работе при недоступности кэша

3. **Шардирование**
   - Используйте консистентное хеширование для равномерного распределения
   - Минимизируйте перераспределение при изменении топологии

```python
import time
from functools import wraps

def fallback_to_source(max_retries=3, retry_delay=0.1):
    """
    Декоратор для работы с кэшем с автоматическим fallback к источнику данных.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(key, *args, **kwargs):
            # Аргументы для оригинальной функции
            fetch_args = args
            fetch_kwargs = kwargs.copy()
            
            # Попытка получить из кэша с повторными попытками
            for attempt in range(max_retries):
                try:
                    cached_value = cache.get(key)
                    if cached_value is not None:
                        return cached_value
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        # Последняя попытка, логируем ошибку и переходим к источнику
                        logger.warning(f"Cache error after {max_retries} attempts: {e}")
                    else:
                        # Ждем перед следующей попыткой
                        time.sleep(retry_delay)
            
            # Получение данных из источника
            try:
                value = func(*fetch_args, **fetch_kwargs)
                
                # Попытка сохранить в кэш
                try:
                    cache.set(key, value, timeout=300)
                except Exception as e:
                    logger.warning(f"Failed to set cache for {key}: {e}")
                
                return value
            except Exception as e:
                # Если источник данных недоступен, логируем ошибку
                logger.error(f"Source data error: {e}")
                raise
        
        return wrapper
    return decorator

@fallback_to_source()
def get_user_data(user_id):
    """Получение данных пользователя с fallback к БД при сбое кэша."""
    # Здесь запрос к источнику данных (БД)
    return db.query(User).filter(User.id == user_id).first()
```

## Подводные камни

### Проблемы с производительностью

1. **Cache stampede (Thundering herd)**
   - Проблема: массовые одновременные запросы к базе данных при истечении кэша
   - Решение: стохастическое обновление, early refresh, блокировки

2. **Чрезмерное кэширование**
   - Проблема: кэширование всего подряд может привести к перерасходу памяти
   - Решение: анализ эффективности кэша, селективное кэширование

3. **Низкая эффективность кэша (Low cache hit ratio)**
   - Проблема: низкое соотношение попаданий к промахам
   - Решение: анализ паттернов доступа, оптимизация стратегии кэширования

4. **Проблемы сериализации**
   - Проблема: неэффективная сериализация увеличивает нагрузку
   - Решение: выбор оптимального сериализатора, кэширование уже сериализованных данных

### Проблемы с консистентностью

1. **Устаревшие данные**
   - Проблема: пользователи видят неактуальную информацию
   - Решение: тщательное управление TTL, инвалидация по событиям

2. **Partial updates**
   - Проблема: частичное обновление кэшированного объекта
   - Решение: атомарные обновления, версионирование

3. **Рассинхронизация в распределенных системах**
   - Проблема: различные данные в разных экземплярах кэша
   - Решение: централизованное управление инвалидацией, eventual consistency

4. **Потеря обновлений**
   - Проблема: обновления могут быть потеряны при конкурентном доступе
   - Решение: транзакции, optimistic/pessimistic locking

### Проблемы с памятью

1. **Memory bloat**
   - Проблема: неконтролируемый рост потребления памяти
   - Решение: лимиты на размер кэша, мониторинг, политики вытеснения

2. **Memory fragmentation**
   - Проблема: фрагментация памяти при частых изменениях размера объектов
   - Решение: перезапуск сервисов, настройка параметров памяти

3. **Eviction policy issues**
   - Проблема: неоптимальная политика вытеснения удаляет нужные данные
   - Решение: тщательный выбор политики в зависимости от паттернов доступа

```python
# Мониторинг использования памяти Redis
def monitor_redis_memory(redis_client, threshold_percent=80, check_interval=60):
    """
    Мониторинг потребления памяти Redis и алерты при достижении порога.
    """
    import time
    import threading
    
    def check_memory():
        while True:
            try:
                # Получение информации о памяти
                info = redis_client.info('memory')
                used_memory = info['used_memory']
                max_memory = info['maxmemory']
                
                if max_memory > 0:  # Если установлен лимит памяти
                    usage_percent = (used_memory / max_memory) * 100
                    
                    if usage_percent > threshold_percent:
                        # Генерация алерта
                        logger.warning(
                            f"Redis memory usage is high: {usage_percent:.2f}% "
                            f"({used_memory / (1024*1024):.2f}MB / {max_memory / (1024*1024):.2f}MB)"
                        )
                        
                        # Информация о ключах для анализа
                        big_keys = get_big_keys(redis_client)
                        logger.info(f"Largest keys: {big_keys}")
            
            except Exception as e:
                logger.error(f"Error monitoring Redis memory: {e}")
            
            time.sleep(check_interval)
    
    # Запуск мониторинга в отдельном потоке
    thread = threading.Thread(target=check_memory, daemon=True)
    thread.start()
    
    return thread

def get_big_keys(redis_client, count=10):
    """Получение самых больших ключей в Redis."""
    # Этот метод требует осторожности в production!
    script = """
    local keys = redis.call('keys', '*')
    local result = {}
    
    for i, key in ipairs(keys) do
        local size = redis.call('memory', 'usage', key)
        table.insert(result, {key, size})
    end
    
    table.sort(result, function(a, b) return a[2] > b[2] end)
    
    local top = {}
    for i = 1, math.min(#result, ARGV[1]) do
        table.insert(top, result[i][1])
        table.insert(top, result[i][2])
    end
    
    return top
    """
    
    return redis_client.eval(script, 0, count)
```

### Проблемы безопасности

1. **Cache poisoning**
   - Проблема: внедрение вредоносных данных в кэш
   - Решение: валидация данных, подписывание кэша

2. **Information disclosure**
   - Проблема: раскрытие чувствительной информации через кэш
   - Решение: не кэшировать чувствительные данные, шифрование

3. **Denial of service через кэш**
   - Проблема: целенаправленное заполнение кэша бесполезными данными
   - Решение: ограничение на размер данных, rate-limiting

## Полезные ресурсы

### Документация

- [Django's cache framework](https://docs.djangoproject.com/en/stable/topics/cache/)
- [Flask-Caching documentation](https://flask-caching.readthedocs.io/)
- [Redis documentation](https://redis.io/documentation)
- [Memcached wiki](https://github.com/memcached/memcached/wiki)
- [FastAPI-Cache](https://github.com/long2ice/fastapi-cache)
- [Python's functools.lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache)
- [Cachetools documentation](https://cachetools.readthedocs.io/)
- [PyMemcache documentation](https://pymemcache.readthedocs.io/)
- [Redis-py documentation](https://redis-py.readthedocs.io/)

### Книги

- "High Performance Django" by Peter Baumgartner and Yann Malet
- "Redis in Action" by Josiah L. Carlson
- "Caching at Scale with Redis" by Salvatore Sanfilippo
- "System Design Interview" by Alex Xu (глава про кэширование)
- "Designing Data-Intensive Applications" by Martin Kleppmann

### Статьи и блоги

- [Caching Best Practices](https://aws.amazon.com/caching/best-practices/)
- [The Hidden Dangers of Caching](https://blog.codinghorror.com/the-hidden-dangers-of-caching/)
- [Caching Strategies and How to Choose the Right One](https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/)
- [The Many Trade-offs of Caching in Distributed Systems](https://medium.com/swlh/the-many-trade-offs-of-caching-in-distributed-systems-21c1a8b1e52f)
- [Scaling Memcache at Facebook](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)

### Инструменты

- [Django Debug Toolbar](https://django-debug-toolbar.readthedocs.io/) - для анализа кэширования в Django
- [Redis Commander](https://github.com/joeferner/redis-commander) - UI для Redis
- [PhpMemcachedAdmin](https://github.com/elijaa/phpmemcachedadmin) - UI для Memcached
- [Prometheus + Grafana](https://prometheus.io/) - для мониторинга кэша
- [Redis Sentinel](https://redis.io/topics/sentinel) - для высокой доступности Redis
- [Redis Cluster](https://redis.io/topics/cluster-tutorial) - для масштабирования Redis