# Оптимизация памяти: слоты, генераторы

## Введение в оптимизацию памяти в Python

Оптимизация памяти — важный аспект разработки на Python, особенно для приложений, обрабатывающих большие объемы данных. Python предоставляет несколько механизмов, позволяющих эффективно управлять использованием памяти, включая слоты, генераторы, итераторы и другие техники.

### Почему важна оптимизация памяти

- **Ограниченные ресурсы**: особенно актуально для встроенных систем и мобильных устройств
- **Масштабирование**: эффективные приложения могут обрабатывать больше данных
- **Производительность**: меньшее использование памяти часто означает более высокую скорость
- **Снижение расходов**: оптимизация ресурсов снижает затраты на инфраструктуру

## Оптимизация с помощью `__slots__`

По умолчанию атрибуты экземпляров классов Python хранятся в словаре `__dict__`. Это обеспечивает динамичность, но требует дополнительной памяти. `__slots__` позволяет определить фиксированный набор атрибутов, что снижает расход памяти.

### Как работают слоты

```python
import sys
import tracemalloc

# Стандартный класс без слотов
class RegularPerson:
    def __init__(self, name, age, address, phone):
        self.name = name
        self.age = age
        self.address = address
        self.phone = phone

# Класс с использованием слотов
class SlottedPerson:
    __slots__ = ['name', 'age', 'address', 'phone']
    
    def __init__(self, name, age, address, phone):
        self.name = name
        self.age = age
        self.address = address
        self.phone = phone

# Сравнение потребления памяти
def compare_memory_usage(n):
    # Включаем отслеживание памяти
    tracemalloc.start()
    
    # Создание обычных объектов
    regular_start = tracemalloc.get_traced_memory()[0]
    regular_objects = [
        RegularPerson(f"Person {i}", 25 + i % 50, f"Address {i}", f"555-{i:04}")
        for i in range(n)
    ]
    regular_end = tracemalloc.get_traced_memory()[0]
    regular_size = regular_end - regular_start
    
    # Создание объектов со слотами
    slotted_start = tracemalloc.get_traced_memory()[0]
    slotted_objects = [
        SlottedPerson(f"Person {i}", 25 + i % 50, f"Address {i}", f"555-{i:04}")
        for i in range(n)
    ]
    slotted_end = tracemalloc.get_traced_memory()[0]
    slotted_size = slotted_end - slotted_start
    
    # Выключаем отслеживание памяти
    tracemalloc.stop()
    
    # Выводим результаты
    print(f"Обычные объекты: {regular_size / (1024 * 1024):.2f} МБ")
    print(f"Объекты со слотами: {slotted_size / (1024 * 1024):.2f} МБ")
    print(f"Экономия памяти: {(regular_size - slotted_size) / regular_size * 100:.1f}%")
    
    # Размер одного объекта
    print(f"\nРазмер одного обычного объекта: {sys.getsizeof(regular_objects[0])} байт")
    print(f"Размер одного объекта со слотами: {sys.getsizeof(slotted_objects[0])} байт")

# Запуск сравнения для 1,000,000 объектов
compare_memory_usage(1000000)
```

Результаты могут быть впечатляющими — экономия памяти до 30-50% при использовании слотов.

### Преимущества и ограничения слотов

**Преимущества:**
- Уменьшение потребления памяти
- Более быстрый доступ к атрибутам
- Предотвращение случайного создания новых атрибутов

**Ограничения:**
- Потеря возможности динамически добавлять атрибуты
- Усложнение наследования (особенно множественного)
- Отсутствие `__dict__` и `__weakref__` (если они не указаны явно)

### Наследование и слоты

```python
# Базовый класс со слотами
class Base:
    __slots__ = ['a', 'b']
    
    def __init__(self, a, b):
        self.a = a
        self.b = b

# Подкласс со своими слотами
class Derived(Base):
    __slots__ = ['c', 'd']  # Только новые атрибуты
    
    def __init__(self, a, b, c, d):
        super().__init__(a, b)
        self.c = c
        self.d = d

# Подкласс с динамическими атрибутами
class DynamicDerived(Base):
    # Нет __slots__, поэтому снова доступен __dict__
    def __init__(self, a, b):
        super().__init__(a, b)
        # Можно добавлять новые атрибуты
        self.dynamic_attr = "I'm dynamic!"

# Подкласс, явно включающий __dict__
class HybridDerived(Base):
    __slots__ = ['c', '__dict__']  # Добавляем __dict__ для поддержки динамических атрибутов
    
    def __init__(self, a, b, c):
        super().__init__(a, b)
        self.c = c
        self.dynamic_attr = "I can be dynamic too!"
```

### Лучшие практики использования слотов

1. **Используйте для классов с множеством экземпляров**
   - Классы данных, точки в 3D-пространстве, узлы графа

2. **Используйте для точно определенных структур данных**
   - Когда атрибуты класса известны заранее и не меняются

3. **Включайте `__weakref__` если нужна поддержка слабых ссылок**
   ```python
   class WithWeakref:
       __slots__ = ['x', 'y', '__weakref__']
   ```

4. **Будьте осторожны с наследованием**
   - Явно указывайте `__slots__` во всех классах цепочки наследования
   - Для гибридных подходов включайте `__dict__` в `__slots__`

## Генераторы для оптимизации памяти

Генераторы позволяют создавать итерируемые последовательности без хранения всех элементов в памяти одновременно. Они особенно полезны при работе с большими наборами данных.

### Сравнение списков и генераторов

```python
import sys
import tracemalloc

def list_vs_generator_memory(n):
    tracemalloc.start()
    
    # Создание списка из n элементов
    start_list = tracemalloc.get_traced_memory()[0]
    numbers_list = [i * i for i in range(n)]
    end_list = tracemalloc.get_traced_memory()[0]
    list_memory = end_list - start_list
    
    # Сброс трассировки
    tracemalloc.reset_peak()
    
    # Создание генератора из n элементов
    start_gen = tracemalloc.get_traced_memory()[0]
    numbers_generator = (i * i for i in range(n))
    end_gen = tracemalloc.get_traced_memory()[0]
    gen_memory = end_gen - start_gen
    
    # Потребление памяти при итерации по генератору
    sum_result = sum(numbers_generator)  # Используем все элементы генератора
    peak_gen = tracemalloc.get_traced_memory()[0] - start_gen
    
    tracemalloc.stop()
    
    print(f"Размер списка ({n} элементов): {list_memory / 1024:.2f} КБ")
    print(f"Размер генератора: {gen_memory / 1024:.2f} КБ")
    print(f"Пиковое использование памяти во время итерации по генератору: {peak_gen / 1024:.2f} КБ")
    print(f"Экономия памяти: {(list_memory - peak_gen) / list_memory * 100:.1f}%")

# Проверка для 10 миллионов элементов
list_vs_generator_memory(10000000)
```

### Генераторные функции

```python
def fibonacci(n):
    """Генератор для последовательности Фибоначчи."""
    a, b = 0, 1
    for _ in range(n):
        yield a
        a, b = b, a + b

# Использование генератора
for num in fibonacci(10):
    print(num)

# Генератор для обработки больших файлов
def read_large_file(file_path, chunk_size=1024):
    """Считывает файл по частям для экономии памяти."""
    with open(file_path, 'r') as file:
        while True:
            chunk = file.read(chunk_size)
            if not chunk:
                break
            yield chunk

# Обработка файла, не загружая его целиком в память
def count_lines(file_path):
    line_count = 0
    for chunk in read_large_file(file_path):
        line_count += chunk.count('\n')
    return line_count
```

### Потоковая обработка данных с генераторами

```python
def process_log_file(file_path):
    """Генератор, который преобразует строки лога в структурированные данные."""
    for line in open(file_path, 'r'):
        if line.strip():  # Пропускаем пустые строки
            parts = line.strip().split('|')
            if len(parts) >= 3:
                timestamp, level, message = parts[0], parts[1], parts[2]
                yield {
                    'timestamp': timestamp,
                    'level': level.strip(),
                    'message': message.strip()
                }

def filter_error_logs(logs_generator):
    """Фильтрует только сообщения об ошибках."""
    for log in logs_generator:
        if log['level'] == 'ERROR':
            yield log

def extract_error_codes(error_logs):
    """Извлекает коды ошибок из сообщений."""
    import re
    pattern = r'Error code: (\d+)'
    for log in error_logs:
        match = re.search(pattern, log['message'])
        if match:
            error_code = match.group(1)
            yield int(error_code)

# Конвейер обработки данных
def analyze_error_codes(log_file):
    # Цепочка генераторов для пошаговой обработки
    logs = process_log_file(log_file)
    error_logs = filter_error_logs(logs)
    error_codes = extract_error_codes(error_logs)
    
    # Подсчет частоты кодов ошибок
    code_frequency = {}
    for code in error_codes:
        code_frequency[code] = code_frequency.get(code, 0) + 1
    
    return code_frequency
```

### Ленивые вычисления с генераторами

```python
def is_prime(n):
    """Проверяет, является ли число простым."""
    if n <= 1:
        return False
    if n <= 3:
        return True
    if n % 2 == 0 or n % 3 == 0:
        return False
    i = 5
    while i * i <= n:
        if n % i == 0 or n % (i + 2) == 0:
            return False
        i += 6
    return True

def lazy_primes():
    """Бесконечный генератор простых чисел."""
    n = 2
    while True:
        if is_prime(n):
            yield n
        n += 1

# Получение первых 10 простых чисел
primes_gen = lazy_primes()
first_10_primes = [next(primes_gen) for _ in range(10)]
print(first_10_primes)

# Нахождение первого простого числа больше 1000
primes_gen = lazy_primes()
first_prime_over_1000 = next(p for p in primes_gen if p > 1000)
print(first_prime_over_1000)
```

## Дополнительные техники оптимизации памяти

### Итераторы и протокол итерации

```python
class EvenNumbers:
    """Итератор, генерирующий четные числа до max_value."""
    
    def __init__(self, max_value):
        self.max_value = max_value
        self.current = 0
    
    def __iter__(self):
        return self
    
    def __next__(self):
        if self.current > self.max_value:
            raise StopIteration
        
        result = self.current
        self.current += 2
        return result

# Использование итератора
for num in EvenNumbers(10):
    print(num)  # Выводит 0, 2, 4, 6, 8, 10
```

### Функция range вместо списков

```python
import sys

# Сравнение памяти для range и списка
list_size = sys.getsizeof(list(range(1000000)))
range_size = sys.getsizeof(range(1000000))

print(f"Размер списка: {list_size / 1024:.2f} КБ")
print(f"Размер range: {range_size / 1024:.2f} КБ")
print(f"Соотношение: {list_size / range_size:.1f}x")

# Использование range вместо генерации списка
total = 0
for i in range(1000000):
    total += i
```

### Встроенные коллекции с меньшим расходом памяти

```python
import sys
from array import array

# Сравнение различных типов коллекций
numbers = list(range(1000))
numbers_tuple = tuple(range(1000))
numbers_array = array('i', range(1000))  # 'i' означает целые числа

print(f"Размер списка: {sys.getsizeof(numbers)} байт")
print(f"Размер кортежа: {sys.getsizeof(numbers_tuple)} байт")
print(f"Размер массива: {sys.getsizeof(numbers_array)} байт")

# Использование _1.arrays для числовых данных
float_array = array('d', [1.5, 2.5, 3.5, 4.5])  # 'd' означает double
float_array.append(5.5)
```

### Экономичные словари: OrderedDict, defaultdict, Counter

```python
import sys
from collections import OrderedDict, defaultdict, Counter

# Обычный словарь
regular_dict = {i: i*i for i in range(1000)}
size_regular = sys.getsizeof(regular_dict)

# OrderedDict (сохраняет порядок вставки)
ordered_dict = OrderedDict((i, i*i) for i in range(1000))
size_ordered = sys.getsizeof(ordered_dict)

# defaultdict (словарь с значением по умолчанию)
default_dict = defaultdict(int)
for i in range(1000):
    default_dict[i] = i*i
size_default = sys.getsizeof(default_dict)

# Counter (словарь для подсчета)
values = [i % 10 for i in range(1000)]
counter = Counter(values)
size_counter = sys.getsizeof(counter)

print(f"Размер обычного словаря: {size_regular} байт")
print(f"Размер OrderedDict: {size_ordered} байт")
print(f"Размер defaultdict: {size_default} байт")
print(f"Размер Counter: {size_counter} байт")
```

### Использование deque вместо списков для очередей

```python
import sys
from collections import deque
import timeit

# Создание структур данных
list_queue = []
deque_queue = deque()

# Сравнение использования памяти
for i in range(10000):
    list_queue.append(i)
    deque_queue.append(i)

print(f"Размер списка: {sys.getsizeof(list_queue)} байт")
print(f"Размер deque: {sys.getsizeof(deque_queue)} байт")

# Сравнение производительности (добавление/удаление в начало)
list_prepend = timeit.timeit(
    "test_list.insert(0, 999); test_list.pop(0)",
    setup="test_list = list(range(10000))",
    number=1000
)

deque_prepend = timeit.timeit(
    "test_deque.appendleft(999); test_deque.popleft()",
    setup="from collections import deque; test_deque = deque(range(10000))",
    number=1000
)

print(f"Время для операций с началом списка: {list_prepend:.6f} с")
print(f"Время для операций с началом deque: {deque_prepend:.6f} с")
print(f"Ускорение: {list_prepend / deque_prepend:.1f}x")
```

### Использование `__slots__` с dataclasses

```python
from dataclasses import dataclass
import sys

# Обычный dataclass
@dataclass
class RegularPoint:
    x: float
    y: float
    z: float

# Dataclass со слотами
@dataclass
class SlottedPoint:
    __slots__ = ['x', 'y', 'z']
    x: float
    y: float
    z: float

# Сравнение размеров
regular_points = [RegularPoint(1.0, 2.0, 3.0) for _ in range(100000)]
slotted_points = [SlottedPoint(1.0, 2.0, 3.0) for _ in range(100000)]

regular_size = sum(sys.getsizeof(p) for p in regular_points)
slotted_size = sum(sys.getsizeof(p) for p in slotted_points)

print(f"Размер 100,000 обычных dataclass: {regular_size / (1024 * 1024):.2f} МБ")
print(f"Размер 100,000 slotted dataclass: {slotted_size / (1024 * 1024):.2f} МБ")
print(f"Экономия: {(regular_size - slotted_size) / regular_size * 100:.1f}%")
```

## Практические примеры и сценарии

### Обработка больших CSV-файлов

```python
import csv
from memory_profiler import profile

@profile
def process_csv_with_list(file_path):
    """Обработка CSV файла с загрузкой всех данных в память."""
    data = []
    with open(file_path, 'r', newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            data.append(row)
    
    # Обработка всех данных
    result = {}
    for row in data:
        category = row['category']
        amount = float(row['amount'])
        result[category] = result.get(category, 0) + amount
    
    return result

@profile
def process_csv_with_generator(file_path):
    """Обработка CSV файла с генератором, без загрузки всех данных в память."""
    def read_csv():
        with open(file_path, 'r', newline='') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                yield row
    
    # Обработка данных по одной записи
    result = {}
    for row in read_csv():
        category = row['category']
        amount = float(row['amount'])
        result[category] = result.get(category, 0) + amount
    
    return result

# Использование функций (в реальном сценарии)
# process_csv_with_list('large_dataset.csv')
# process_csv_with_generator('large_dataset.csv')
```

### Обработка потоковых данных

```python
def process_log_stream(log_stream, error_patterns=None):
    """
    Обрабатывает поток лог-данных построчно, выполняя поиск паттернов ошибок.
    
    Args:
        log_stream: Итерируемый источник строк лога (файл или другой поток)
        error_patterns: Список регулярных выражений для поиска ошибок
    
    Returns:
        Generator, выдающий найденные ошибки с контекстом
    """
    import re
    
    # Компилируем регулярные выражения для производительности
    if error_patterns is None:
        error_patterns = [r'error', r'exception', r'fail', r'critical']
    
    compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in error_patterns]
    
    # Создаем буфер для хранения контекста (предыдущие строки)
    context_buffer = []
    context_size = 3  # Количество предыдущих строк для контекста
    
    for line_num, line in enumerate(log_stream, 1):
        # Добавляем строку в буфер контекста
        context_buffer.append((line_num, line))
        if len(context_buffer) > context_size:
            context_buffer.pop(0)
        
        # Проверяем наличие паттернов ошибок
        for pattern in compiled_patterns:
            if pattern.search(line):
                # Нашли ошибку, выдаем ее с контекстом
                yield {
                    'error_line': (line_num, line),
                    'context': context_buffer[:-1],  # Исключаем текущую строку
                    'pattern': pattern.pattern
                }
                break

# Пример использования
def tail_and_analyze_log(file_path):
    """Имитирует отслеживание файла лога в режиме 'tail -f'."""
    import time
    import os
    
    file_size = os.path.getsize(file_path)
    
    with open(file_path, 'r') as f:
        # Переходим к концу файла
        f.seek(file_size)
        
        while True:
            line = f.readline()
            if not line:
                # Достигнут конец файла, ждем новых данных
                time.sleep(0.1)
                continue
            
            # Анализируем новую строку
            for error_info in process_log_stream([line]):
                handle_error(error_info)

def handle_error(error_info):
    """Обрабатывает найденную ошибку (например, отправляет уведомление)."""
    error_line_num, error_line = error_info['error_line']
    print(f"ERROR detected at line {error_line_num}: {error_line.strip()}")
    print("Context:")
    for ctx_line_num, ctx_line in error_info['context']:
        print(f"  {ctx_line_num}: {ctx_line.strip()}")
    print(f"Matched pattern: {error_info['pattern']}")
    print("---")
```

### Оптимизация вычислений с памятью на сжатых шаблонах

```python
def memory_efficient_matrix_multiply(A, B):
    """
    Перемножает разреженные матрицы A и B, используя словари для хранения только ненулевых элементов.
    Экономит память для разреженных матриц (с большим количеством нулей).
    
    Args:
        A: Матрица в виде словаря {(строка, столбец): значение}
        B: Матрица в виде словаря {(строка, столбец): значение}
    
    Returns:
        Результат умножения в виде словаря {(строка, столбец): значение}
    """
    # Находим размеры матриц
    a_rows = max(i for i, j in A.keys()) + 1 if A else 0
    b_cols = max(j for i, j in B.keys()) + 1 if B else 0
    
    # Создаем индекс для B по столбцам для быстрого доступа
    B_cols = {}
    for (i, j), value in B.items():
        if j not in B_cols:
            B_cols[j] = {}
        B_cols[j][i] = value
    
    # Вычисляем произведение
    result = {}
    for (a_row, a_col), a_val in A.items():
        for b_col, b_col_dict in B_cols.items():
            # Если у нас есть a_col в b_col
            if a_col in b_col_dict:
                b_val = b_col_dict[a_col]
                result_key = (a_row, b_col)
                # Прибавляем к результату или инициализируем
                if result_key in result:
                    result[result_key] += a_val * b_val
                else:
                    result[result_key] = a_val * b_val
    
    return result

# Пример использования
def create_sparse_matrix(rows, cols, sparsity=0.9):
    """Создает разреженную матрицу заданного размера."""
    import random
    matrix = {}
    for i in range(rows):
        for j in range(cols):
            if random.random() > sparsity:  # Генерируем ненулевые элементы с вероятностью (1-sparsity)
                matrix[(i, j)] = random.uniform(-1, 1)
    return matrix

# Создаем две разреженные матрицы
A = create_sparse_matrix(1000, 1000, sparsity=0.99)  # 1000x1000 с 1% ненулевых элементов
B = create_sparse_matrix(1000, 1000, sparsity=0.99)

# Вычисляем произведение
C = memory_efficient_matrix_multiply(A, B)

print(f"Размер A: {len(A)} ненулевых элементов")
print(f"Размер B: {len(B)} ненулевых элементов")
print(f"Размер C: {len(C)} ненулевых элементов")
```

## Мониторинг и отладка использования памяти

### Использование Pympler для анализа объектов

```python
from pympler import asizeof, tracker

# Анализ размера одиночного объекта
class MyClass:
    def __init__(self, data):
        self.data = data
        self.processed = {}

obj = MyClass([1, 2, 3, 4, 5])
print(f"Размер объекта: {asizeof.asizeof(obj)} байт")

# Отслеживание эволюции объектов
tr = tracker.SummaryTracker()

obj.processed = {i: i*i for i in range(100)}
tr.print_diff()  # Показывает изменения с момента создания трекера

obj.data.extend([6, 7, 8, 9, 10])
tr.print_diff()  # Показывает новые изменения
```

### Утечки памяти и циклические ссылки

```python
import gc
import weakref

class Node:
    def __init__(self, value):
        self.value = value
        self.next = None
        self.prev = None
    
    def __repr__(self):
        return f"Node({self.value})"

# Создаем циклическую структуру
def create_circular_references():
    a = Node("A")
    b = Node("B")
    c = Node("C")
    
    a.next = b
    b.next = c
    c.next = a  # Создаем цикл
    
    b.prev = a
    c.prev = b
    a.prev = c  # Создаем второй цикл
    
    # Эти объекты не будут собраны сборщиком мусора,
    # даже если они выходят из области видимости

# Решение 1: Явное уничтожение цикла
def break_circular_references(node):
    current = node
    while current:
        next_node = current.next
        current.next = None
        current.prev = None
        current = next_node
        if current == node:
            break

# Решение 2: Использование слабых ссылок
class WeakNode:
    def __init__(self, value):
        self.value = value
        self.next = None
        self._prev = None  # Обычная ссылка для внутреннего использования
        
    @property
    def prev(self):
        if self._prev is None:
            return None
        return self._prev()  # Получаем объект из слабой ссылки
    
    @prev.setter
    def prev(self, node):
        if node is None:
            self._prev = None
        else:
            self._prev = weakref.ref(node)  # Создаем слабую ссылку

# Демонстрация работы сборщика мусора с циклическими ссылками
def demo_gc():
    print("Создаем циклические ссылки...")
    create_circular_references()
    
    # Принудительно запускаем сборщик мусора
    collected = gc.collect()
    print(f"Собрано {collected} объектов.")
    
    # Проверяем, есть ли недоступные объекты, которые не были собраны
    unreachable = gc.get_objects()
    nodes = [obj for obj in unreachable if isinstance(obj, Node)]
    print(f"Найдено {len(nodes)} недоступных узлов Node.")
    
    # Отключаем циклический сборщик мусора для демонстрации
    gc.disable()
    
    print("\nСоздаем новые циклические ссылки без сборщика мусора...")
    create_circular_references()
    
    # Проверяем снова
    unreachable = gc.get_objects()
    nodes = [obj for obj in unreachable if isinstance(obj, Node)]
    print(f"Найдено {len(nodes)} недоступных узлов Node.")
    
    # Включаем сборщик мусора обратно
    gc.enable()
    
    print("\nРазрываем циклические ссылки вручную...")
    # Нам нужно иметь доступ к узлу, чтобы разорвать цикл
    # В реальном сценарии вам нужно хранить ссылку где-то
    nodes[0].next = None
    nodes[0].prev = None
    
    collected = gc.collect()
    print(f"Собрано {collected} объектов после разрыва цикла.")

# Запускаем демонстрацию
demo_gc()
```

## Заключение

Оптимизация памяти в Python — важный аспект разработки высокопроизводительных приложений. Использование `__slots__`, генераторов, соответствующих структур данных и понимание особенностей управления памятью в Python позволяет значительно сократить потребление ресурсов вашими программами.

Основные принципы оптимизации памяти:

1. **Измеряйте перед оптимизацией** — профилируйте использование памяти
2. **Используйте `__slots__`** для классов с множеством экземпляров
3. **Применяйте генераторы** вместо списков для обработки больших наборов данных
4. **Выбирайте подходящие структуры данных** для задачи (array, deque, OrderedDict и т.д.)
5. **Обрабатывайте данные потоково**, не загружая их целиком в память
6. **Следите за циклическими ссылками** и правильно управляйте объектами
7. **Используйте слабые ссылки** при необходимости для предотвращения утечек памяти
8. **Рассматривайте алгоритмические улучшения** — оптимизация алгоритма может снизить требования к памяти