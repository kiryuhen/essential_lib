# 2.4. Управление памятью и оптимизация в Python

## Содержание
- [Модель памяти в Python](#модель-памяти-в-python)
- [Сборка мусора и управление ресурсами](#сборка-мусора-и-управление-ресурсами)
- [Оптимизация кода и профилирование](#оптимизация-кода-и-профилирование)
- [Измерение производительности](#измерение-производительности)
- [Global Interpreter Lock (GIL)](#global-interpreter-lock-gil)
- [Утечки памяти и их предотвращение](#утечки-памяти-и-их-предотвращение)
- [Оптимизация в критических местах](#оптимизация-в-критических-местах)
- [Практические задачи](#практические-задачи)
- [Полезные ресурсы](#полезные-ресурсы)

## Модель памяти в Python

### Объекты и ссылки

В Python всё является объектами, и переменные – это просто имена (ссылки), указывающие на объекты.

```python
# a и b ссылаются на один и тот же объект
a = [1, 2, 3]
b = a

print(a is b)  # True - они указывают на один объект в памяти

# Изменение объекта через одну ссылку отражается для всех ссылок
b.append(4)
print(a)  # [1, 2, 3, 4] - a и b всё ещё указывают на один объект

# Создание нового объекта
c = a.copy()  # или c = a[:] для поверхностной копии
print(c is a)  # False - c указывает на новый объект
c.append(5)
print(a)  # [1, 2, 3, 4] - a не изменился
print(c)  # [1, 2, 3, 4, 5]
```

### Изменяемые и неизменяемые типы

```python
# Неизменяемые типы: int, float, bool, str, tuple, frozenset
x = 10
y = x
x = 20
print(y)  # 10 - y всё ещё ссылается на исходное значение

# Изменяемые типы: list, dict, set
lst1 = [1, 2, 3]
lst2 = lst1
lst1.append(4)  # Изменяем объект, на который указывают обе ссылки
print(lst2)  # [1, 2, 3, 4]

# Вложенные структуры и глубокое копирование
import copy

nested = [[1, 2], [3, 4]]
shallow_copy = copy.copy(nested)  # или list(nested) или nested.copy()
deep_copy = copy.deepcopy(nested)

nested[0][0] = 99
print(shallow_copy)  # [[99, 2], [3, 4]] - внутренние списки всё ещё общие
print(deep_copy)     # [[1, 2], [3, 4]] - полностью независимая копия
```

### Счётчики ссылок и жизненный цикл объектов

```python
import sys

# Получение количества ссылок на объект
a = [1, 2, 3]
print(sys.getrefcount(a))  # Обычно 2 (сам объект a и временная ссылка в getrefcount)

b = a
print(sys.getrefcount(a))  # Теперь 3

# При выходе переменной из области видимости, счётчик ссылок уменьшается
def func():
    c = a  # Увеличивает счётчик ссылок
    print(sys.getrefcount(a))  # 4
    # После выхода из функции, c будет уничтожена и счётчик уменьшится

func()
print(sys.getrefcount(a))  # Снова 3

# Явное удаление ссылки
del b
print(sys.getrefcount(a))  # Снова 2
```

### Объекты-одиночки (интернирование)

Python оптимизирует использование памяти, создавая только один экземпляр для некоторых часто используемых объектов.

```python
# Интернирование строк
a = 'hello'
b = 'hello'
print(a is b)  # True - Python интернирует небольшие строки

# Интернирование чисел
x = 5
y = 5
print(x is y)  # True - маленькие целые числа (обычно от -5 до 256) интернируются

# Но это не всегда работает для больших чисел
big_x = 1000
big_y = 1000
print(big_x is big_y)  # Может быть True или False в зависимости от реализации

# Интернирование меняет поведение
a = 257
b = 257
print(a is b)  # Может быть False

c = 257; d = 257  # В одной строке компилятор может оптимизировать
print(c is d)  # Может быть True
```

### Представление объектов в памяти

```python
import sys

# Размер объектов
print(sys.getsizeof(1))          # Обычно 28 байт (зависит от версии Python)
print(sys.getsizeof("hello"))    # Обычно 54 байт (5 символов + накладные расходы)
print(sys.getsizeof([1, 2, 3]))  # Список требует больше памяти, чем сумма его элементов

# Размер сложных структур
empty_list = []
one_element_list = [0]
print(sys.getsizeof(empty_list))        # ~56 байт
print(sys.getsizeof(one_element_list))  # ~64 байт

# Накладные расходы для словарей
empty_dict = {}
one_element_dict = {"key": "value"}
print(sys.getsizeof(empty_dict))        # ~240 байт
print(sys.getsizeof(one_element_dict))  # ~240 байт

# Но sys.getsizeof не учитывает размер вложенных объектов
nested_list = [[1, 2, 3], [4, 5, 6]]
print(sys.getsizeof(nested_list))  # Не включает размер внутренних списков

# Более точное измерение с рекурсивным обходом
def get_size(obj, seen=None):
    """Рекурсивно находит размер объекта в байтах."""
    if seen is None:
        seen = set()
    
    # Если объект уже посчитан, возвращаем 0
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    
    # Отмечаем объект как посчитанный
    seen.add(obj_id)
    size = sys.getsizeof(obj)
    
    # Если объект — контейнер, добавляем размер его элементов
    if isinstance(obj, (list, tuple, set, dict)):
        if isinstance(obj, (list, tuple, set)):
            size += sum(get_size(item, seen) for item in obj)
        else:  # dict
            size += sum(get_size(k, seen) + get_size(v, seen) for k, v in obj.items())
    
    return size

print(get_size(nested_list))  # Включает размер всех внутренних объектов
```

## Сборка мусора и управление ресурсами

### Механизмы сборки мусора в Python

Python использует несколько механизмов для управления памятью:

1. **Подсчёт ссылок**: основной механизм, который освобождает объекты, когда их счётчик ссылок падает до нуля.
2. **Поколенческая сборка мусора**: дополнительный механизм, который обнаруживает циклические ссылки, которые не могут быть обнаружены с помощью подсчёта ссылок.

```python
import gc

# Получение статистики о сборке мусора
print(gc.get_stats())

# Настройка порогов для поколенческой сборки мусора
thresholds = gc.get_threshold()
print(f"Текущие пороги сборки мусора: {thresholds}")

# Ручной запуск сборки мусора
gc.collect()

# Циклические ссылки, которые не могут быть обнаружены подсчётом ссылок
def create_cycle():
    lst = [1, 2, 3]
    lst.append(lst)  # Создаём циклическую ссылку
    return lst

cycle = create_cycle()
del cycle  # Удаляем ссылку, но циклическая структура остаётся в памяти

# Принудительная сборка мусора для удаления циклических структур
gc.collect()

# Отключение сборки мусора (не рекомендуется в обычных ситуациях)
gc.disable()
# ... критическая секция кода ...
gc.enable()
```

### Использование контекстных менеджеров для управления ресурсами

```python
# Базовое использование with для управления файлами
with open('file.txt', 'w') as f:
    f.write('Текст')
# Файл автоматически закрывается после выхода из блока with

# Создание собственных контекстных менеджеров
class DatabaseConnection:
    def __init__(self, connection_string):
        self.connection_string = connection_string
        self.connection = None
    
    def __enter__(self):
        print(f"Connecting to {self.connection_string}")
        self.connection = {"connected": True}  # В реальности - подключение к БД
        return self.connection
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.connection:
            print("Closing database connection")
            self.connection = None
        # Возвращаем False, чтобы пробросить исключение наружу
        return False

# Использование собственного контекстного менеджера
with DatabaseConnection("mysql://localhost:3306/mydb") as conn:
    print(f"Connection established: {conn}")
    # ... работа с базой данных ...
# Соединение автоматически закрывается

# Использование contextlib для создания контекстных менеджеров
from contextlib import contextmanager

@contextmanager
def file_manager(filename, mode):
    try:
        f = open(filename, mode)
        yield f
    finally:
        f.close()

with file_manager('file.txt', 'r') as f:
    content = f.read()
    print(f"Прочитано {len(content)} символов")
```

### Слабые ссылки

Слабые ссылки (weak references) не увеличивают счётчик ссылок на объект, что позволяет им ссылаться на объект, не предотвращая его сборку, когда на него больше нет сильных ссылок.

```python
import weakref

class LargeObject:
    def __init__(self, name):
        self.name = name
        self.data = [0] * 1000000  # Занимает много памяти
    
    def __del__(self):
        print(f"Deleting {self.name}")

# Обычная (сильная) ссылка
obj = LargeObject("MyObject")

# Создание слабой ссылки
weak_ref = weakref.ref(obj)

# Получение объекта из слабой ссылки
print(weak_ref() is obj)  # True

# Удаление сильной ссылки
del obj

# Теперь объект удалён, и слабая ссылка указывает на None
print(weak_ref())  # None

# Использование слабых словарей для кэширования
cache = weakref.WeakValueDictionary()

# Добавляем объекты в кэш
key1 = "key1"
obj1 = LargeObject("Object1")
cache[key1] = obj1

# Если удалить последнюю сильную ссылку, объект будет удален из кэша
del obj1
print(list(cache.items()))  # Пустой список
```

## Оптимизация кода и профилирование

### Профилирование с помощью cProfile

```python
import cProfile
import pstats
from pstats import SortKey

# Функция для профилирования
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Профилирование с помощью cProfile
def profile_function():
    # Прямое использование
    cProfile.run('fibonacci(30)')
    
    # Более детальное использование с сохранением результатов
    profiler = cProfile.Profile()
    profiler.enable()
    fibonacci(30)
    profiler.disable()
    
    # Вывод результатов, отсортированных по времени
    stats = pstats.Stats(profiler).sort_stats(SortKey.TIME)
    stats.print_stats(10)  # Вывод топ-10 функций по времени выполнения
    
    # Сохранение результатов в файл для последующего анализа
    stats.dump_stats('fibonacci_profile.prof')

# Запуск профилирования
profile_function()

# Анализ результатов профилирования из файла
def analyze_profile():
    stats = pstats.Stats('fibonacci_profile.prof')
    stats.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(10)
    # Вывод вызывающих и вызываемых функций
    stats.print_callers(10)
    stats.print_callees(10)

# analyze_profile()  # Раскомментируйте для анализа сохранённых результатов
```

### Использование line_profiler для построчного профилирования

```bash
# Установка line_profiler
# pip install line_profiler
```

```python
# Необходимо использовать декоратор @profile, но не импортировать его
# Декоратор добавляется автоматически при запуске с kernprof

# file: line_profile_example.py
def slow_function():
    total = 0
    for i in range(1000000):
        total += i
    return total

@profile  # Не импортируйте этот декоратор
def main():
    result1 = slow_function()
    result2 = sum(range(1000000))  # Более быстрый способ
    return result1, result2

if __name__ == "__main__":
    main()
```

Запуск line_profiler:

```bash
# kernprof -l -v line_profile_example.py
```

### Профилирование памяти с memory_profiler

```bash
# Установка memory_profiler
# pip install memory_profiler
```

```python
# file: memory_profile_example.py
from memory_profiler import profile

@profile
def memory_intensive_function():
    # Создание большого списка
    big_list = [0] * 1000000
    
    # Создание списка списков
    list_of_lists = [[i] * 100 for i in range(1000)]
    
    # Создание большого словаря
    big_dict = {i: i * 2 for i in range(100000)}
    
    return "Done"

if __name__ == "__main__":
    memory_intensive_function()
```

Запуск memory_profiler:

```bash
# python -m memory_profiler memory_profile_example.py
```

### Использование timeit для микротестов производительности

```python
import timeit

# Измерение времени выполнения небольшого фрагмента кода
def measure_list_vs_generator():
    # Измерение времени создания списка
    list_time = timeit.timeit(
        '[i * 2 for i in range(10000)]',
        number=1000
    )
    
    # Измерение времени создания генератора (без материализации)
    generator_time = timeit.timeit(
        '(i * 2 for i in range(10000))',
        number=1000
    )
    
    # Измерение времени создания и обхода генератора
    generator_iter_time = timeit.timeit(
        'list(i * 2 for i in range(10000))',
        number=1000
    )
    
    print(f"Время создания списка: {list_time:.6f} сек")
    print(f"Время создания генератора: {generator_time:.6f} сек")
    print(f"Время создания и обхода генератора: {generator_iter_time:.6f} сек")

# Измерение времени выполнения функции
def measure_function():
    setup_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
"""
    
    # Измерение времени выполнения функции fibonacci(20)
    time_taken = timeit.timeit(
        'fibonacci(20)',
        setup=setup_code,
        number=100
    )
    
    print(f"Время выполнения fibonacci(20): {time_taken:.6f} сек")

# Сравнение различных подходов
def compare_approaches():
    # Различные способы конкатенации строк
    setup_code = """
import random
strings = ['a' * random.randint(1, 10) for _ in range(1000)]
"""
    
    # Использование + оператора
    plus_time = timeit.timeit(
        'result = ""; [result + s for s in strings]',
        setup=setup_code,
        number=100
    )
    
    # Использование join
    join_time = timeit.timeit(
        '"".join(strings)',
        setup=setup_code,
        number=100
    )
    
    print(f"Время конкатенации с +: {plus_time:.6f} сек")
    print(f"Время конкатенации с join: {join_time:.6f} сек")

# Запуск тестов производительности
measure_list_vs_generator()
measure_function()
compare_approaches()
```

## Измерение производительности

### Определение узких мест

```python
import time
import random

# Простая функция для отслеживания времени выполнения
def time_execution(func, *args, **kwargs):
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    print(f"{func.__name__} выполнилась за {end_time - start_time:.6f} сек")
    return result

# Функция с несколькими операциями
def process_data(data):
    start_time = time.time()
    # Шаг 1: Фильтрация данных
    step1_time = time.time()
    filtered_data = [x for x in data if x % 2 == 0]
    step2_time = time.time()
    print(f"Фильтрация: {step2_time - step1_time:.6f} сек")
    
    # Шаг 2: Преобразование данных
    transformed_data = [x ** 2 for x in filtered_data]
    step3_time = time.time()
    print(f"Преобразование: {step3_time - step2_time:.6f} сек")
    
    # Шаг 3: Сортировка данных
    sorted_data = sorted(transformed_data)
    step4_time = time.time()
    print(f"Сортировка: {step4_time - step3_time:.6f} сек")
    
    end_time = time.time()
    print(f"Общее время: {end_time - start_time:.6f} сек")
    
    return sorted_data

# Тестовые данные для обработки
test_data = [random.randint(1, 1000) for _ in range(100000)]
process_data(test_data)
```

### Сравнение различных алгоритмов

```python
def compare_sorting_algorithms():
    # Генерация случайного списка
    import random
    data = [random.randint(1, 1000) for _ in range(10000)]
    
    # Функции сортировки для сравнения
    def bubble_sort(arr):
        n = len(arr)
        for i in range(n):
            for j in range(0, n - i - 1):
                if arr[j] > arr[j + 1]:
                    arr[j], arr[j + 1] = arr[j + 1], arr[j]
        return arr
    
    def insertion_sort(arr):
        for i in range(1, len(arr)):
            key = arr[i]
            j = i - 1
            while j >= 0 and arr[j] > key:
                arr[j + 1] = arr[j]
                j -= 1
            arr[j + 1] = key
        return arr
    
    def built_in_sort(arr):
        return sorted(arr)
    
    # Сравнение времени выполнения
    algorithms = [
        (bubble_sort, "Сортировка пузырьком", 1000),  # Ограничиваем размер для медленных алгоритмов
        (insertion_sort, "Сортировка вставками", 1000),
        (built_in_sort, "Встроенная сортировка", 10000)
    ]
    
    for algo, name, limit in algorithms:
        test_data = data[:limit]  # Берём часть данных для медленных алгоритмов
        start_time = time.time()
        algo(test_data.copy())  # Копируем, чтобы не изменять исходные данные
        end_time = time.time()
        print(f"{name} для {limit} элементов: {end_time - start_time:.6f} сек")

compare_sorting_algorithms()
```

### Оптимизация с использованием альтернативных структур данных

```python
def compare_data_structures():
    import time
    import random
    from collections import deque
    
    # Генерация тестовых данных
    n = 100000
    items = list(range(n))
    random.shuffle(items)
    
    # Тест 1: Вставка в начало
    print("Вставка элемента в начало:")
    
    # Список
    start_time = time.time()
    test_list = []
    for i in range(1000):  # Ограничиваем количество операций для списка
        test_list.insert(0, i)
    end_time = time.time()
    print(f"Список (1000 операций): {end_time - start_time:.6f} сек")
    
    # Deque
    start_time = time.time()
    test_deque = deque()
    for i in range(n):
        test_deque.appendleft(i)
    end_time = time.time()
    print(f"Deque ({n} операций): {end_time - start_time:.6f} сек")
    
    # Тест 2: Проверка наличия элемента
    print("\nПроверка наличия элемента:")
    
    # Список
    test_list = items.copy()
    start_time = time.time()
    for i in range(1000):  # Ограничиваем для списка
        i in test_list
    end_time = time.time()
    print(f"Список (1000 операций): {end_time - start_time:.6f} сек")
    
    # Множество
    test_set = set(items)
    start_time = time.time()
    for i in range(n):
        i in test_set
    end_time = time.time()
    print(f"Множество ({n} операций): {end_time - start_time:.6f} сек")
    
    # Словарь
    test_dict = {i: i for i in items}
    start_time = time.time()
    for i in range(n):
        i in test_dict
    end_time = time.time()
    print(f"Словарь ({n} операций): {end_time - start_time:.6f} сек")

compare_data_structures()
```

### Оптимизация с использованием оптимизированных библиотек

```python
def compare_numpy_vs_list():
    import time
    import random
    import numpy as np
    
    # Генерация тестовых данных
    n = 1000000
    data = [random.random() for _ in range(n)]
    np_data = np.array(data)
    
    # Вычисление суммы
    print("Вычисление суммы:")
    
    # Список + стандартный Python
    start_time = time.time()
    sum_list = sum(data)
    end_time = time.time()
    print(f"Python sum(): {end_time - start_time:.6f} сек")
    
    # NumPy
    start_time = time.time()
    sum_np = np.sum(np_data)
    end_time = time.time()
    print(f"NumPy sum(): {end_time - start_time:.6f} сек")
    
    # Вычисление среднего значения
    print("\nВычисление среднего значения:")
    
    # Список + стандартный Python
    start_time = time.time()
    avg_list = sum(data) / len(data)
    end_time = time.time()
    print(f"Python sum()/len(): {end_time - start_time:.6f} сек")
    
    # NumPy
    start_time = time.time()
    avg_np = np.mean(np_data)
    end_time = time.time()
    print(f"NumPy mean(): {end_time - start_time:.6f} сек")
    
    # Матричные операции
    print("\nМатричное умножение (200x200):")
    
    # Python
    matrix_a = [[random.random() for _ in range(200)] for _ in range(200)]
    matrix_b = [[random.random() for _ in range(200)] for _ in range(200)]
    
    start_time = time.time()
    result = [[sum(a*b for a, b in zip(row_a, col_b)) for col_b in zip(*matrix_b)] for row_a in matrix_a]
    end_time = time.time()
    print(f"Python: {end_time - start_time:.6f} сек")
    
    # NumPy
    np_matrix_a = np.array(matrix_a)
    np_matrix_b = np.array(matrix_b)
    
    start_time = time.time()
    np_result = np.matmul(np_matrix_a, np_matrix_b)
    end_time = time.time()
    print(f"NumPy: {end_time - start_time:.6f} сек")

# compare_numpy_vs_list()  # Может занять некоторое время
```

## Global Interpreter Lock (GIL)

### Что такое GIL и почему он существует

Global Interpreter Lock (GIL) — это механизм в интерпретаторе CPython, который не позволяет нескольким нативным потокам одновременно выполнять Python-код. GIL был введён, чтобы упростить управление памятью и повысить производительность для однопоточных программ, но он ограничивает возможности параллельного выполнения.

### Ограничения GIL и их обход

```python
import threading
import time
import multiprocessing
import concurrent.futures

# Демонстрация влияния GIL на CPU-bound задачи
def cpu_bound_task(n):
    """Задача, интенсивно использующая CPU."""
    count = 0
    for i in range(n):
        count += i
    return count

# Выполнение в одном потоке
def single_threaded():
    start_time = time.time()
    cpu_bound_task(10**7)
    cpu_bound_task(10**7)
    end_time = time.time()
    print(f"Однопоточное выполнение: {end_time - start_time:.2f} сек")

# Выполнение в нескольких потоках
def multi_threaded():
    start_time = time.time()
    
    # Создание потоков
    t1 = threading.Thread(target=cpu_bound_task, args=(10**7,))
    t2 = threading.Thread(target=cpu_bound_task, args=(10**7,))
    
    # Запуск потоков
    t1.start()
    t2.start()
    
    # Ожидание завершения потоков
    t1.join()
    t2.join()
    
    end_time = time.time()
    print(f"Многопоточное выполнение: {end_time - start_time:.2f} сек")

# Выполнение в нескольких процессах
def multi_processed():
    start_time = time.time()
    
    # Создание процессов
    p1 = multiprocessing.Process(target=cpu_bound_task, args=(10**7,))
    p2 = multiprocessing.Process(target=cpu_bound_task, args=(10**7,))
    
    # Запуск процессов
    p1.start()
    p2.start()
    
    # Ожидание завершения процессов
    p1.join()
    p2.join()
    
    end_time = time.time()
    print(f"Многопроцессное выполнение: {end_time - start_time:.2f} сек")

# Выполнение с использованием пула процессов
def process_pool():
    start_time = time.time()
    
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = [executor.submit(cpu_bound_task, 10**7) for _ in range(2)]
        for future in concurrent.futures.as_completed(futures):
            _ = future.result()
    
    end_time = time.time()
    print(f"Выполнение с пулом процессов: {end_time - start_time:.2f} сек")

# Сравнение подходов
def compare_gil_approaches():
    print("Сравнение влияния GIL на CPU-bound задачи:")
    single_threaded()
    multi_threaded()  # Из-за GIL будет примерно такое же время, как и однопоточное
    multi_processed()  # Обход GIL с помощью многопроцессности
    process_pool()    # Ещё один способ обхода GIL

# compare_gil_approaches()  # Может занять некоторое время

# Демонстрация отсутствия влияния GIL на I/O-bound задачи
def io_bound_task():
    """Задача, интенсивно использующая I/O."""
    time.sleep(1)  # Имитация I/O операции
    return "Готово"

def single_threaded_io():
    start_time = time.time()
    for _ in range(5):
        io_bound_task()
    end_time = time.time()
    print(f"Однопоточное I/O: {end_time - start_time:.2f} сек")

def multi_threaded_io():
    start_time = time.time()
    
    threads = []
    for _ in range(5):
        thread = threading.Thread(target=io_bound_task)
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    end_time = time.time()
    print(f"Многопоточное I/O: {end_time - start_time:.2f} сек")

def compare_io_bound():
    print("\nСравнение влияния GIL на I/O-bound задачи:")
    single_threaded_io()  # Будет занимать ~5 секунд
    multi_threaded_io()   # Будет занимать ~1 секунду, т.к. потоки ожидают I/O параллельно

# compare_io_bound()  # Может занять некоторое время
```

### Когда GIL становится проблемой

- **CPU-bound задачи**: если ваше приложение выполняет тяжёлые вычисления, GIL будет препятствовать эффективному использованию нескольких ядер CPU в однопроцессной модели.
- **Многопоточность для вычислений**: использование потоков для параллельных вычислений в Python малоэффективно из-за GIL.

### Когда GIL не является проблемой

- **I/O-bound задачи**: если ваше приложение проводит большую часть времени в ожидании I/O операций (сеть, диск), GIL не создаёт существенных ограничений.
- **Многопроцессная модель**: запуск нескольких процессов Python позволяет обойти ограничения GIL.
- **Альтернативные реализации Python**: PyPy, Jython и IronPython не имеют GIL.

### Стратегии работы с GIL

```python
# 1. Использование многопроцессной обработки для CPU-bound задач
def multiprocessing_example():
    import multiprocessing
    
    def compute_intensive(n):
        """Вычислительно-интенсивная задача."""
        return sum(i * i for i in range(n))
    
    # Создание пула процессов
    with multiprocessing.Pool() as pool:
        # Распределение задач по процессам
        results = pool.map(compute_intensive, [10**7, 10**7, 10**7, 10**7])
    
    return results

# 2. Использование C-расширений для критических секций кода
# Код на C не ограничен GIL и может быть запущен параллельно
# Пример: NumPy, которая выполняет векторизованные операции в C-коде

# 3. Использование асинхронности для I/O-bound задач
import asyncio

async def async_io_task(n):
    """Асинхронная I/O-bound задача."""
    print(f"Задача {n} начата")
    await asyncio.sleep(1)  # Имитация I/O операции
    print(f"Задача {n} завершена")
    return n

async def main():
    # Создание и запуск задач
    tasks = [async_io_task(i) for i in range(5)]
    results = await asyncio.gather(*tasks)
    return results

def asyncio_example():
    return asyncio.run(main())
```

## Утечки памяти и их предотвращение

### Обнаружение утечек памяти

```python
import tracemalloc
import gc
import sys

# Пример функции с утечкой памяти (циклические ссылки)
def create_memory_leak():
    class Node:
        def __init__(self, value):
            self.value = value
            self.next = None
    
    # Создаем циклическую структуру
    head = Node(1)
    current = head
    for i in range(2, 1000):
        current.next = Node(i)
        current = current.next
    
    # Создаем цикл
    current.next = head
    
    # Возвращаем ссылку на начало
    return head

# Отслеживание выделения памяти с помощью tracemalloc
def track_memory_usage():
    # Запускаем отслеживание
    tracemalloc.start()
    
    # Создаем потенциальную утечку
    leak = create_memory_leak()
    
    # Получаем текущий и пиковый размер выделенной памяти
    current, peak = tracemalloc.get_traced_memory()
    print(f"Текущая память: {current / 10**6:.1f} МБ")
    print(f"Пиковая память: {peak / 10**6:.1f} МБ")
    
    # Получаем статистику по местам выделения памяти
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')
    
    print("[ Топ-10 мест выделения памяти ]")
    for stat in top_stats[:10]:
        print(stat)
    
    # Удаляем ссылку и вызываем сборщик мусора
    del leak
    gc.collect()
    
    # Проверяем память после сборки мусора
    current, peak = tracemalloc.get_traced_memory()
    print(f"\nПосле сборки мусора:")
    print(f"Текущая память: {current / 10**6:.1f} МБ")
    
    # Останавливаем отслеживание
    tracemalloc.stop()

# track_memory_usage()  # Запустите для демонстрации
```

### Обнаружение циклических ссылок

```python
import gc

# Включаем отладку сборщика мусора
gc.set_debug(gc.DEBUG_LEAK)

# Функция для создания циклических ссылок
def create_cycles():
    # Создаем циклические структуры
    l1 = []
    l2 = []
    l1.append(l2)
    l2.append(l1)
    
    d1 = {}
    d2 = {}
    d1['next'] = d2
    d2['prev'] = d1
    
    # Удаляем ссылки из локальной области видимости
    del l1, l2, d1, d2

# Функция для поиска и отображения циклических ссылок
def find_cycles():
    # Создаем циклические ссылки
    create_cycles()
    
    # Вызываем сборщик мусора для поиска циклов
    gc.collect()
    
    # Ищем оставшиеся объекты
    print("Найденные циклические ссылки:")
    for obj in gc.get_objects():
        try:
            if isinstance(obj, list) and len(obj) == 1:
                if isinstance(obj[0], list) and obj[0] and obj[0][0] is obj:
                    print(f"Цикл списков: {obj} <-> {obj[0]}")
            elif isinstance(obj, dict) and 'next' in obj and 'prev' in obj.get('next', {}):
                if obj.get('next', {}).get('prev') is obj:
                    print(f"Цикл словарей: {obj} <-> {obj['next']}")
        except (TypeError, AttributeError, KeyError):
            # Игнорируем ошибки при доступе к объектам
            pass

# find_cycles()  # Запустите для демонстрации
```

### Общие источники утечек памяти

1. **Циклические ссылки между объектами**
2. **Кэширование без ограничения размера**
3. **Глобальные переменные и синглтоны**
4. **Незакрытые файловые дескрипторы и сетевые соединения**
5. **События и колбэки, которые не отписываются**

### Стратегии предотвращения утечек памяти

```python
# 1. Использование слабых ссылок для предотвращения циклических ссылок
import weakref

class Parent:
    def __init__(self, name):
        self.name = name
        self.children = []
    
    def add_child(self, child):
        self.children.append(child)

class Child:
    def __init__(self, name, parent):
        self.name = name
        # Используем слабую ссылку на родителя
        self.parent = weakref.ref(parent)
        parent.add_child(self)
    
    def get_parent(self):
        # Получаем объект из слабой ссылки
        return self.parent()

# Создаем родителя и ребенка
parent = Parent("Father")
child = Child("Son", parent)

# Проверяем, что ссылки работают
print(f"Ребенок {child.name} имеет родителя {child.get_parent().name}")
print(f"Родитель {parent.name} имеет детей: {[c.name for c in parent.children]}")

# Удаляем ссылку на родителя
del parent

# Проверяем, что ребенок больше не имеет ссылки на родителя
print(f"Ребенок {child.name} имеет родителя: {child.get_parent()}")  # None

# 2. Ограничение размера кэша с помощью functools.lru_cache
from functools import lru_cache

@lru_cache(maxsize=128)
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# 3. Использование контекстных менеджеров для автоматического освобождения ресурсов
def process_file():
    with open('file.txt', 'r') as f:
        content = f.read()
        # Обработка содержимого
    # Файл автоматически закрывается после выхода из блока with

# 4. Использование атрибута __slots__ для уменьшения потребления памяти
class PointWithSlots:
    __slots__ = ('x', 'y')  # Определяет допустимые атрибуты и предотвращает создание __dict__
    
    def __init__(self, x, y):
        self.x = x
        self.y = y

# Сравнение потребления памяти
class PointWithoutSlots:
    def __init__(self, x, y):
        self.x = x
        self.y = y

def compare_slots_memory_usage():
    import sys
    
    # Создаем объекты
    p1 = PointWithSlots(10, 20)
    p2 = PointWithoutSlots(10, 20)
    
    # Сравниваем размер
    print(f"Размер объекта со __slots__: {sys.getsizeof(p1)} байт")
    print(f"Размер объекта без __slots__: {sys.getsizeof(p2)} байт")
    
    # Размер __dict__ для объекта без __slots__
    print(f"Размер __dict__: {sys.getsizeof(p2.__dict__)} байт")

# compare_slots_memory_usage()  # Запустите для демонстрации
```

## Оптимизация в критических местах

### Оптимизация производительности

```python
# 1. Уменьшение количества операций
def optimize_operations():
    # Неоптимальный вариант (N операций)
    def slow_append():
        result = []
        for i in range(1000000):
            result.append(i)
        return result
    
    # Оптимизированный вариант (1 операция)
    def fast_create():
        return list(range(1000000))
    
    # Измерение времени
    import time
    
    start_time = time.time()
    slow_append()
    slow_time = time.time() - start_time
    
    start_time = time.time()
    fast_create()
    fast_time = time.time() - start_time
    
    print(f"Медленный вариант: {slow_time:.6f} сек")
    print(f"Быстрый вариант: {fast_time:.6f} сек")
    print(f"Ускорение: {slow_time / fast_time:.2f}x")

# optimize_operations()  # Запустите для демонстрации

# 2. Оптимизация циклов
def optimize_loops():
    import random
    data = [random.random() for _ in range(1000000)]
    
    # Неоптимальный вариант (поиск min и max отдельно)
    def slow_min_max():
        min_val = min(data)
        max_val = max(data)
        return min_val, max_val
    
    # Оптимизированный вариант (один проход)
    def fast_min_max():
        min_val = max_val = data[0]
        for x in data[1:]:
            if x < min_val:
                min_val = x
            elif x > max_val:
                max_val = x
        return min_val, max_val
    
    # Измерение времени
    import time
    
    start_time = time.time()
    slow_min_max()
    slow_time = time.time() - start_time
    
    start_time = time.time()
    fast_min_max()
    fast_time = time.time() - start_time
    
    print(f"Два прохода: {slow_time:.6f} сек")
    print(f"Один проход: {fast_time:.6f} сек")
    print(f"Ускорение: {slow_time / fast_time:.2f}x")

# optimize_loops()  # Запустите для демонстрации

# 3. Локальные переменные быстрее глобальных
def optimize_variable_access():
    import time
    import math
    
    # Использование глобальной функции
    def using_global():
        result = 0
        for i in range(1000000):
            result += math.sin(i * 0.001)
        return result
    
    # Локализация функции
    def using_local():
        result = 0
        sin = math.sin  # Локальная переменная
        for i in range(1000000):
            result += sin(i * 0.001)
        return result
    
    # Измерение времени
    start_time = time.time()
    using_global()
    global_time = time.time() - start_time
    
    start_time = time.time()
    using_local()
    local_time = time.time() - start_time
    
    print(f"Глобальная функция: {global_time:.6f} сек")
    print(f"Локальная функция: {local_time:.6f} сек")
    print(f"Ускорение: {global_time / local_time:.2f}x")

# optimize_variable_access()  # Запустите для демонстрации
```

### Использование встроенных функций и библиотек

```python
# 1. Использование встроенных функций вместо циклов
def use_builtin_functions():
    import random
    import time
    
    # Генерация тестовых данных
    data = [random.randint(1, 100) for _ in range(1000000)]
    
    # Суммирование с помощью цикла
    def manual_sum():
        total = 0
        for item in data:
            total += item
        return total
    
    # Суммирование с помощью встроенной функции
    def builtin_sum():
        return sum(data)
    
    # Измерение времени
    start_time = time.time()
    manual_sum()
    manual_time = time.time() - start_time
    
    start_time = time.time()
    builtin_sum()
    builtin_time = time.time() - start_time
    
    print(f"Ручное суммирование: {manual_time:.6f} сек")
    print(f"Встроенная функция sum(): {builtin_time:.6f} сек")
    print(f"Ускорение: {manual_time / builtin_time:.2f}x")

# use_builtin_functions()  # Запустите для демонстрации

# 2. Использование специализированных библиотек
def use_specialized_libraries():
    import time
    import random
    
    # Генерация матриц для умножения
    n = 100
    matrix_a = [[random.random() for _ in range(n)] for _ in range(n)]
    matrix_b = [[random.random() for _ in range(n)] for _ in range(n)]
    
    # Умножение матриц на чистом Python
    def multiply_python():
        result = [[0 for _ in range(n)] for _ in range(n)]
        for i in range(n):
            for j in range(n):
                for k in range(n):
                    result[i][j] += matrix_a[i][k] * matrix_b[k][j]
        return result
    
    # Умножение матриц с NumPy
    def multiply_numpy():
        import numpy as np
        a = np.array(matrix_a)
        b = np.array(matrix_b)
        return np.matmul(a, b)
    
    # Измерение времени
    try:
        start_time = time.time()
        multiply_python()
        python_time = time.time() - start_time
        
        start_time = time.time()
        multiply_numpy()
        numpy_time = time.time() - start_time
        
        print(f"Умножение на Python: {python_time:.6f} сек")
        print(f"Умножение с NumPy: {numpy_time:.6f} сек")
        print(f"Ускорение: {python_time / numpy_time:.2f}x")
    except ImportError:
        print("NumPy не установлен. Установите его для сравнения: pip install numpy")

# use_specialized_libraries()  # Запустите для демонстрации
```

### Компромисс между памятью и скоростью

```python
def memory_vs_speed():
    import time
    import random
    
    # Генерация тестовых данных
    n = 10000
    data = [random.randint(1, 100) for _ in range(n)]
    
    # Поиск повторений
    def find_duplicates_list():
        """Использует меньше памяти, но работает медленнее."""
        duplicates = []
        for i, item in enumerate(data):
            if item in data[:i] and item not in duplicates:
                duplicates.append(item)
        return duplicates
    
    def find_duplicates_set():
        """Использует больше памяти, но работает быстрее."""
        seen = set()
        duplicates = set()
        for item in data:
            if item in seen:
                duplicates.add(item)
            else:
                seen.add(item)
        return list(duplicates)
    
    # Измерение времени и памяти
    import tracemalloc
    
    # Метод с листом
    tracemalloc.start()
    start_time = time.time()
    list_result = find_duplicates_list()
    list_time = time.time() - start_time
    list_memory = tracemalloc.get_traced_memory()[1]
    tracemalloc.stop()
    
    # Метод с множеством
    tracemalloc.start()
    start_time = time.time()
    set_result = find_duplicates_set()
    set_time = time.time() - start_time
    set_memory = tracemalloc.get_traced_memory()[1]
    tracemalloc.stop()
    
    print(f"Метод с листом: {list_time:.6f} сек, {list_memory / 1024:.2f} КБ")
    print(f"Метод с множеством: {set_time:.6f} сек, {set_memory / 1024:.2f} КБ")
    print(f"Ускорение: {list_time / set_time:.2f}x, Увеличение памяти: {set_memory / list_memory:.2f}x")

# memory_vs_speed()  # Запустите для демонстрации
```

### Использование генераторов для экономии памяти

```python
def generators_save_memory():
    import tracemalloc
    import sys
    
    # Генерация большого набора данных
    def large_list():
        """Создает список всех чисел от 0 до n-1."""
        return [i for i in range(10**6)]
    
    def large_generator():
        """Создает генератор для чисел от 0 до n-1."""
        for i in range(10**6):
            yield i
    
    # Измерение памяти для списка
    tracemalloc.start()
    lst = large_list()
    list_memory = tracemalloc.get_traced_memory()[1]
    tracemalloc.stop()
    
    # Измерение памяти для генератора
    tracemalloc.start()
    gen = large_generator()
    generator_memory = tracemalloc.get_traced_memory()[1]
    tracemalloc.stop()
    
    print(f"Память для списка: {list_memory / (1024 * 1024):.2f} МБ")
    print(f"Память для генератора: {generator_memory / 1024:.2f} КБ")
    print(f"Экономия памяти: {list_memory / generator_memory:.2f}x")
    
    # Проверка, что генератор работает как ожидается
    print(f"Первые 10 элементов из генератора: {[next(gen) for _ in range(10)]}")

# generators_save_memory()  # Запустите для демонстрации
```

## Практические задачи

### Задача 1: Оптимизация чтения и обработки больших файлов
Напишите программу, которая:
1. Эффективно читает большой текстовый файл (несколько ГБ) построчно
2. Выполняет подсчет частоты слов в файле
3. Использует оптимальные структуры данных для хранения результатов
4. Минимизирует потребление памяти
5. Измеряет и выводит статистику производительности

### Задача 2: Профилирование и оптимизация алгоритма
Выберите медленный алгоритм (например, наивную реализацию поиска всех простых чисел до N) и:
1. Реализуйте начальную версию алгоритма
2. Профилируйте его с помощью cProfile
3. Определите узкие места
4. Оптимизируйте алгоритм
5. Сравните производительность до и после оптимизации

### Задача 3: Управление памятью и ресурсами
Разработайте класс, который:
1. Управляет кэшем в памяти с ограничением по размеру
2. Реализует стратегию вытеснения (например, LRU - Least Recently Used)
3. Избегает утечек памяти
4. Корректно освобождает ресурсы (например, файловые дескрипторы)
5. Предоставляет статистику использования кэша

### Задача 4: Оптимизация параллельных вычислений
Создайте программу, которая:
1. Выполняет вычислительно-интенсивную задачу (например, умножение матриц)
2. Реализует как однопоточное, так и многопроцессное решение
3. Измеряет ускорение на многоядерной системе
4. Определяет оптимальное количество процессов для вашей системы
5. Анализирует влияние GIL на производительность

### Задача 5: Мониторинг и диагностика утечек памяти
Разработайте систему для мониторинга использования памяти, которая:
1. Отслеживает выделение памяти в разных частях программы
2. Выявляет потенциальные утечки памяти
3. Генерирует отчеты с графиками использования памяти со временем
4. Предоставляет рекомендации по оптимизации
5. Реализует инструменты для автоматической очистки циклических ссылок

## Полезные ресурсы

- [Python's Memory Management](https://realpython.com/python-memory-management/)
- [Документация Python по профилированию](https://docs.python.org/3/library/profile.html)
- [A Guide to Tracking Memory Usage in Python](https://pypi.org/project/memory-profiler/)
- [Global Interpreter Lock](https://realpython.com/python-gil/)
- [Документация по модулю gc](https://docs.python.org/3/library/gc.html)
- [Python Optimization](https://wiki.python.org/moin/PythonSpeed/PerformanceTips)
- [Python Profilers](https://docs.python.org/3/library/profile.html)
- [Memory Management in Python](https://docs.python.org/3/c-api/memory.html)
- [Finding and Fixing Memory Leaks in Python](https://medium.com/zendesk-engineering/hunting-for-memory-leaks-in-python-applications-6824d0518774)
- [Optimizing Python Performance with C Extensions](https://docs.python.org/3/extending/extending.html)
