# Многопоточность и многопроцессорность в Python

## Введение

В современном программировании параллельное выполнение задач играет важную роль для повышения производительности программ. Python предоставляет несколько инструментов для реализации параллелизма:

- **Многопоточность (threading)** — запуск нескольких потоков в рамках одного процесса
- **Многопроцессорность (multiprocessing)** — запуск нескольких процессов, каждый со своим интерпретатором Python
- **Асинхронное программирование (asyncio)** — выполнение задач с использованием событийного цикла
- **Высокоуровневые абстракции (concurrent.futures)** — упрощенный интерфейс для работы с потоками и процессами

В этом уроке мы рассмотрим первые два подхода (threading и multiprocessing), а также познакомимся с модулем concurrent.futures, который предоставляет высокоуровневый интерфейс для обоих подходов.

## Понимание разницы между потоками и процессами

Прежде чем мы начнем, важно понять разницу между потоками и процессами:

- **Процесс** — это экземпляр запущенной программы с выделенным адресным пространством и ресурсами операционной системы
- **Поток** — это единица выполнения внутри процесса, использующая общее адресное пространство с другими потоками того же процесса

### Ключевые различия:

1. **Ресурсы**: 
   - Процессы имеют отдельные области памяти
   - Потоки совместно используют память процесса

2. **Коммуникация**:
   - Между процессами требуется межпроцессное взаимодействие (IPC)
   - Потоки могут обмениваться данными напрямую через общую память

3. **Устойчивость**:
   - Сбой в одном процессе не влияет на другие
   - Сбой в одном потоке может привести к сбою всего процесса

4. **Глобальная блокировка интерпретатора (GIL)**:
   - В CPython (стандартная реализация Python) GIL ограничивает выполнение Python-кода одним потоком в каждый момент времени
   - GIL не влияет на процессы, поскольку каждый процесс имеет свой интерпретатор и свой GIL

## Многопоточность в Python (модуль threading)

Многопоточность в Python реализуется с помощью модуля `threading`. Несмотря на ограничения GIL, потоки полезны для задач, включающих ожидание ввода-вывода (например, запросы к сети или файловые операции).

### Создание и запуск потоков

```python
import threading
import time

def task(name, delay):
    """Функция, которая будет выполняться в отдельном потоке"""
    print(f"{name}: начало выполнения")
    time.sleep(delay)  # Имитация работы
    print(f"{name}: завершение (после {delay} секунд)")

# Создание потоков
thread1 = threading.Thread(target=task, args=("Поток 1", 2))
thread2 = threading.Thread(target=task, args=("Поток 2", 3))

# Запуск потоков
print("Основная программа: запуск потоков")
thread1.start()
thread2.start()

# Ожидание завершения потоков
print("Основная программа: ожидание завершения потоков")
thread1.join()
thread2.join()

print("Основная программа: все потоки завершены")
```

Вывод программы может выглядеть так:

```
Основная программа: запуск потоков
Основная программа: ожидание завершения потоков
Поток 1: начало выполнения
Поток 2: начало выполнения
Поток 1: завершение (после 2 секунд)
Поток 2: завершение (после 3 секунд)
Основная программа: все потоки завершены
```

### Создание потока путем наследования от класса Thread

Можно также создать поток, наследуя от класса `Thread` и переопределяя метод `run()`:

```python
import threading
import time

class MyThread(threading.Thread):
    def __init__(self, name, delay):
        super().__init__()
        self.name = name
        self.delay = delay
    
    def run(self):
        """Метод, который будет выполняться при запуске потока"""
        print(f"{self.name}: начало выполнения")
        time.sleep(self.delay)  # Имитация работы
        print(f"{self.name}: завершение (после {self.delay} секунд)")

# Создание потоков
thread1 = MyThread("Поток 1", 2)
thread2 = MyThread("Поток 2", 3)

# Запуск потоков
print("Основная программа: запуск потоков")
thread1.start()
thread2.start()

# Ожидание завершения потоков
print("Основная программа: ожидание завершения потоков")
thread1.join()
thread2.join()

print("Основная программа: все потоки завершены")
```

### Методы класса Thread

- `start()` — запускает поток, вызывая метод `run()`
- `join([timeout])` — блокирует вызывающий поток до завершения присоединяемого потока
- `is_alive()` — проверяет, выполняется ли поток
- `daemon` — свойство, определяющее, является ли поток демоном (демон-потоки завершаются автоматически при завершении основной программы)
- `name` — свойство, содержащее имя потока

```python
import threading
import time

def task(delay):
    time.sleep(delay)
    print(f"Задача выполнена после {delay} секунд")

# Создаем поток с именем
thread = threading.Thread(target=task, args=(3,), name="МойПоток")

# Устанавливаем его как демон
thread.daemon = True

# Запускаем поток
thread.start()

# Проверяем, работает ли поток
print(f"Поток '{thread.name}' работает? {thread.is_alive()}")

# Ждем 1 секунду (меньше, чем нужно потоку)
time.sleep(1)

# Снова проверяем
print(f"Поток '{thread.name}' всё ещё работает? {thread.is_alive()}")

# Основная программа завершается, демон-поток будет принудительно завершен
print("Основная программа завершена")
```

### Синхронизация потоков с помощью блокировок

При использовании общих ресурсов в нескольких потоках может возникнуть проблема конкуренции. Для защиты совместно используемых ресурсов используются блокировки:

```python
import threading
import time

# Общий ресурс - счетчик
counter = 0

# Создаем блокировку
lock = threading.Lock()

def increment_counter(name, count):
    global counter
    
    for _ in range(count):
        # Захватываем блокировку
        lock.acquire()
        try:
            # Критическая секция
            current = counter
            time.sleep(0.001)  # Имитация сложных вычислений
            counter = current + 1
        finally:
            # Освобождаем блокировку
            lock.release()
        
    print(f"{name}: увеличил счетчик до {counter}")

# Альтернативный способ с использованием контекстного менеджера
def increment_counter_with_context(name, count):
    global counter
    
    for _ in range(count):
        # Используем блокировку с контекстным менеджером
        with lock:
            # Критическая секция
            current = counter
            time.sleep(0.001)  # Имитация сложных вычислений
            counter = current + 1
        
    print(f"{name}: увеличил счетчик до {counter}")

# Создаем потоки
thread1 = threading.Thread(target=increment_counter, args=("Поток 1", 10))
thread2 = threading.Thread(target=increment_counter_with_context, args=("Поток 2", 10))

# Запускаем потоки
thread1.start()
thread2.start()

# Ждем завершения потоков
thread1.join()
thread2.join()

print(f"Итоговое значение счетчика: {counter}")
```

### Другие инструменты синхронизации

Python предоставляет и другие инструменты синхронизации:

- `RLock` — повторно входимая блокировка, которую может повторно захватить тот же поток
- `Semaphore` — семафор, ограничивающий доступ к ресурсу определенным количеством потоков
- `Event` — событие для оповещения потоков о наступлении определенного условия
- `Condition` — условная переменная для синхронизации по условию
- `Barrier` — барьер для синхронизации определенного числа потоков

```python
import threading
import time
import random

# Пример использования семафора
def use_resource(name, semaphore):
    # Запрашиваем доступ к ресурсу
    with semaphore:
        print(f"{name}: получил доступ к ресурсу")
        time.sleep(random.randint(1, 3))  # Используем ресурс
        print(f"{name}: освободил ресурс")

# Создаем семафор, разрешающий доступ только 2 потокам одновременно
semaphore = threading.Semaphore(2)

# Создаем и запускаем 5 потоков
threads = []
for i in range(5):
    thread = threading.Thread(target=use_resource, args=(f"Поток {i+1}", semaphore))
    threads.append(thread)
    thread.start()

# Пример использования события
def wait_for_event(name, event):
    print(f"{name}: ожидает событие")
    event.wait()  # Блокируется до тех пор, пока событие не будет установлено
    print(f"{name}: получил уведомление о событии")

# Создаем событие
event = threading.Event()

# Создаем потоки, ожидающие событие
for i in range(3):
    thread = threading.Thread(target=wait_for_event, args=(f"Ожидающий {i+1}", event))
    thread.start()

# Даем потокам время начать ожидание
time.sleep(2)

# Устанавливаем событие, оповещая все ожидающие потоки
print("Основная программа: устанавливаем событие")
event.set()
```

### Потокобезопасные структуры данных

Python предоставляет потокобезопасные структуры данных в модуле `queue`:

- `Queue` — стандартная очередь FIFO (First-In-First-Out)
- `LifoQueue` — стек LIFO (Last-In-First-Out)
- `PriorityQueue` — очередь с приоритетами

```python
import threading
import queue
import time
import random

# Создаем очередь задач
task_queue = queue.Queue()

# Функция производителя (добавляет задачи в очередь)
def producer(name, count):
    for i in range(count):
        task = f"Задача {i+1} от {name}"
        task_queue.put(task)
        print(f"{name}: добавил {task}")
        time.sleep(random.random())  # Случайная пауза

# Функция потребителя (извлекает задачи из очереди)
def consumer(name):
    while True:
        try:
            # Получаем задачу из очереди с таймаутом
            task = task_queue.get(timeout=3)
            print(f"{name}: выполняет {task}")
            time.sleep(random.random() * 2)  # Имитация выполнения
            
            # Отмечаем задачу как выполненную
            task_queue.task_done()
        except queue.Empty:
            # Если очередь пуста в течение таймаута, завершаем работу
            print(f"{name}: очередь пуста, завершение работы")
            break

# Создаем производителей
producers = []
for i in range(2):
    producer_thread = threading.Thread(target=producer, args=(f"Производитель {i+1}", 5))
    producers.append(producer_thread)
    producer_thread.start()

# Создаем потребителей
consumers = []
for i in range(3):
    consumer_thread = threading.Thread(target=consumer, args=(f"Потребитель {i+1}",))
    consumers.append(consumer_thread)
    consumer_thread.daemon = True  # Делаем потребителей демонами
    consumer_thread.start()

# Ожидаем завершения производителей
for producer_thread in producers:
    producer_thread.join()

# Ожидаем завершения всех задач в очереди
task_queue.join()

print("Все задачи обработаны")
```

### Локальные данные потока (ThreadLocal)

Для хранения данных, которые должны быть уникальными для каждого потока, используется `threading.local()`:

```python
import threading
import random

# Создаем локальные данные потока
thread_local = threading.local()

def initialize_thread_data():
    # Каждый поток получает свой уникальный ID
    thread_local.id = random.randint(1000, 9999)
    print(f"Поток {threading.current_thread().name}: инициализирован с ID {thread_local.id}")

def process_data():
    # Используем локальные данные потока
    print(f"Поток {threading.current_thread().name}: обрабатывает данные с ID {thread_local.id}")

def worker():
    initialize_thread_data()
    process_data()

# Создаем несколько потоков
threads = []
for i in range(3):
    thread = threading.Thread(target=worker, name=f"Поток-{i+1}")
    threads.append(thread)
    thread.start()

# Ожидаем завершения всех потоков
for thread in threads:
    thread.join()
```

## Многопроцессорность в Python (модуль multiprocessing)

Модуль `multiprocessing` позволяет создавать процессы, используя API, аналогичный модулю `threading`. Каждый процесс имеет свой интерпретатор Python, поэтому многопроцессорность позволяет обойти ограничения GIL и достичь истинного параллелизма при выполнении CPU-bound задач.

### Создание и запуск процессов

```python
import multiprocessing
import time

def task(name, delay):
    """Функция, которая будет выполняться в отдельном процессе"""
    print(f"{name}: начало выполнения")
    time.sleep(delay)  # Имитация работы
    print(f"{name}: завершение (после {delay} секунд)")

if __name__ == '__main__':
    # Важно: код в блоке if __name__ == '__main__' для multiprocessing
    
    # Создание процессов
    process1 = multiprocessing.Process(target=task, args=("Процесс 1", 2))
    process2 = multiprocessing.Process(target=task, args=("Процесс 2", 3))
    
    # Запуск процессов
    print("Основная программа: запуск процессов")
    process1.start()
    process2.start()
    
    # Ожидание завершения процессов
    print("Основная программа: ожидание завершения процессов")
    process1.join()
    process2.join()
    
    print("Основная программа: все процессы завершены")
```

### Создание процесса путем наследования от класса Process

```python
import multiprocessing
import time

class MyProcess(multiprocessing.Process):
    def __init__(self, name, delay):
        super().__init__()
        self.name = name
        self.delay = delay
    
    def run(self):
        """Метод, который будет выполняться при запуске процесса"""
        print(f"{self.name}: начало выполнения")
        time.sleep(self.delay)  # Имитация работы
        print(f"{self.name}: завершение (после {self.delay} секунд)")

if __name__ == '__main__':
    # Создание процессов
    process1 = MyProcess("Процесс 1", 2)
    process2 = MyProcess("Процесс 2", 3)
    
    # Запуск процессов
    print("Основная программа: запуск процессов")
    process1.start()
    process2.start()
    
    # Ожидание завершения процессов
    print("Основная программа: ожидание завершения процессов")
    process1.join()
    process2.join()
    
    print("Основная программа: все процессы завершены")
```

### Обмен данными между процессами

В отличие от потоков, процессы не могут напрямую обращаться к общей памяти. Для обмена данными между процессами используются следующие механизмы:

- `Queue` — очередь для обмена объектами между процессами
- `Pipe` — двусторонний канал связи между процессами
- `Value` и `Array` — совместно используемые значения и массивы
- `Manager` — более гибкий способ для создания разделяемых объектов

#### Пример с использованием Queue

```python
import multiprocessing
import time
import random

def producer(queue, name, count):
    """Процесс-производитель, добавляющий данные в очередь"""
    for i in range(count):
        item = f"Элемент {i+1} от {name}"
        queue.put(item)
        print(f"{name}: отправил {item}")
        time.sleep(random.random())

def consumer(queue, name):
    """Процесс-потребитель, извлекающий данные из очереди"""
    while True:
        try:
            item = queue.get(timeout=3)
            print(f"{name}: получил {item}")
            time.sleep(random.random() * 2)
        except Exception:
            print(f"{name}: очередь пуста или закрыта, завершение работы")
            break

if __name__ == '__main__':
    # Создаем очередь для обмена данными
    task_queue = multiprocessing.Queue()
    
    # Создаем процессы
    producer1 = multiprocessing.Process(target=producer, args=(task_queue, "Производитель 1", 5))
    producer2 = multiprocessing.Process(target=producer, args=(task_queue, "Производитель 2", 3))
    consumer1 = multiprocessing.Process(target=consumer, args=(task_queue, "Потребитель 1"))
    consumer2 = multiprocessing.Process(target=consumer, args=(task_queue, "Потребитель 2"))
    
    # Запускаем процессы
    producer1.start()
    producer2.start()
    consumer1.start()
    consumer2.start()
    
    # Ожидаем завершения производителей
    producer1.join()
    producer2.join()
    
    # Даем время потребителям обработать все элементы
    time.sleep(5)
    
    # Завершаем потребителей (они будут завершены после таймаута)
    consumer1.join()
    consumer2.join()
    
    print("Все процессы завершены")
```

#### Пример с использованием Pipe

```python
import multiprocessing
import time

def ping_process(conn):
    """Отправляет 'ping' и ожидает 'pong'"""
    conn.send("ping")
    print("Процесс 1: отправил ping")
    
    response = conn.recv()
    print(f"Процесс 1: получил {response}")

def pong_process(conn):
    """Ожидает 'ping' и отправляет 'pong'"""
    request = conn.recv()
    print(f"Процесс 2: получил {request}")
    
    time.sleep(1)  # Небольшая задержка
    
    conn.send("pong")
    print("Процесс 2: отправил pong")

if __name__ == '__main__':
    # Создаем двунаправленный канал
    parent_conn, child_conn = multiprocessing.Pipe()
    
    # Создаем процессы
    process1 = multiprocessing.Process(target=ping_process, args=(parent_conn,))
    process2 = multiprocessing.Process(target=pong_process, args=(child_conn,))
    
    # Запускаем процессы
    process1.start()
    process2.start()
    
    # Ожидаем завершения процессов
    process1.join()
    process2.join()
    
    print("Обмен сообщениями завершен")
```

#### Пример с использованием Value и Array

```python
import multiprocessing
import time

def increment_value(shared_value, lock):
    """Увеличивает разделяемое значение"""
    for _ in range(100):
        with lock:
            shared_value.value += 1
    print(f"Процесс {multiprocessing.current_process().name}: значение = {shared_value.value}")

def update_array(shared_array, lock):
    """Обновляет элементы разделяемого массива"""
    for i in range(len(shared_array)):
        with lock:
            shared_array[i] = shared_array[i] * 2
    print(f"Процесс {multiprocessing.current_process().name}: массив = {list(shared_array)}")

if __name__ == '__main__':
    # Создаем разделяемое значение и массив
    shared_value = multiprocessing.Value('i', 0)  # 'i' - тип int
    shared_array = multiprocessing.Array('i', [1, 2, 3, 4, 5])  # 'i' - тип int
    
    # Создаем блокировку для синхронизации
    lock = multiprocessing.Lock()
    
    # Создаем процессы
    processes = []
    for i in range(4):
        if i % 2 == 0:
            p = multiprocessing.Process(target=increment_value, args=(shared_value, lock))
        else:
            p = multiprocessing.Process(target=update_array, args=(shared_array, lock))
        processes.append(p)
    
    # Запускаем процессы
    for p in processes:
        p.start()
    
    # Ожидаем завершения процессов
    for p in processes:
        p.join()
    
    print(f"Итоговое значение: {shared_value.value}")
    print(f"Итоговый массив: {list(shared_array)}")
```

#### Пример с использованием Manager

```python
import multiprocessing
import time
import random

def update_dict(shared_dict, key, lock):
    """Обновляет разделяемый словарь"""
    with lock:
        if key in shared_dict:
            shared_dict[key] += 1
        else:
            shared_dict[key] = 1
    
    print(f"Процесс {multiprocessing.current_process().name}: {key} = {shared_dict[key]}")

def update_list(shared_list, lock):
    """Добавляет элементы в разделяемый список"""
    value = random.randint(1, 100)
    with lock:
        shared_list.append(value)
    
    print(f"Процесс {multiprocessing.current_process().name}: добавил {value}, список = {list(shared_list)}")

if __name__ == '__main__':
    # Создаем менеджер
    with multiprocessing.Manager() as manager:
        # Создаем разделяемые объекты
        shared_dict = manager.dict()
        shared_list = manager.list()
        
        # Создаем блокировку
        lock = multiprocessing.Lock()
        
        # Создаем процессы
        processes = []
        for i in range(10):
            if i % 2 == 0:
                key = f"key{i//2}"
                p = multiprocessing.Process(target=update_dict, args=(shared_dict, key, lock))
            else:
                p = multiprocessing.Process(target=update_list, args=(shared_list, lock))
            processes.append(p)
        
        # Запускаем процессы
        for p in processes:
            p.start()
        
        # Ожидаем завершения процессов
        for p in processes:
            p.join()
        
        # Выводим итоговые результаты
        print(f"Итоговый словарь: {dict(shared_dict)}")
        print(f"Итоговый список: {list(shared_list)}")
```

### Пул процессов

Для упрощения работы с множеством процессов, модуль `multiprocessing` предоставляет класс `Pool`, позволяющий распределить выполнение функции между несколькими процессами:

```python
import multiprocessing
import time
import os

def cpu_bound_task(n):
    """Функция с интенсивными вычислениями"""
    print(f"Задача {n} выполняется в процессе {os.getpid()}")
    result = 0
    for i in range(10**7):  # Выполняем много итераций
        result += i
    return f"Задача {n} завершена с результатом {result}"

if __name__ == '__main__':
    # Получаем количество CPU
    num_cpu = multiprocessing.cpu_count()
    print(f"Количество CPU: {num_cpu}")
    
    # Создаем пул процессов
    with multiprocessing.Pool(processes=num_cpu) as pool:
        # Применяем функцию к каждому элементу входной последовательности
        results = pool.map(cpu_bound_task, range(num_cpu * 2))
        
        # Выводим результаты
        for result in results:
            print(result)
```

Методы Pool:

- `map(func, iterable)` — применяет функцию к каждому элементу последовательности и возвращает список результатов
- `apply(func, args)` — применяет функцию к аргументам и возвращает результат (блокирующий вызов)
- `apply_async(func, args)` — применяет функцию к аргументам асинхронно и возвращает объект AsyncResult
- `map_async(func, iterable)` — версия map, которая возвращает объект AsyncResult
- `imap(func, iterable)` — ленивая версия map, возвращающая итератор
- `imap_unordered(func, iterable)` — как imap, но результаты возвращаются в произвольном порядке

```python
import multiprocessing
import time
import random

def process_item(item):
    """Обрабатывает элемент с случайной задержкой"""
    delay = random.random() * 2
    time.sleep(delay)
    return f"Элемент {item} обработан за {delay:.2f} секунд"

if __name__ == '__main__':
    with multiprocessing.Pool(processes=4) as pool:
        # Асинхронный вызов map
        result_async = pool.map_async(process_item, range(10))
        
        # Можно выполнять другие операции, пока результаты не готовы
        print("Ожидание результатов...")
        
        # Получаем результаты (блокирующий вызов)
        results = result_async.get()
        
        print("Результаты map_async:")
        for result in results:
            print(result)
        
        # Использование imap_unordered для получения результатов по мере готовности
        print("\nРезультаты imap_unordered (по мере готовности):")
        for result in pool.imap_unordered(process_item, range(10, 20)):
            print(result)
```

## Модуль concurrent.futures

Модуль `concurrent.futures` предоставляет высокоуровневый интерфейс для асинхронного выполнения задач. Он имеет два основных исполнителя:

- `ThreadPoolExecutor` — использует потоки для параллельного выполнения
- `ProcessPoolExecutor` — использует процессы для параллельного выполнения

### ThreadPoolExecutor

```python
from concurrent.futures import ThreadPoolExecutor
import time
import random

def task(n):
    """Функция с задержкой"""
    delay = random.random() * 3
    print(f"Задача {n} начала выполнение, задержка {delay:.2f} секунд")
    time.sleep(delay)
    return f"Задача {n} завершена после {delay:.2f} секунд"

# Создаем ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=5) as executor:
    # Отправляем задачи на выполнение
    futures = [executor.submit(task, i) for i in range(10)]
    
    # Получаем результаты по мере их готовности
    for future in futures:
        print(future.result())
```

### ProcessPoolExecutor

```python
from concurrent.futures import ProcessPoolExecutor
import time
import random
import os

def cpu_task(n):
    """Функция с интенсивными вычислениями"""
    print(f"Задача {n} выполняется в процессе {os.getpid()}")
    result = 0
    # Выполняем вычисления
    for i in range(10**7):
        result += i
    return f"Задача {n} завершена с результатом {result}"

# Создаем ProcessPoolExecutor
with ProcessPoolExecutor(max_workers=4) as executor:
    # Отправляем задачи на выполнение
    futures = [executor.submit(cpu_task, i) for i in range(8)]
    
    # Получаем результаты по мере их готовности
    for future in futures:
        print(future.result())
```

### Метод map

Оба исполнителя предоставляют метод `map`, аналогичный встроенной функции `map`, но выполняющий задачи параллельно:

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

def process_item(item):
    """Обрабатывает элемент"""
    time.sleep(1)  # Имитация работы
    return item * item

# Использование ThreadPoolExecutor с map
with ThreadPoolExecutor(max_workers=4) as executor:
    # Применяем функцию к каждому элементу
    results = list(executor.map(process_item, range(10)))
    print(f"Результаты ThreadPoolExecutor: {results}")

# Использование ProcessPoolExecutor с map
with ProcessPoolExecutor(max_workers=4) as executor:
    # Применяем функцию к каждому элементу
    results = list(executor.map(process_item, range(10)))
    print(f"Результаты ProcessPoolExecutor: {results}")
```

### Ожидание завершения и отмена задач

```python
from concurrent.futures import ThreadPoolExecutor, wait, as_completed
import time
import random

def task(n):
    """Функция с случайной задержкой"""
    delay = random.random() * 5
    print(f"Задача {n} начала выполнение, задержка {delay:.2f} секунд")
    time.sleep(delay)
    return f"Результат задачи {n}: {delay:.2f} секунд"

# Создаем исполнитель
with ThreadPoolExecutor(max_workers=5) as executor:
    # Отправляем задачи на выполнение
    futures = [executor.submit(task, i) for i in range(10)]
    
    # Ожидание завершения определенного набора задач
    done, not_done = wait(futures[:5], return_when="FIRST_COMPLETED")
    
    print(f"Завершено задач: {len(done)}")
    print(f"Ожидают завершения задач: {len(not_done)}")
    
    # Получаем результаты завершенных задач
    for future in done:
        print(future.result())
    
    # Отменяем оставшиеся задачи
    for future in not_done:
        future.cancel()
        print(f"Задача отменена: {future.cancelled()}")
    
    # Получаем результаты по мере их готовности
    print("\nРезультаты всех не отмененных задач:")
    for future in as_completed(futures):
        if not future.cancelled():
            try:
                result = future.result()
                print(result)
            except Exception as exc:
                print(f"Задача вызвала исключение: {exc}")
```

## Практические примеры

### Пример 1: Скачивание файлов

```python
import requests
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

def download_file(url):
    """Скачивает файл по URL и возвращает его размер"""
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()  # Проверка на ошибки
        
        # Извлекаем имя файла из URL
        filename = os.path.basename(url)
        
        # Сохраняем файл
        with open(filename, 'wb') as file:
            for chunk in response.iter_content(chunk_size=8192):
                file.write(chunk)
        
        # Получаем размер файла
        file_size = os.path.getsize(filename)
        
        return f"Скачан файл {filename}, размер: {file_size} байт"
    except Exception as e:
        return f"Ошибка при скачивании {url}: {str(e)}"

def download_files_sequential(urls):
    """Последовательно скачивает файлы"""
    results = []
    start_time = time.time()
    
    for url in urls:
        result = download_file(url)
        results.append(result)
    
    elapsed = time.time() - start_time
    return results, elapsed

def download_files_parallel(urls, max_workers=5):
    """Параллельно скачивает файлы"""
    results = []
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(download_file, url): url for url in urls}
        
        for future in as_completed(future_to_url):
            results.append(future.result())
    
    elapsed = time.time() - start_time
    return results, elapsed

# Список URL для скачивания
urls = [
    "https://www.python.org/static/img/python-logo.png",
    "https://www.python.org/static/img/python-logo-only.png",
    "https://www.python.org/static/community_logos/python-powered-h-140x182.png",
    "https://www.python.org/static/community_logos/python-powered-h-50x65.png",
    "https://www.python.org/static/community_logos/python-powered-h.png",
    "https://www.python.org/static/community_logos/python-powered-w-100x40.png",
    "https://www.python.org/static/community_logos/python-powered-w-200x80.png",
    "https://www.python.org/static/community_logos/python-powered-w-70x28.png",
    "https://www.python.org/static/community_logos/python-powered-w.png",
    "https://www.python.org/static/favicon.ico"
]

# Последовательное скачивание
sequential_results, sequential_time = download_files_sequential(urls)
print(f"Последовательное скачивание заняло {sequential_time:.2f} секунд")
for result in sequential_results:
    print(f"  {result}")

# Параллельное скачивание
parallel_results, parallel_time = download_files_parallel(urls)
print(f"\nПараллельное скачивание заняло {parallel_time:.2f} секунд")
for result in parallel_results:
    print(f"  {result}")

print(f"\nУскорение: {sequential_time / parallel_time:.2f}x")
```

### Пример 2: Обработка изображений

```python
from PIL import Image, ImageFilter
import os
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

def process_image(image_path, output_folder, effect):
    """Применяет эффект к изображению и сохраняет результат"""
    try:
        # Открываем изображение
        img = Image.open(image_path)
        
        # Получаем имя файла
        filename = os.path.basename(image_path)
        name, ext = os.path.splitext(filename)
        
        # Применяем эффект
        if effect == 'blur':
            processed_img = img.filter(ImageFilter.BLUR)
            output_name = f"{name}_blur{ext}"
        elif effect == 'contour':
            processed_img = img.filter(ImageFilter.CONTOUR)
            output_name = f"{name}_contour{ext}"
        elif effect == 'emboss':
            processed_img = img.filter(ImageFilter.EMBOSS)
            output_name = f"{name}_emboss{ext}"
        elif effect == 'grayscale':
            processed_img = img.convert('L')
            output_name = f"{name}_grayscale{ext}"
        elif effect == 'resize':
            # Уменьшаем изображение в два раза
            width, height = img.size
            processed_img = img.resize((width // 2, height // 2))
            output_name = f"{name}_resized{ext}"
        else:
            return f"Неизвестный эффект: {effect}"
        
        # Путь для сохранения
        output_path = os.path.join(output_folder, output_name)
        
        # Сохраняем изображение
        processed_img.save(output_path)
        
        return f"Обработано: {filename} -> {output_name}"
    except Exception as e:
        return f"Ошибка при обработке {image_path}: {str(e)}"

def process_images_parallel(image_paths, output_folder, effect, max_workers=None):
    """Параллельно обрабатывает изображения"""
    # Если не указано количество процессов, используем количество CPU
    if max_workers is None:
        max_workers = multiprocessing.cpu_count()
    
    # Создаем выходную папку, если её нет
    os.makedirs(output_folder, exist_ok=True)
    
    results = []
    start_time = time.time()
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Создаем задачи
        futures = [
            executor.submit(process_image, img_path, output_folder, effect)
            for img_path in image_paths
        ]
        
        # Получаем результаты по мере готовности
        for future in as_completed(futures):
            results.append(future.result())
    
    elapsed = time.time() - start_time
    return results, elapsed

# Пример использования:
# Предположим, у нас есть папка с изображениями
# image_folder = "path/to/images"
# output_folder = "path/to/output"
# image_paths = [os.path.join(image_folder, filename) for filename in os.listdir(image_folder) if filename.endswith(('.jpg', '.png', '.jpeg'))]
# results, elapsed = process_images_parallel(image_paths, output_folder, 'blur')
# print(f"Обработка заняла {elapsed:.2f} секунд")
# for result in results:
#     print(result)
```

### Пример 3: Поиск файлов и анализ содержимого

```python
import os
import re
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed

def find_files(directory, pattern):
    """Рекурсивно ищет файлы, соответствующие шаблону"""
    matches = []
    for root, _, files in os.walk(directory):
        for filename in files:
            if re.search(pattern, filename):
                matches.append(os.path.join(root, filename))
    return matches

def count_word_occurrences(file_path, word):
    """Подсчитывает количество вхождений слова в файле"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            content = file.read().lower()
            count = len(re.findall(r'\b' + re.escape(word.lower()) + r'\b', content))
        return file_path, count
    except Exception as e:
        return file_path, f"Ошибка: {str(e)}"

def search_in_files(directory, file_pattern, search_word, max_workers=4):
    """Ищет файлы и подсчитывает вхождения слова"""
    start_time = time.time()
    
    # Ищем файлы, соответствующие шаблону
    print(f"Поиск файлов с шаблоном '{file_pattern}' в '{directory}'...")
    files = find_files(directory, file_pattern)
    print(f"Найдено {len(files)} файлов")
    
    # Если файлы не найдены, завершаем
    if not files:
        return [], time.time() - start_time
    
    results = []
    
    # Анализируем содержимое с помощью ProcessPoolExecutor
    print(f"Поиск слова '{search_word}' в файлах...")
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Создаем задачи
        futures = [
            executor.submit(count_word_occurrences, file_path, search_word)
            for file_path in files
        ]
        
        # Получаем результаты
        for future in as_completed(futures):
            file_path, count = future.result()
            results.append((file_path, count))
    
    # Сортируем результаты по количеству вхождений (в порядке убывания)
    results.sort(key=lambda x: x[1] if isinstance(x[1], int) else -1, reverse=True)
    
    return results, time.time() - start_time

# Пример использования
# search_results, elapsed = search_in_files('/path/to/directory', r'\.txt$', 'Python')
# print(f"\nПоиск завершен за {elapsed:.2f} секунд")
# print("\nТоп-10 файлов по количеству вхождений:")
# for file_path, count in search_results[:10]:
#     if isinstance(count, int):
#         print(f"{count} вхождений в {file_path}")
#     else:
#         print(f"{file_path}: {count}")
```

## Практические задания

### Задание 1: Многопоточный веб-краулер

Разработайте многопоточный веб-краулер, который будет обходить веб-страницы, начиная с заданного URL, и собирать ссылки на другие страницы.

**Решение:**

```python
import requests
from bs4 import BeautifulSoup
import threading
import queue
import time
import urllib.parse
import re

class WebCrawler:
    def __init__(self, start_url, max_depth=2, max_threads=5):
        """
        Инициализирует веб-краулер
        
        Args:
            start_url: Начальный URL для обхода
            max_depth: Максимальная глубина обхода
            max_threads: Максимальное количество потоков
        """
        self.start_url = start_url
        self.max_depth = max_depth
        self.max_threads = max_threads
        
        # Базовый домен для ограничения обхода
        self.base_domain = self._get_domain(start_url)
        
        # Очередь URL для обхода (URL, глубина)
        self.url_queue = queue.Queue()
        
        # Множество посещенных URL
        self.visited_urls = set()
        
        # Блокировка для синхронизации доступа к множеству посещенных URL
        self.lock = threading.Lock()
        
        # Результаты обхода: словарь {url: [ссылки]}
        self.results = {}
        
        # Счетчик активных потоков
        self.active_threads = 0
        
        # Событие для сигнализации о завершении работы
        self.done_event = threading.Event()
    
    def _get_domain(self, url):
        """Извлекает домен из URL"""
        parsed_url = urllib.parse.urlparse(url)
        return parsed_url.netloc
    
    def _is_valid_url(self, url):
        """Проверяет, является ли URL допустимым для обхода"""
        # Проверка на абсолютный URL
        if not url.startswith(('http://', 'https://')):
            return False
        
        # Проверка на принадлежность к базовому домену
        return self._get_domain(url) == self.base_domain
    
    def _normalize_url(self, base_url, url):
        """Нормализует URL (абсолютный/относительный)"""
        # Удаляем якоря
        url = url.split('#')[0]
        
        # Если URL пустой после удаления якоря
        if not url:
            return None
        
        # Если URL относительный, преобразуем его в абсолютный
        if not url.startswith(('http://', 'https://')):
            return urllib.parse.urljoin(base_url, url)
        
        return url
    
    def _extract_links(self, url, html_content):
        """Извлекает ссылки из HTML-содержимого"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            
            # Ищем все теги <a> с атрибутом href
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href'].strip()
                
                # Нормализуем URL
                normalized_url = self._normalize_url(url, href)
                
                if normalized_url and self._is_valid_url(normalized_url):
                    links.append(normalized_url)
            
            return links
        except Exception as e:
            print(f"Ошибка при извлечении ссылок из {url}: {str(e)}")
            return []
    
    def _fetch_url(self, url):
        """Загружает содержимое URL"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # Проверка на ошибки HTTP
            return response.text
        except Exception as e:
            print(f"Ошибка при загрузке {url}: {str(e)}")
            return None
    
    def _worker(self):
        """Поток для обхода URL"""
        while not self.done_event.is_set():
            try:
                # Получаем URL из очереди с таймаутом
                url, depth = self.url_queue.get(timeout=1)
                
                # Проверяем, был ли URL уже посещен
                with self.lock:
                    if url in self.visited_urls:
                        self.url_queue.task_done()
                        continue
                    
                    # Помечаем URL как посещенный
                    self.visited_urls.add(url)
                
                print(f"Обработка: {url} (глубина: {depth})")
                
                # Загружаем содержимое URL
                html_content = self._fetch_url(url)
                
                if html_content:
                    # Извлекаем ссылки
                    links = self._extract_links(url, html_content)
                    
                    # Сохраняем результаты
                    with self.lock:
                        self.results[url] = links
                    
                    # Если не достигли максимальной глубины, добавляем найденные ссылки в очередь
                    if depth < self.max_depth:
                        for link in links:
                            # Добавляем только новые URL
                            with self.lock:
                                if link not in self.visited_urls:
                                    self.url_queue.put((link, depth + 1))
                
                # Отмечаем задачу как выполненную
                self.url_queue.task_done()
            
            except queue.Empty:
                # Очередь пуста, проверяем, есть ли активные потоки
                with self.lock:
                    if self.active_threads <= 1:  # Только текущий поток
                        self.done_event.set()
            except Exception as e:
                print(f"Ошибка в потоке: {str(e)}")
                self.url_queue.task_done()
        
        # Уменьшаем счетчик активных потоков при завершении
        with self.lock:
            self.active_threads -= 1
    
    def crawl(self):
        """Запускает процесс обхода"""
        start_time = time.time()
        
        # Добавляем начальный URL в очередь
        self.url_queue.put((self.start_url, 0))
        
        # Создаем потоки
        threads = []
        for _ in range(self.max_threads):
            thread = threading.Thread(target=self._worker)
            thread.daemon = True
            threads.append(thread)
        
        # Запускаем потоки
        with self.lock:
            self.active_threads = self.max_threads
        
        for thread in threads:
            thread.start()
        
        # Ожидаем завершения всех потоков или сигнала о завершении
        while not self.done_event.is_set():
            time.sleep(0.1)
        
        # Вычисляем статистику
        elapsed_time = time.time() - start_time
        urls_count = len(self.visited_urls)
        links_count = sum(len(links) for links in self.results.values())
        
        return {
            'elapsed_time': elapsed_time,
            'urls_count': urls_count,
            'links_count': links_count,
            'results': self.results
        }

# Функция для вывода результатов обхода
def print_crawl_results(results, max_urls=20):
    # Вывод общей статистики
    print("\n=== Статистика обхода ===")
    print(f"Время выполнения: {results['elapsed_time']:.2f} секунд")
    print(f"Посещено URL: {results['urls_count']}")
    print(f"Найдено ссылок: {results['links_count']}")
    
    # Вывод посещенных URL и найденных ссылок
    print("\n=== Посещенные URL и найденные ссылки ===")
    for i, (url, links) in enumerate(list(results['results'].items())[:max_urls]):
        print(f"\n{i+1}. {url}")
        print(f"   Найдено {len(links)} ссылок:")
        for j, link in enumerate(links[:5]):  # Показываем только первые 5 ссылок
            print(f"   - {link}")
        if len(links) > 5:
            print(f"   ... и еще {len(links)-5} ссылок")
    
    if len(results['results']) > max_urls:
        print(f"\n... и еще {len(results['results'])-max_urls} URL")

# Пример использования
if __name__ == "__main__":
    # Создаем краулер с начальным URL, максимальной глубиной 2 и 5 потоками
    crawler = WebCrawler("https://www.python.org", max_depth=2, max_threads=5)
    
    # Запускаем обход
    print("Начало обхода...")
    results = crawler.crawl()
    
    # Выводим результаты
    print_crawl_results(results)
```

### Задание 2: Многопроцессорная обработка и анализ больших данных

Создайте программу, которая:

1. Генерирует несколько больших файлов с случайными числами
2. Анализирует файлы, вычисляя статистические характеристики (среднее, медиана, стандартное отклонение и т.д.)
3. Сравнивает время выполнения в однопроцессорном и многопроцессорном режимах

**Решение:**

```python
import os
import random
import time
import statistics
import multiprocessing
import matplotlib.pyplot as plt
import numpy as np
from concurrent.futures import ProcessPoolExecutor

class DataAnalyzer:
    def __init__(self, data_dir="data_files", num_files=5, num_values=1000000):
        """
        Инициализирует анализатор данных
        
        Args:
            data_dir: Директория для хранения файлов
            num_files: Количество файлов для генерации
            num_values: Количество значений в каждом файле
        """
        self.data_dir = data_dir
        self.num_files = num_files
        self.num_values = num_values
        self.file_paths = []
        
        # Создаем директорию, если она не существует
        os.makedirs(data_dir, exist_ok=True)
    
    def generate_data_files(self):
        """Генерирует файлы с случайными числами"""
        print(f"Генерация {self.num_files} файлов с {self.num_values} значениями каждый...")
        
        self.file_paths = []
        for i in range(self.num_files):
            file_path = os.path.join(self.data_dir, f"data_{i+1}.txt")
            
            with open(file_path, 'w') as file:
                # Генерируем значения и записываем их в файл
                for _ in range(self.num_values):
                    value = random.uniform(-1000, 1000)
                    file.write(f"{value}\n")
            
            self.file_paths.append(file_path)
            print(f"Создан файл: {file_path}")
        
        return self.file_paths
    
    def analyze_file(self, file_path):
        """Анализирует один файл и возвращает статистические характеристики"""
        try:
            # Читаем значения из файла
            values = []
            with open(file_path, 'r') as file:
                for line in file:
                    values.append(float(line.strip()))
            
            # Вычисляем статистические характеристики
            mean = statistics.mean(values)
            median = statistics.median(values)
            stdev = statistics.stdev(values)
            min_val = min(values)
            max_val = max(values)
            
            # Вычисляем гистограмму (разбиваем диапазон на 10 бинов)
            bins = 10
            bin_width = (max_val - min_val) / bins
            histogram = [0] * bins
            
            for value in values:
                bin_index = min(int((value - min_val) / bin_width), bins - 1)
                histogram[bin_index] += 1
            
            return {
                'file_path': file_path,
                'count': len(values),
                'mean': mean,
                'median': median,
                'stdev': stdev,
                'min': min_val,
                'max': max_val,
                'histogram': histogram
            }
        except Exception as e:
            return {
                'file_path': file_path,
                'error': str(e)
            }
    
    def analyze_sequential(self):
        """Анализирует все файлы последовательно"""
        print("\nЗапуск последовательного анализа...")
        
        start_time = time.time()
        results = []
        
        for file_path in self.file_paths:
            result = self.analyze_file(file_path)
            results.append(result)
        
        elapsed_time = time.time() - start_time
        print(f"Последовательный анализ завершен за {elapsed_time:.2f} секунд")
        
        return results, elapsed_time
    
    def analyze_parallel(self, num_processes=None):
        """Анализирует все файлы параллельно"""
        # Если не указано количество процессов, используем количество CPU
        if num_processes is None:
            num_processes = multiprocessing.cpu_count()
        
        print(f"\nЗапуск параллельного анализа с {num_processes} процессами...")
        
        start_time = time.time()
        results = []
        
        with ProcessPoolExecutor(max_workers=num_processes) as executor:
            # Отправляем задачи на выполнение
            future_to_file = {executor.submit(self.analyze_file, file_path): file_path for file_path in self.file_paths}
            
            # Получаем результаты
            for future in future_to_file:
                results.append(future.result())
        
        elapsed_time = time.time() - start_time
        print(f"Параллельный анализ завершен за {elapsed_time:.2f} секунд")
        
        return results, elapsed_time
    
    def compare_performance(self):
        """Сравнивает производительность последовательного и параллельного подходов"""
        # Последовательный анализ
        sequential_results, sequential_time = self.analyze_sequential()
        
        # Параллельный анализ
        parallel_results, parallel_time = self.analyze_parallel()
        
        # Вычисляем ускорение
        speedup = sequential_time / parallel_time
        
        print("\n=== Сравнение производительности ===")
        print(f"Последовательный анализ: {sequential_time:.2f} секунд")
        print(f"Параллельный анализ: {parallel_time:.2f} секунд")
        print(f"Ускорение: {speedup:.2f}x")
        
        # Визуализация результатов
        self.visualize_results(sequential_results, parallel_results, sequential_time, parallel_time)
        
        return {
            'sequential_time': sequential_time,
            'parallel_time': parallel_time,
            'speedup': speedup,
            'sequential_results': sequential_results,
            'parallel_results': parallel_results
        }
    
    def visualize_results(self, sequential_results, parallel_results, sequential_time, parallel_time):
        """Визуализирует результаты анализа"""
        # Создаем новый рисунок с подграфиками
        fig, axs = plt.subplots(2, 2, figsize=(15, 10))
        
        # График времени выполнения
        times = [sequential_time, parallel_time]
        axs[0, 0].bar(['Последовательный', 'Параллельный'], times, color=['blue', 'green'])
        axs[0, 0].set_title('Время выполнения')
        axs[0, 0].set_ylabel('Время (секунды)')
        
        # Добавляем значения на график
        for i, v in enumerate(times):
            axs[0, 0].text(i, v + 0.1, f"{v:.2f} сек", ha='center')
        
        # Среднее значение по файлам
        means_sequential = [result['mean'] for result in sequential_results if 'mean' in result]
        means_parallel = [result['mean'] for result in parallel_results if 'mean' in result]
        
        file_names = [os.path.basename(result['file_path']) for result in sequential_results if 'file_path' in result]
        
        axs[0, 1].plot(file_names, means_sequential, 'o-', label='Последовательный')
        axs[0, 1].plot(file_names, means_parallel, 'o-', label='Параллельный')
        axs[0, 1].set_title('Среднее значение по файлам')
        axs[0, 1].set_xticks(range(len(file_names)))
        axs[0, 1].set_xticklabels(file_names, rotation=45)
        axs[0, 1].legend()
        
        # Стандартное отклонение по файлам
        stdevs_sequential = [result['stdev'] for result in sequential_results if 'stdev' in result]
        stdevs_parallel = [result['stdev'] for result in parallel_results if 'stdev' in result]
        
        axs[1, 0].plot(file_names, stdevs_sequential, 'o-', label='Последовательный')
        axs[1, 0].plot(file_names, stdevs_parallel, 'o-', label='Параллельный')
        axs[1, 0].set_title('Стандартное отклонение по файлам')
        axs[1, 0].set_xticks(range(len(file_names)))
        axs[1, 0].set_xticklabels(file_names, rotation=45)
        axs[1, 0].legend()
        
        # Гистограмма значений для первого файла
        if 'histogram' in sequential_results[0]:
            histogram = sequential_results[0]['histogram']
            min_val = sequential_results[0]['min']
            max_val = sequential_results[0]['max']
            
            bin_edges = np.linspace(min_val, max_val, len(histogram) + 1)
            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
            
            axs[1, 1].bar(bin_centers, histogram, width=(max_val - min_val) / len(histogram), alpha=0.7)
            axs[1, 1].set_title(f'Гистограмма значений ({file_names[0]})')
            axs[1, 1].set_xlabel('Значение')
            axs[1, 1].set_ylabel('Частота')
        
        # Настройка макета
        plt.tight_layout()
        plt.savefig('data_analysis_results.png')
        print("Визуализация сохранена в файле 'data_analysis_results.png'")

# Пример использования
if __name__ == "__main__":
    # Создаем анализатор данных
    analyzer = DataAnalyzer(num_files=4, num_values=500000)
    
    # Генерируем файлы с данными
    analyzer.generate_data_files()
    
    # Сравниваем производительность
    results = analyzer.compare_performance()
```

### Задание 3: Мониторинг системы в реальном времени

Разработайте программу для мониторинга системных ресурсов (CPU, память, диск) в реальном времени с использованием многопоточности для сбора и отображения данных.

**Решение:**

```python
import psutil
import time
import threading
import queue
import datetime
import os
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from collections import deque

class SystemMonitor:
    def __init__(self, history_length=60):
        """
        Инициализирует монитор системных ресурсов
        
        Args:
            history_length: Длина истории для хранения (в секундах)
        """
        self.history_length = history_length
        
        # Создаем очереди для обмена данными между потоками
        self.cpu_queue = queue.Queue()
        self.memory_queue = queue.Queue()
        self.disk_queue = queue.Queue()
        self.network_queue = queue.Queue()
        
        # Создаем структуры для хранения исторических данных
        self.timestamps = deque(maxlen=history_length)
        self.cpu_history = deque(maxlen=history_length)
        self.memory_history = deque(maxlen=history_length)
        self.disk_read_history = deque(maxlen=history_length)
        self.disk_write_history = deque(maxlen=history_length)
        self.network_sent_history = deque(maxlen=history_length)
        self.network_recv_history = deque(maxlen=history_length)
        
        # Переменные для отслеживания предыдущих значений
        self.prev_disk_read = 0
        self.prev_disk_write = 0
        self.prev_net_sent = 0
        self.prev_net_recv = 0
        self.prev_time = time.time()
        
        # Флаг для контроля за работой потоков
        self.running = False
        
        # Потоки для сбора данных
        self.cpu_thread = None
        self.memory_thread = None
        self.disk_thread = None
        self.network_thread = None
        
        # Поток для обновления графиков
        self.update_thread = None
        
        # Для анимации графиков
        self.fig = None
        self.axes = None
        self.animation = None
    
    def collect_cpu_data(self):
        """Собирает данные о загрузке CPU"""
        while self.running:
            cpu_percent = psutil.cpu_percent(interval=1)
            self.cpu_queue.put(cpu_percent)
    
    def collect_memory_data(self):
        """Собирает данные о использовании памяти"""
        while self.running:
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            self.memory_queue.put(memory_percent)
            time.sleep(1)
    
    def collect_disk_data(self):
        """Собирает данные о дисковой активности"""
        while self.running:
            disk_io = psutil.disk_io_counters()
            current_time = time.time()
            
            # Вычисляем скорость чтения/записи
            if self.prev_disk_read > 0 and self.prev_disk_write > 0:
                read_speed = (disk_io.read_bytes - self.prev_disk_read) / (current_time - self.prev_time) / 1024  # КБ/с
                write_speed = (disk_io.write_bytes - self.prev_disk_write) / (current_time - self.prev_time) / 1024  # КБ/с
                self.disk_queue.put((read_speed, write_speed))
            
            # Обновляем предыдущие значения
            self.prev_disk_read = disk_io.read_bytes
            self.prev_disk_write = disk_io.write_bytes
            self.prev_time = current_time
            
            time.sleep(1)
    
    def collect_network_data(self):
        """Собирает данные о сетевой активности"""
        while self.running:
            net_io = psutil.net_io_counters()
            current_time = time.time()
            
            # Вычисляем скорость отправки/получения
            if self.prev_net_sent > 0 and self.prev_net_recv > 0:
                sent_speed = (net_io.bytes_sent - self.prev_net_sent) / (current_time - self.prev_time) / 1024  # КБ/с
                recv_speed = (net_io.bytes_recv - self.prev_net_recv) / (current_time - self.prev_time) / 1024  # КБ/с
                self.network_queue.put((sent_speed, recv_speed))
            
            # Обновляем предыдущие значения
            self.prev_net_sent = net_io.bytes_sent
            self.prev_net_recv = net_io.bytes_recv
            self.prev_time = current_time
            
            time.sleep(1)
    
    def update_data(self):
        """Обновляет исторические данные из очередей"""
        while self.running:
            current_time = datetime.datetime.now().strftime('%H:%M:%S')
            
            # Получаем данные из очередей
            try:
                cpu_percent = self.cpu_queue.get(block=False)
                self.cpu_history.append(cpu_percent)
                self.timestamps.append(current_time)
            except queue.Empty:
                pass
            
            try:
                memory_percent = self.memory_queue.get(block=False)
                self.memory_history.append(memory_percent)
            except queue.Empty:
                pass
            
            try:
                read_speed, write_speed = self.disk_queue.get(block=False)
                self.disk_read_history.append(read_speed)
                self.disk_write_history.append(write_speed)
            except queue.Empty:
                pass
            
            try:
                sent_speed, recv_speed = self.network_queue.get(block=False)
                self.network_sent_history.append(sent_speed)
                self.network_recv_history.append(recv_speed)
            except queue.Empty:
                pass
            
            time.sleep(0.1)
    
    def start_monitoring(self):
        """Запускает мониторинг системных ресурсов"""
        if self.running:
            print("Мониторинг уже запущен")
            return
        
        self.running = True
        
        # Запускаем потоки для сбора данных
        self.cpu_thread = threading.Thread(target=self.collect_cpu_data)
        self.memory_thread = threading.Thread(target=self.collect_memory_data)
        self.disk_thread = threading.Thread(target=self.collect_disk_data)
        self.network_thread = threading.Thread(target=self.collect_network_data)
        
        # Запускаем поток для обновления графиков
        self.update_thread = threading.Thread(target=self.update_data)
        
        # Делаем потоки демонами, чтобы они завершались при завершении программы
        self.cpu_thread.daemon = True
        self.memory_thread.daemon = True
        self.disk_thread.daemon = True
        self.network_thread.daemon = True
        self.update_thread.daemon = True
        
        # Запускаем потоки
        self.cpu_thread.start()
        self.memory_thread.start()
        self.disk_thread.start()
        self.network_thread.start()
        self.update_thread.start()
        
        print("Мониторинг запущен")
    
    def stop_monitoring(self):
        """Останавливает мониторинг системных ресурсов"""
        self.running = False
        
        # Ожидаем завершения потоков
        if self.cpu_thread:
            self.cpu_thread.join(timeout=1)
        if self.memory_thread:
            self.memory_thread.join(timeout=1)
        if self.disk_thread:
            self.disk_thread.join(timeout=1)
        if self.network_thread:
            self.network_thread.join(timeout=1)
        if self.update_thread:
            self.update_thread.join(timeout=1)
        
        print("Мониторинг остановлен")
    
    def show_current_stats(self):
        """Выводит текущие статистические данные"""
        print("\n=== Текущие статистические данные ===")
        
        # CPU
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"CPU использование: {cpu_percent}%")
        
        # Память
        memory = psutil.virtual_memory()
        print(f"Использование памяти: {memory.percent}%")
        print(f"Всего памяти: {memory.total / (1024**3):.2f} ГБ")
        print(f"Доступно памяти: {memory.available / (1024**3):.2f} ГБ")
        
        # Диск
        disk = psutil.disk_usage('/')
        print(f"Использование диска: {disk.percent}%")
        print(f"Всего места на диске: {disk.total / (1024**3):.2f} ГБ")
        print(f"Свободно места на диске: {disk.free / (1024**3):.2f} ГБ")
        
        # Сеть
        net_io = psutil.net_io_counters()
        print(f"Отправлено: {net_io.bytes_sent / (1024**2):.2f} МБ")
        print(f"Получено: {net_io.bytes_recv / (1024**2):.2f} МБ")
    
    def update_plot(self, frame):
        """Обновляет графики в реальном времени"""
        # Очищаем графики
        for ax in self.axes:
            ax.clear()
        
        # Строим графики с доступными данными
        if len(self.timestamps) > 0:
            x_values = list(range(len(self.timestamps)))  # Используем индексы для оси X
            
            # CPU
            if len(self.cpu_history) > 0:
                self.axes[0].plot(x_values[-len(self.cpu_history):], list(self.cpu_history), 'r-', label='CPU %')
                self.axes[0].set_title('CPU использование')
                self.axes[0].set_ylim(0, 100)
                self.axes[0].set_ylabel('Процент использования')
                self.axes[0].legend()
            
            # Память
            if len(self.memory_history) > 0:
                self.axes[1].plot(x_values[-len(self.memory_history):], list(self.memory_history), 'b-', label='Memory %')
                self.axes[1].set_title('Использование памяти')
                self.axes[1].set_ylim(0, 100)
                self.axes[1].set_ylabel('Процент использования')
                self.axes[1].legend()
            
            # Диск
            if len(self.disk_read_history) > 0 and len(self.disk_write_history) > 0:
                self.axes[2].plot(x_values[-len(self.disk_read_history):], list(self.disk_read_history), 'g-', label='Чтение (КБ/с)')
                self.axes[2].plot(x_values[-len(self.disk_write_history):], list(self.disk_write_history), 'm-', label='Запись (КБ/с)')
                self.axes[2].set_title('Дисковая активность')
                self.axes[2].set_ylabel('КБ/с')
                self.axes[2].legend()
            
            # Сеть
            if len(self.network_sent_history) > 0 and len(self.network_recv_history) > 0:
                self.axes[3].plot(x_values[-len(self.network_sent_history):], list(self.network_sent_history), 'c-', label='Отправлено (КБ/с)')
                self.axes[3].plot(x_values[-len(self.network_recv_history):], list(self.network_recv_history), 'y-', label='Получено (КБ/с)')
                self.axes[3].set_title('Сетевая активность')
                self.axes[3].set_ylabel('КБ/с')
                self.axes[3].legend()
            
            # Настраиваем метки оси X
            for ax in self.axes:
                # Показываем только несколько меток для читаемости
                if len(self.timestamps) > 0:
                    step = max(1, len(self.timestamps) // 5)
                    indices = list(range(0, len(self.timestamps), step))
                    if len(self.timestamps) - 1 not in indices:
                        indices.append(len(self.timestamps) - 1)
                    
                    ax.set_xticks(indices)
                    ax.set_xticklabels([list(self.timestamps)[i] for i in indices], rotation=45)
        
        self.fig.tight_layout()
        
        return self.axes
    
    def show_interactive_dashboard(self):
        """Показывает интерактивную панель мониторинга"""
        # Запускаем мониторинг, если он еще не запущен
        if not self.running:
            self.start_monitoring()
        
        # Создаем фигуру и оси для графиков
        self.fig, self.axes = plt.subplots(2, 2, figsize=(12, 8))
        self.axes = self.axes.flatten()
        
        # Запускаем анимацию
        self.animation = FuncAnimation(self.fig, self.update_plot, interval=1000, cache_frame_data=False)
        
        # Показываем графики
        plt.tight_layout()
        plt.show()
    
    def save_stats_to_file(self, file_path="system_stats.txt"):
        """Сохраняет текущие статистические данные в файл"""
        with open(file_path, 'w') as file:
            file.write("=== Статистика системы ===\n")
            file.write(f"Дата и время: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # CPU
            file.write("--- CPU ---\n")
            file.write(f"Общая загрузка: {psutil.cpu_percent()}%\n")
            for i, percent in enumerate(psutil.cpu_percent(percpu=True)):
                file.write(f"CPU {i}: {percent}%\n")
            
            # Память
            memory = psutil.virtual_memory()
            file.write("\n--- Память ---\n")
            file.write(f"Всего: {memory.total / (1024**3):.2f} ГБ\n")
            file.write(f"Доступно: {memory.available / (1024**3):.2f} ГБ\n")
            file.write(f"Использовано: {memory.used / (1024**3):.2f} ГБ ({memory.percent}%)\n")
            
            # Диск
            file.write("\n--- Диск ---\n")
            for partition in psutil.disk_partitions():
                try:
                    usage = psutil.disk_usage(partition.mountpoint)
                    file.write(f"Раздел: {partition.device} (точка монтирования: {partition.mountpoint})\n")
                    file.write(f"  Всего: {usage.total / (1024**3):.2f} ГБ\n")
                    file.write(f"  Свободно: {usage.free / (1024**3):.2f} ГБ\n")
                    file.write(f"  Использовано: {usage.used / (1024**3):.2f} ГБ ({usage.percent}%)\n")
                except:
                    # Некоторые разделы могут быть недоступны
                    pass
            
            # Сеть
            file.write("\n--- Сеть ---\n")
            net_io = psutil.net_io_counters()
            file.write(f"Отправлено: {net_io.bytes_sent / (1024**2):.2f} МБ\n")
            file.write(f"Получено: {net_io.bytes_recv / (1024**2):.2f} МБ\n")
            
            # Процессы
            file.write("\n--- Топ 10 процессов по использованию CPU ---\n")
            processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
                try:
                    processes.append(proc.info)
                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                    pass
            
            # Сортируем по использованию CPU
            processes = sorted(processes, key=lambda x: x['cpu_percent'], reverse=True)
            for i, proc in enumerate(processes[:10]):
                file.write(f"{i+1}. PID: {proc['pid']}, Имя: {proc['name']}, CPU: {proc['cpu_percent']}%, Память: {proc['memory_percent']:.2f}%\n")
        
        print(f"Статистика сохранена в файл: {file_path}")

# Пример использования
if __name__ == "__main__":
    # Создаем монитор системы
    monitor = SystemMonitor()
    
    # Запускаем мониторинг
    monitor.start_monitoring()
    
    try:
        # Показываем текущие статистические данные
        monitor.show_current_stats()
        
        # Сохраняем статистику в файл
        monitor.save_stats_to_file()
        
        # Показываем интерактивную панель мониторинга
        print("\nЗапуск интерактивного дашборда...")
        monitor.show_interactive_dashboard()
    
    except KeyboardInterrupt:
        print("\nМониторинг прерван")
    finally:
        # Останавливаем мониторинг
        monitor.stop_monitoring()
```

## Идеи для улучшения

Вот несколько идей для улучшения этих заданий:

1. Веб-краулер:
   - Добавьте возможность фильтрации URL по ключевым словам
   - Реализуйте сохранение результатов в базу данных
   - Добавьте веб-интерфейс для управления краулером
   - Реализуйте более продвинутый парсинг HTML (извлечение текста, изображений и т.д.)
   - Добавьте функцию скачивания ресурсов (изображений, PDF и других файлов)

2. Анализатор данных:
   - Добавьте более продвинутые методы анализа (корреляция, регрессия и т.д.)
   - Реализуйте генерацию более сложных данных (с определенными закономерностями)
   - Добавьте возможность визуализации данных в реальном времени
   - Реализуйте экспорт результатов в различных форматах (CSV, Excel, JSON)
   - Добавьте веб-интерфейс для управления анализом

3. Монитор системы:
   - Добавьте возможность установки пороговых значений и оповещений
   - Реализуйте сохранение исторических данных в базу данных
   - Добавьте веб-интерфейс для удаленного мониторинга
   - Реализуйте мониторинг сетевых сервисов (HTTP, SSH и т.д.)
   - Добавьте функцию автоматического запуска действий при определенных условиях

## Заключение

В этом уроке мы изучили многопоточность и многопроцессорность в Python, включая:

- Основы потоков и процессов, их различия и особенности
- Использование модуля `threading` для создания и управления потоками
- Использование модуля `multiprocessing` для создания и управления процессами
- Различные механизмы синхронизации (блокировки, семафоры, события и т.д.)
- Передачу данных между потоками и процессами
- Использование пулов потоков и процессов
- Высокоуровневые абстракции модуля `concurrent.futures`

Параллельное программирование позволяет значительно повысить производительность программ, особенно на многоядерных системах. Однако при выборе между многопоточностью и многопроцессорностью необходимо учитывать особенности задачи:

- Для задач, связанных с ожиданием ввода-вывода (сетевые запросы, файловые операции), часто достаточно многопоточности.
- Для задач, требующих интенсивных вычислений (обработка данных, математические расчеты), лучше использовать многопроцессорность, чтобы обойти ограничения GIL.

В следующем уроке мы рассмотрим сетевое программирование в Python, включая работу с сокетами, HTTP-запросами и различными API.
